# Ethics in Data Science

In recent years, for example, we've seen near endless examples of data science systems not just failing to solve societal inequities, but instead reinforcing them. A hiring algorithm at Amazon was recently scrapped after it was discovered that it systematically [discriminated against female job candidates](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G). An algorithm for prioritizing kidney transplants was found to make it less likely that [Black patients would receive kidney transplants than White patients](https://www.wired.com/story/how-algorithm-blocked-kidney-transplants-black-patients/). Another medical algorithm recommended less preventative care for [sick Black patients than White patients](https://www.washingtonpost.com/health/2019/10/24/racial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients/). And a popular "risk assessment" algorithm used by judges around the country to make decisions about whether defendants should be released pending trial, held on bond, or held without bonds—as well as how convicted criminals should be sentenced—[was found to make more mistakes for Black defendants than White defendants, systematically reporting to judges that they were more likely to re-offend than they actually were according to subsequent analyses.](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) 

Similarly, in the political sphere [Facebook's own research](https://www.wsj.com/articles/facebook-knows-it-encourages-division-top-executives-nixed-solutions-11590507499) has shown that its algorithms polarize users and drive division, a fact that internal documents show its own executives chose to largely ignore.[^facebook_engagement] Facebook's internal research also found that "64% of all extremist group joins are due to our recommendation tools" like "Groups You Should Join" and the Discover tools.

[^facebook_engagement]: This research found the key driver of polarization was Facebook's decision to design recommendations algorithms that prioritized "user engagement" (clicks, shares, comments) which resulted in promoting partisan, polarizing, sensationalist, or extreme content.

While much of these problems are the result of a lack of carelessness or a lack of critical thought on the part of the data scientists developing these algorithms, in other cases problems have arisen because of a lack of critical evaluation of data science tools by their *users*. In 2020, for example, it was discovered that company that provided doctors with (suspiciously free) software designed to provide "clinical decision support" (advice on tests that might be advisable or drugs that might be prescribed for patients based on their medical records) was taking kickbacks from an opioid producing pharmaceutical company to ensure opioids remained a suggested treatment for patients, despite the strong recommendation of groups like the American Medical Association to reduce opioid use in light of the opioid overdose epidemic the US.


- Motivation: piles of case studies. See list under “Prediction: ML Bias” (March 29 at the moment) here
Two types of “unethical algorithms: biased performance and biased predictions. 
- Biased Performance:
    - Super ethically straightforward—works for one group (e.g., White people) but not another (e.g., Black people). “Works” can be defined in terms of whatever performance metrics we care about (accuracy, recall, etc.), but the point is that the error rate is different across groups (e.g., the zoom background blurring algorithm that can’t see Black faces, so just blurs their screens). 
- Biased Predictions:
    - Now the much more complicated one: suppose that your model gives different predictions for people from different group (Black/White), but that’s because that’s what is in the data. Is that ok?
    - If the data is biased: probably not! E.g. Amazon (I think this is what happened) tried to make algorithm that would look at a resume and predict how good an employee that person would be if hired. So it used resumes to train a dataset using managerial employee ratings as labels. But… managers were biased. That was the problem. So algorithm just learned to recapitulate the misogynistic rating of managers. Gotta make sure you know what question is being answered by your algorithm…
    - If the data is NOT biased: depends! Suppose that a model to predict success of a kidney transplant designed to maximize the efficiency of kidney transplant allocations systematically scores black recipients as being less likely to still have their transplanted kidneys and be healthy after ten years… but it’s not an anomaly of the model, it’s a real fact in the world. Is it OK to down-rank Black recipients? (not a contrived example). In that context, providing black recipients with less access to kidneys seems deeply problematic, even if rating them higher will result in less “years of healthy kidneys” after transplant. Here it seems like racially disparate outcomes are unethical. But what if we want to allocate HIV preventative interventions? Black Americans have a much higher incidence of HIV than any other ethnic group in the US—a model that suggests targeting their communities with more interventions would seem appropriate, on a model that doesn’t recognize that HIV rates in the Black community are more than 2x the next closest group seems like it wouldn’t be very helpful.
    - Chicago crime example: https://www.theverge.com/c/22444020/chicago-pd-predictive-policing-heat-list
    - https://filtermag.org/chicago-crime-prediction/
    - https://medium.com/analytics-vidhya/predicting-arrests-looking-into-chicagos-crime-through-machine-learning-78697cc930b9

But… it’s data? How do our values come into play!
Like… checking performance of algorithms for different racial/ethnic groups. 
Or looking for racial/gender heterogeneity during descriptive analyses.
