# 1. The 30-Second Sell

In my six years working with students in the Duke Masters of Interdisciplinary Data Science (MIDS) program — of which I am now the Faculty Director — I have found that the hardest part of training data scientists preparing them for the transition from doing technically challenging but well-structured classroom exercises to applying data science tools effectively when faced with the ambiguities of real world problems. Helping students developing this ability is a focus of our program, and this book — *Solving Real Problems with Data* — is an outgrowth of those efforts. 

All too often, data science education is focused on the technical — coding, statistics, and model evaluation. Technical competence is unquestionably important, but for applied data scientists interesting in using their skills to solve problems (rather than just develop new algorithms), it is just as important we prepare students to navigate the ambiguity of real-world problems. 

*Solving Real Problems with Data* is designed to help fill this gap in data science training. It provides readers with a systematic framework for thinking about their goals and how to achieve them using data science methods. It is written for data scientists who have already learned the basics of statistical inference and machine learning, and focuses instead on everything that comes before and after fitting a model: how to work with stakeholders to clearly articulate the problem they want to address; how to formulate questions whose answers will help address your stakeholder's problem; how to choose the appropriate tool based on the question one seeks to answer; and critically how to evaluate and refine one's models based on your stakeholders needs.

With the help of this framework, as well as case studies and exercises, *Solving Real Problems with Data* will help data science students develop a systematic understanding of how to approach and manage data science projects from conception through delivery and adoption. It will provide a unified perspective on how the perspectives and tools learned in other courses complement one another, and when different approaches to data science are most appropriate.

# 2. The Market

This book is targeted at two markets: courses in applied data science programs, and young data scientists interested in improving their problem-solving skills. In that sense, it is similar to other books in the data science space designed to help young analysts and data scientists bridge the gap between the technical and applied, such as *Trustworthy Online Controlled Experiments* from Kohavi, Tang and Xu (Cambridge University Press, 2020) or *Regressions and Other Stories* from Gelman, Hill and Vehtari (Cambridge University Press, 2020).

## Applied Data Science Programs

This book was developed to address a need in the curriculum of the two-year, applied data science program with which I work (MIDS). As such, it is well suited to use in any of the numerous applied data science programs around the country, including (but not limited to) programs like the Vanderbilt Masters in Data Science, the UNC Chapel Hill Applied Data Science Masters, the Carnegie Mellon Master of Science in Applied Data Science, the Columbia Data Science Institute MS in Data Science, the University of Washington Data Science Masters, or the Stanford Statistics and Data Science Masters.

This book would also be appropriate for the growing number of data science Masters programs targeting specific substantive specializations, such as the University of Chicago Masters in Computational Analysis and Public Policy, or the new Columbia Masters in Political Analytics.

The book is also particularly well suited to the growing number of programs that offer data science as an area of concentration within existing programs, like Economics, Political Science, Statistics, and Sociology. Many such programs are emerging in an effort to capture student excitement about data science, but sometimes struggle to fully integrate training in computational methods with substantive questions of interest. This book's emphasis on developing critical reasoning and problem-solving skills at precisely the place where the technical meets the substantive makes it ideally suited for such programs. 

Because the book assumes familiarity with the basics of statistical inference and machine learning, it fits best in the middle or towards the end of a Masters degree or undergraduate major. For example, it is an excellent resource to pair with a capstone class, or a class on project development.

## Young Professionals

The book is also designed to be of interest to young data scientists in industry. Because most assessments (exercises and exams) in data science programs are focused on technical competence, and many data science hiring processes are organized around technical interviews, students frequently get the impression that technical skills are the only skills required to be successful. As a result, it is only when students graduate that they come to realize they are inadequately prepared to effectively work with stakeholders or design and iterate problems.

# 3. About The Book

Few fields have shown as much promise to address the world's problems as data science. Today, data science is improving our understanding of and adaptation to climate change. It is being used in medicine to speed drug discovery, improve the quality of X-rays and MRIs, and ensure that patients receive appropriate medical care. It is used in courtrooms to fight for fair elections and electoral maps and by data journalists to document and communicate the injustices prevalent in our criminal justice system and issues in policing.

Data science also enables new technologies that may improve our lives. Autonomous drones are delivering blood and medical supplies to rural health clinics from Rwanda to [North Carolina](https://www.theverge.com/2020/5/27/21270351/zipline-drones-novant-health-medical-center-hospital-supplies-ppe/), and driver-aid features continue to make progress in reducing the over 30,000 traffic deaths and millions of injuries that occur in the US alone every year. And nearly every facet of business — from the way businesses source materials and manage inventory to the way product offerings respond to customer behavior — has been reshaped by data science.

At the same time, businesses and regulators are also coming to appreciate the potential of data science tools to reinforce racial and gender inequities. Algorithms at Amazon have been found to [discriminate against female job applicants](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G). Medical algorithms have been found to prioritize White patients over Black patients [for kidney transplants](https://www.wired.com/story/how-algorithm-blocked-kidney-transplants-black-patients/) and [preventative care](https://www.washingtonpost.com/health/2019/10/24/racial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients/). In the criminal justice system, algorithms have been found to [incorrectly identify Black defendants than White defendants as being a "danger to society" when providing risk assessments to judges deciding on pre-trial release, bail and sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). And even Meta's own research has shown its algorithms drive political polarization and division among users, and push users into extremist groups.

How, then, should a burgeoning data scientist approach this discipline, full of such promise and peril? Why have so many data science endeavors failed to deliver on their promise? And why do we need *yet another* data science book?

## This Book

This book is different from many other data science books you may have read. Where most data science books are designed to teach specific data science techniques or methods, the aim of this book is to provide you with a framework for thinking about your goals and how to achieve them using data science. It is, in a sense, about everything you need to know *beyond* the technicalities of model fitting. This is about everything that comes *before* and *after* you fit your model: it will help you work with stakeholders to clearly articulate the problem they want to address, formulate questions whose answers will help address your stakeholder's problem, choose an appropriate tool based on the question you seek to answer, and, critically, evaluate and refine your model based on your stakeholders needs.

The importance of these skills is often underestimated by data science students, and for understandable reasons. Data science curricula usually begin with coding, statistics, and model evaluation techniques. As a result, the hardest part of data science classes is often mastering the technical details of model implementation. Moreover, the limited time available to instructors and the need to support full classes of students means data science exercises almost always have to come with clear directions and problem scaffolding to ensure students meet their learning goals.

But real-world problems don't come with directions. Indeed, a problem that is clearly defined and for which a solution is obvious isn't a problem anyone will pay you very much to solve. No, classroom exercises are carefully structured to foster learning and to make it possible for instructors to grade and provide feedback at scale. But real problems — the kind you will encounter in industry, government, or research — are hard to even articulate clearly, never mind solve. And that is why, as we will see, what really sets exceptional professional data scientists apart is not their ability to get a high AUC — **it's their ability to navigate and thrive in the face of ambiguous problems and goals.**

## Four Big Ideas

This book is organized around four big ideas:

1. **Data science is about solving problems.** 

All too often, young data scientists get lost in the technical details of models and lose sight of the bigger picture. Data science is not about maximizing accuracy — it's about using data and quantitative methods to solve problems, and at the end of the day the only "metric" that matters is whether your work has helped solve the problem you set out to address.

To some readers, this idea may seem self-evident and uncontroversial. In my experience as an instructor, however, this is not a natural perspective for students. On one level they recognize that data science is generally meant to accomplish a goal — though when asked about the goal of a data scientist they often offer more generic answers like "generate insights from data" or "make recommendations," but then struggle to explain what makes something an insight, or on what basis one would make a recommendation. This lack of attention to project motivation and design is often further reinforced by classroom projects organized around instructor-provided, clearly articulated goals. And even when students are left free to choose their own topics, those topics are often backwards designed from available datasets and are rarely the subject of faculty feedback.  

That is not the case in this book. Time and again, students are pushed to remember that difficult questions — like how to weigh trade-offs in loss-function specifications — must always be answered *in the context of the problem you are trying to solve.*

The importance of problem articulation is also the subject of the first two substantive chapters of the book. **Chapter 3: Solving the Right Problem** addresses head-on the importance of clearly articulating your problem in an actionable manner and illustrate through examples what happens if you *don't* articulate your problem correctly. It provides concrete suggestions for how to reframe problems, and examples of how reframing of problems has successfully helped simplify apparently intractable problems in the past. 

Then in **Chapter 4: Stakeholder Management**, the book steps back from the unrealistic simplification that the problem is the *reader's* problem, and instead acknowledges that most data scientists are working *for* a stakeholder. This chapter discusses the delicate art of working with stakeholders to refine your mutual understanding of the problem you are trying to solve. Crucially, this chapter discusses the importance of being respectful of the stakeholder and their domain expertise while also not being *overly* deferential and assuming the stakeholder always knows best — and problem I often see with young data scientists.

2. **Data scientists solve problems by answering questions.** 

The second big idea of the book is that data scientists solve problems by answering questions about the world. Given that, we can reframe the challenge of a data scientist from the more amorphous task of "figuring out how to solve the problem" to the more concrete "what question, if answered, would make it easier to solve this problem?" Moreover, once we've articulated a question to answer, we can turn to choosing the best tool for generating an answer to that specific type of question. And it is only at this stage—not at the beginning!—that we start thinking about what statistical method, algorithm, or model is most appropriate.

This idea emerges from the fact that all data science tools, at their core, are tools for answering questions about the world, and that understanding data science tools through the lens of "what question is this tool answering?" is extremely powerful.

In the case of some tools — like clustering algorithms or other unsupervised machine learning tools — this idea is relatively uncontroversial. A clustering algorithms answer some form of the question "if I wanted to partition the observations in this data to maximize the similarity of points within each cluster (in terms of a specified set of variables and a similarity metric) and minimize similarity between clusters, how would I do so?" 

In other contexts — like in supervised machine learning — the relevance of this perspective can seem less relevant (or potentially just pedantic). But let's take the example of a supervised machine learning algorithm designed to classify mammograms as normal or abnormal trained on data labelled by radiologists at a Boston hospital, an example I use in the book. What question is this model answering? It is *not* answering the question, for any input mammogram, "Is this mammogram normal or abnormal?" Rather, in a very real sense it is answering the question "if this mammogram were shown to one of the Boston radiologists how labelled the training data, how likely are *they* to label the mammogram as normal or abnormal?" And while we hope the answer to those questions is similar, being explicit about the distinction makes clear how human biases and tendencies end up being replicated in our models.

For experienced data scientists, this idea is, by this point, second nature. But in my experience, reframing what predictive models are doing in this manner to students very often causes a "light bulb" moment of realization.

3. **The questions data scientists answer can be divided into three categories: exploratory, passive predictive, and causal.**

This third big idea is where this book departs more substantially from just presenting existing ideas in what I hope is a particularly effective manner and begins to lay out some new concepts. In particular, the book presents students with a novel taxonomy of question types organized around questions' substantive purposes, not intellectual tradition from which they are drawn or the computational machinery being deployed. 

**Exploratory Questions** are questions about patterns and regularities in the world around us. Answering Exploratory Questions helps data scientists better understand the landscape of the problem they wish to solve — for example, by helping them understand where the problem is most acute, or what factors seem most strongly correlated with problem intensity — and often aid in prioritizing subsequent efforts. 

Answering Exploratory Questions is largely the domain of statistical inference and unsupervised machine learning. However, as I emphasize to students, many of the tools and approaches to data science we encounter are artifacts of how university departments are organized, and our substantive goals as data scientists will not always follow the lines we draw between computer science, statistics, and social science departments. 

Answering Exploratory Questions is **not** synonymous with "Exploratory Data Analysis" (EDA). As it is commonly understood and practiced by students, EDA refers to the process of poking around in a new data set before fitting a more complicated statistical model. It entails learning what variables are present, how they are coded, and *sometimes* looking at general patterns in the data prior to model fitting. Crucially, it is generally defined by what it is *not* — it is what you do *before* you fit a complicated model — and by the tools used — EDA consists of plotting distributions or cross-tabulations, not fitting models or doing more sophisticated analyses.

Answering Exploratory Questions, by contrast, is about achieving a substantive goal — learning about patterns *in the world* — not the tools used to do it, or what it comes before. Answering an important Exploratory Question may require you to actively seek out new data, merge data from different sources together, and potentially do novel data collection. It may also entail model fitting or use of unsupervised machine learning algorithms to uncover latent patterns.

Indeed, the book allocates a [full sub-chapter](https://ds4humans.com/30_questions/07_eda.html) to discussion of the conceptual problems with how the term EDA is commonly used. 

**Passive Prediction Questions** are questions about the unobserved outcomes of individual entities (people, stocks, stores, etc.). Because Passive Prediction Questions are questions about individual entities, they don't necessarily have one "big" answer. Rather, Passive Prediction Questions are answered by fitting or training a model that can take the characteristics of an individual entity as inputs (e.g., this patient is age 67, has blood pressure of 160/90, and no history of heart disease) and spitting out an answer *for that individual* (given that, her probability of surgical complications is 82%). This differentiates Passive Prediction Questions from Exploratory Questions, which are about global patterns, not individual level predictions.

Passive Prediction Questions are usually deployed for one of two business purposes: 

1) identifying individual entities of particular interest (high-risk patients, high-value clients, factory machinery in need of preventative maintenance, etc.), and
2) automating classification or labeling tasks currently performed by people (reading mammograms, reviewing job applicant resumes, identifying internet posts that violate terms of use).

In general, these two business purposes correspond to the two types of "predictions" being made. Identifying individual entities of particular interest is generally accomplished by creating predictions about what future outcomes are likely to obtain absent intervention. "Given this new customer's behavior on my website, are they likely to spend a lot over the next year?"

This ability to make predictions about future outcomes is obviously of tremendous use to stakeholders as it allows them to tailor their approach at the individual level. A hospital that can predict which patients are most likely to experience complications after surgery can allocate their follow-up care resources accordingly. A business that knows which customers are more likely to be big spenders can be sure that those customers are given priority by customer care specialists.

But the meaning of the term "Prediction" in Passive Prediction Questions extends beyond "predicting the future". Passive Prediction Questions also encompass efforts to predict how a third party *would* behave or interpret something about an individual if given the chance. Here we can return to the example of an ML model for reading mammograms. Suppose a hospital stakeholder wanted to automate the reading of mammograms so that rural hospitals without full-time radiologists could give patients diagnoses more quickly (or, more cynically, pay fewer radiologists). 
They could train a model by feeding it a dataset of mammograms labelled by human radiologists. That model would then effectively be answering the question: "if a radiologist looked at this particular scan, would they conclude it is abnormal?"

The value of this type of prediction to stakeholders is likely also self-evident, as it opens the door for automation and scaling of tasks that would otherwise be too costly or difficult for humans. Indeed, answering this question is the type of task for which machine learning has become most famous. Spam filtering amounts to answering the question "If the user saw this email, would they tag it as spam?" Automated content moderation amounts to answering "Would a Meta contractor conclude the content of this photo violates Facebook's Community Guidelines?" Indeed, even Large Language Models (LLMs) like chatGPT, Bard, and LLaMA can be understood in this way, as we will discuss later.

Again, this framing of what supervised machine learning models do as answering this type of question may feel slightly pedantic to some readers, but in my experience it helps students understand how these types of models inherit the limitations of their training data, and open the door to discussion of issues of misalignment which I also address.

Finally, **Causal Questions** are questions about the likely consequences of actions the stakeholder. Causal Questions arise when stakeholders want to *do* something — buy a Superbowl ad, change how the recommendation engine in their app works, authorize a new prescription drug — but they fear the action they are considering may be costly and not actually work. In these situations, stakeholders will often turn to a data scientist in the hope that the scientist can provide greater certainty about the likely consequences of different courses of action before the stakeholder is forced to act at scale. This, in turn, helps to reduce the risk the stakeholder has to bear when making their decision — something all stakeholders appreciate.

By emphasizing that Causal Questions in an applied data scientist's career are generally motivated by a desire to understand an action the stakeholder is considering taking, this book is able to discuss concepts of external validity much more concretely than in many other texts on causal inference which — in my experience — tend to be motivated by social science questions in which there is no clear, specific intervention in mind for which external validity becomes a first-order concern. 

4. **Reasoning rigorously about uncertainty and errors is what differentiates good data scientists from great data scientists.** Data science isn't just about minimizing classification errors and uncertainty — it's also about deciding how unavoidable errors should be distributed, how to weigh the risks and trade-offs inherent in probabilistic decision-making rigorously and in a manner that takes into account the problem you are trying to solve, and to take uncertainty into account when acting on data.

Here again this books emphasis on starting with a motivating problem aids dramatically in its ability to keep discussion of errors and trade-offs when making decisions under uncertainty concrete. Whether in discussion of the problem with p-values, the distinction between substantive and statistical significance, or in discussing custom loss functions when doing classification, a continual emphasis on how to think about these issues *in the context of the problem you are seeking to solve* helps keep things concrete.

## Table of Contents

- **Chapter 1: Introduction:** An overview of the philosophy of the book and an introduction to the taxonomy of questions used throughout the book.

**Part 1: Solving Problems**

- **Chapter 2: Solving the *Right* Problem: The Importance of Problem Articulation:** When starting a new project, there is always a temptation to dive into the data. After all, few things in data science feel quite as satisfying as successfully fitting a model. But when you fail to stop and ensure you clearly understand the problem you are trying to solve, it is easy to spend large amounts of energy creating technically impressive results that, in the end, in no way solve a problem or improve the lives of anyone involved.
- **Chapter 3: Stakeholder Management:** Most data scientists are not actually employed to solve their own problems; they are employed to help solve the problem of a stakeholder. The chapter discusses the art of "stakeholder management" — working *with* a stakeholder to create a clear problem statement and plan.

**Part 2: Solving Problems Using Questions**

*Part 2 details how data scientists use different types of data science questions to solve problems.* 

- **Chapter 5: Descriptive v. Prescriptive Questions:** Before discussing each of the three types of questions introduced in the introduction in detail, this chapter takes one step back to introduce the distinction between descriptive questions (questions about how the world *is*) and prescriptive questions (questions about how the world *should* be). The main goal of the chapter is to ensure data scientists recognize when the questions they are answering entail value judgments.
- **Chapter 6: Using Exploratory Questions:** Often the most underappreciated type of question, Exploratory Questions play a critical role in helping data scientists ensure they putting their energy in the right places. Exploratory Questions help data scientists better understand the contours of the problem they wish to solve, such as where their problem is most acute (and thus where energy is likely to be best spent addressing the problem), or where the problem may have already been solved (and thus where they may look for inspiration). This chapter also includes an [extended discussion](https://ds4humans.com/30_questions/07_eda.html) on the distinction between the practice commonly referred to as "Exploratory Data Analysis (EDA)" and what is meant by Exploratory Questions.
- **Chapter 7: Using Passive Predictive Questions:** Passive Predictive Questions are the trendiest type of question in data science today. This chapter discusses the two major ways Passive Predictive Questions are used in business — identifying individual entities for additional attention and automation. It also dives deeply into the distinction between Exploratory Questions and Passive Predictive Questions. Students often struggle with the idea that the same tool — say, logistic regression — can be used to answer different types of questions, but that the *way* we use and evaluate model performance changes depending on our goals. Discussing how what matters most when answering Passive Prediction Questions (e.g., classification accuracy) may be different from what matters most when answering Exploratory Questions (e.g., the size of standard errors on regressors) helps students begin to recognize that there is no "one way" to use data science tools correctly; the correct way to use a tool depends on ones substantive goals.
- **Chapter 8: Using Causal Questions:** Causal Questions are some of the most powerful and yet difficult questions in data science to answer. Causal Questions most often arise when a stakeholder is interested in undertaking a high-stakes action, but wishes to de-risk that decision by modelling its likely outcome. This chapter discusses the nature of Causal Questions and what differentiates them from Passive Predictive Questions. It also then touches on the distinction between experimental and observational methods and when each is more appropriate.

**Part 3: Reasoning About Uncertainty and Errors**

*Part 3 is the meatiest portion of the book, and discusses the types of issues that arise when answering different types of questions. The emphasis of this Part is on the types of issues that tend not to be emphasized in introductory statistics or machine learning courses. This Part will therefore skip over topics like overfitting and model diagnostics, and instead focus on concepts like external validity, Goodhart's Law/Campbell's Law/The Lucas Critique, adversarial users, adverse selection in deployment, statistical decision-making, and customizing loss functions to suit the substantive context.*

- **Chapter 9: Internal and External Validity:** 
- **Chapter 10: Exploratory Question and Faithful Summarizations:**
- **Chapter 11: Passive Predictive Questions and The Right Way to be Wrong:**
- **Chapter 12: Causal Questions and The Challenges of Deployment:**

**Part 4: Data Science Professionalization**

- **Chapter 13: What *is* Data Science? A Historical Perspective:** Data science has yet to reach the level of maturation of fields like computer science, economics, or mechanical engineering. One consequence of this immaturity is that students often struggle to understand when differences in terminology are substantive and when they simply represent different intellectual traditions coming up with different names for the same phenomenon. In this chapter, I discuss the ways in which the organization of universities has shaped the sometimes myopic exposure to the field many practicing data scientists received in school and the fragmented language around data science. This, I argue, is important for young data scientists to know as it is both the source of some of the most exciting opportunities for intellectual arbitrage across bureaucratic divisions, and also some of the biggest challenges a holistically trained data scientist is likely to face when working with data scientists trained in a more traditional academic silo.
- **Chapter 14: Writing to Stakeholders:** 
- **Chapter 15: Providing Feedback:**


# 4. Competition

There are *many* books in the data science space, but relatively few that I see filling the niche I hope to serve. For example, O'Reilly Publishing has published many books on the topic of data science, but nearly all emphasize coding and modelling, not critical thinking at the interface of the technical and substantive.

The closest peer in terms of *content* is probably *The Art of Data Science* by Peng and Matsui (Lulu.com Publishing, 2016). Though short, the book aims to provide readers with a similar holistic perspective on the lifecycle of a data science project. To be entirely honest, I have avoided reading it *too* closely out of a desire to avoid letting it color my writing, but there are several distinctions between its approach and my own. 

First, it does not assume prior familiarity with statistical inference or machine learning. As a result, it is forced to operate at a more superficial level of analysis, and spends far more ink explaining basic concepts. Second, it was written in 2016, and as a result it is unable to engage with many of the more recent technical developments in data science (e.g., LLMs) or more recent case studies around topics like algorithmic bias.

# 5. Production Basics

At this time, I have completed a draft of the majority of the book manuscript. I have also had the opportunity to use the material in the book in class with MIDS students several times, allowing for refinement of the material. The current draft compiles to about 140 pages A4, and I imagine a final draft would be of similar length. The text has a fair number of figures, but I do not see color printing being a requirement. It seems like its most natural format would be as an A5 sized book.

In addition to publication of a physical book, I also this the book living online in a manner similar to an increasing number of books, like [Scott Cunningham's *Causal Mixtape*](https://mixtape.scunning.com/) or [Gelman, Hill, and Vehtari's *Regression and Other Stories*](https://avehtari.github.io/ROS-Examples/). Indeed, the project has mostly been developed in an online format, as can be seen at [https://ds4humans.com](https://ds4humans.com). I have primarily written the material in Markdown and compiled it to HTML using [Jupyter Book](https://jupyterbook.org/en/stable/intro.html), a system with also supports compilation to LaTeX. The current projects materials can be found [here](https://github.com/nickeubank/ds4humans), and a PDF build can be found [here](https://github.com/nickeubank/ds4humans/blob/main/_build/latex/ds4humans.pdf).

My hope is to work on tightening up the manuscript this summer, including filling in some missing sections, then use the refined manuscript in my spring course one last time for refinement before calling it "done." With that in my, my goal for completion would be early summer 2026.

# 6. Open Access

I am very interested in making the materials in this book open access, although I have not yet secured funding specifically for that purpose.

# 7. Supplementary Materials

Because I have been using this material in my course, I have also been developing exercises to go along with it, including in-class exercises, coding exercises, and project outlines. These are all materials I look forward to providing alongside the text.

# 8. About The Author

The interdisciplinary and eclectic nature of the book reflects my own background. As an undergraduate, I studied Economics and International Relations at Pomona College, then set out for what I thought was the start of a career as an empirical development economist by joining the Development Economic Research Group at the World Bank. After a few years analyzing data on the educational ecosystem in India and Pakistan, however, I became disillusioned with the approach of macroeconomics and turned my interest to the intersection of political science and economics. I did some additional work on the topic at the World Bank and with the Center for Global Development in Washington, DC, then started a PhD at Stanford. While completing my PhD in Political Economy from the Stanford Graduate School of Business and an MA in Economics, I discovered the power of data science to answer the questions I cared about and dove deeply into computer science and computer engineering. 

Around the time I completed my PhD, the focus of my work shifted away from international development issues for personal reasons. Instead, I turned my attention — and newfound data science skills — towards the US electoral system. Together with Jonathan Rodden, I developed new techniques for measuring the fairness of electoral districts and began to supplement my academic publishing with working as an Expert Consultant on voting rights and gerrymandering litigation.

Today, I am an Assistant Research Professor in Political Science at Duke University, the Faculty Director of the Masters of Interdisciplinary Data Science (MIDS) program, and Associate Director for the Rhodes Information Initiative @ Duke. My research has been published in top political science journals — including the *American Political Science Review*, *Political Analysis*, *Quarterly Journal of Political Science*, and *Political Science Research and Methods*. My work has been covered by *The Economist* and the *Washington Post*, and in an Op-Ed in *The Guardian*. And perhaps most importantly I have had the privilege to have worked on voting rights cases in Kansas, Arizona, Ohio, and North Carolina.
