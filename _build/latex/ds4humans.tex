%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Class Schedule}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Solving Problems With Data}
\date{Mar 01, 2025}
\release{}
\author{Nick Eubank}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{landing_page::doc}}


\sphinxAtStartPar
This website is both the course site for the Duke Interdisciplinary Data Science Course \sphinxstyleemphasis{Solving Problems with Data} (IDS 701), as well as the beginning of what I hope will eventually become a stand\sphinxhyphen{}alone textbook by \sphinxhref{https://www.nickeubank.com}{Nick Eubank}.

\sphinxAtStartPar
\sphinxstylestrong{If you \sphinxstyleemphasis{aren’t} a Duke IDS 701 student} and wish to explore the content of the course, I’d suggest starting with the {\hyperref[\detokenize{10_introduction/10_solving_problems_with_data::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Introduction chapter here}}}}. You’re also welcome to browse the class schedule of topics covered in the class in the {\hyperref[\detokenize{00_class_schedule/class_schedule::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{class schedule}}}} link, or hop down to the readings in \sphinxstylestrong{Data Science in Practice} towards the bottom of the left\sphinxhyphen{}hand navigation menu for a few stand\sphinxhyphen{}alone readings on various professionalization topics, like {\hyperref[\detokenize{40_in_practice/25_writing_to_stakeholders::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Writing to Stakeholders}}}} or {\hyperref[\detokenize{40_in_practice/30_giving_feedback::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Providing Feedback}}}}.

\sphinxAtStartPar
If you \sphinxstyleemphasis{are} a MIDS student, here’s a little more about the course.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Solving Problems with Data (IDS 701)}
\end{DUlineblock}

\sphinxAtStartPar
Few fields have shown as much promise to address the world’s problems as data science. At the same time, however, recent years have also made clear that today’s global challenges will not be met by simply “throwing data science at the problem” and hoping things will work out. Even in business, where many assume that Artificial Intelligence is a sure ticket to profits, \sphinxhref{https://www.wired.com/story/companies-rushing-use-ai-few-see-payoff/}{a major recent study found only > 11\%} of businesses that had piloted or employed Artificial Intelligence had reaped a sizeable return on their AI investments.

\sphinxAtStartPar
How, then, should a burgeoning data scientist approach this field full of such promise but also so many pitfalls? And why have so many data science endeavors failed to deliver on their promise?

\sphinxAtStartPar
The answer lies — at least in significant part — in a failure to provide students with a systematic approach to bringing the techniques learned in statistical modeling and machine learning courses to bear on real\sphinxhyphen{}world problems. Data science curricula usually begin with coding, statistics, and model evaluation techniques. All too often, however, that’s where they stop. But while the hardest part of data science \sphinxstyleemphasis{classes} is often fitting a model well or getting a good AUC score, the hardest part of being an effective \sphinxstyleemphasis{professional} data scientist is ensuring that the models being fit and the results being interpreted actually solve the problem that motivated you (or your stakeholder) in the first place.

\sphinxAtStartPar
This class is designed to fill this gap. Through exercises, case studies, and projects, students will develop a \sphinxstyleemphasis{systematic} understanding of how to approach and manage data science projects from conception through delivery and adoption. It will provide a unified perspective on how the perspectives and tools learned in other courses complement one another, and \sphinxstyleemphasis{when} different approaches to data science are most appropriate.

\sphinxAtStartPar
In addition, this course will also provide an in\sphinxhyphen{}depth introduction into \sphinxstyleemphasis{causal inference} — the practice of answering causal questions. Given the interests of MIDS students, this introduction will focus heavily on experiments and A/B testing, but will also cover the use of observational data (data that did not come from an experiment that employed random assignment to treatment) for answering causal questions.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large How to Succeed in this Class}
\end{DUlineblock}

\sphinxAtStartPar
In Duke course reviews, students are asked, “What would you like to say about this course to a student who is considering taking this course in the future?”

\sphinxAtStartPar
\sphinxstyleemphasis{By far} the most consistent thing past students say they would like to tell a student considering taking it in the future is to \sphinxstylestrong{do the readings and take them seriously.}

\sphinxAtStartPar
There is a tendency among data science students — especially those from a STEM background — to assume that readings that don’t have a lot of math aren’t “serious,” and consequently don’t require substantial focus. That’s a mistake. This course is about the critical reasoning required to make the leap from the relatively clean math of statistics and machine learning to the messiness of real world problems. To help you learn how to do so, the readings are full of examples, ways to think about problems, and problem\sphinxhyphen{}solving frameworks to help you cross that wobbly bridge from the clean world of problem sets to the real, under\sphinxhyphen{} or mis\sphinxhyphen{}defined problems you will face when you enter the work force. But with this type of material, what you get out of it depends on what you put into it, and unlike with a theorem — which you either follow or you don’t — thinking critically happens on many levels. So while it’s easy to skim a reading and — because you weren’t confused by any greek notation — assume you internalized it, succeeding in this class requires actively wrestling with the material, not just letting your eyes glide over it.

\sphinxAtStartPar
In other words, \sphinxstylestrong{take the readings for this course just as seriously as the exercises.} There is as much learning to be done by thinking deeply about the readings as there is to be gained from doing the exercises, a fact that is also reflected in how the course is graded (individual or two\sphinxhyphen{}person exercises make up 20\% of your grade, while reading comprehension quizzes and the midterm count for 20\% each).

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Big Ideas}
\end{DUlineblock}

\sphinxAtStartPar
This course is organized around three big ideas:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data science is about solving problems.} All too often, data scientists get lost in the technical details of models and lose sight of the bigger picture. Data science is not about maximizing accuracy or AUC scores — it’s about using data and quantitative methods to solve problems, and at the end of the day the only “metric” that matters is whether your work has solved the problem you set out to address.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data scientists solve problems by answering questions, and the question you are asking determines what tool is appropriate.} At their core, all data science tools are tools for answering questions, whether you realize it or not. Learning to recognize how data scientists use questions to solve problems — and exactly what questions are being answered by the tools you use every day — is key to navigating the ambiguity of real\sphinxhyphen{}world problem solving.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reasoning rigorously about uncertainty and errors is what differentiates good data scientists from great data scientists.} Data science isn’t just about minimizing classification errors and uncertainty — it’s also about deciding how unavoidable errors should be distributed, and how to weigh the risks and trade\sphinxhyphen{}offs inherent in probabilistic decision\sphinxhyphen{}making rigorously and in a manner that takes into account the problem you are trying to solve.

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Pre\sphinxhyphen{}Requisites for Non\sphinxhyphen{}MIDS Students}
\end{DUlineblock}

\sphinxAtStartPar
This course is primarily designed for students in the Duke Masters in Interdisciplinary Data Science (MIDS) program, but students from other programs are more than welcome if they have the appropriate pre\sphinxhyphen{}requisite training. Data Science is a fundamentally interdisciplinary field, so the more perspectives we have represented in the classroom the better!

\sphinxAtStartPar
This course will assume that enrolled students have a good grasp of inferential statistics and statistical modeling (e.g. a course in linear models), though no prior experience with causal inference is expected. In addition, MIDS students will be taking a concurrent course in applied machine learning, so incoming students will also be expected to have some basic experience with machine learning or be concurrently enrolled in an applied machine learning course.

\sphinxAtStartPar
This course will also assume students are comfortable manipulating real\sphinxhyphen{}world data in Python. The substantive content of this course is language\sphinxhyphen{}independent, but because students will be required to work on their projects in teams, comfort with Python will be required to facilitate collaboration (MIDS students are, generally, “bilingual” in R and Python, but have a strong preference for Python, and it’s hard to write problem sets to accommodate multiple languages).

\sphinxAtStartPar
Finally, students will also be expected to be comfortable collaborating using git and github. If you meet the other requirements for this course but are not familiar with git and github, this is a skill you should be able to pick up on your own in advance of the course without too much difficulty. You can read more about \sphinxhref{https://www.practicaldatascience.org/ids720\_specific/exercises/Exercise\_git.html}{git and github here}. The \sphinxhref{https://library.duke.edu/data}{Duke Center for Data and Visualization Science} also hosts git and github workshops if you are a Duke student.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Course Schedule}
\end{DUlineblock}

\sphinxAtStartPar
You can find the {\hyperref[\detokenize{00_class_schedule/class_schedule::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{course schedule here}}}}. Note that our schedule is subject to change, but this should give you a good sense of the material we will cover.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Syllabus}
\end{DUlineblock}

\sphinxAtStartPar
To learn more about the course, please \sphinxhref{https://github.com/nickeubank/unifyingdatascience/raw/master/syllabus/Syllabus\_UnifyingDataScience.pdf}{read the course syllabus here.}.

\sphinxstepscope


\part{Class Schedule}

\sphinxstepscope


\chapter{Class Schedule}
\label{\detokenize{00_class_schedule/class_schedule:class-schedule}}\label{\detokenize{00_class_schedule/class_schedule::doc}}
\sphinxAtStartPar
Class: Tuesday / Thursday, 1:25\sphinxhyphen{}2:40pm

\sphinxAtStartPar
Office Hours:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Nick: Thursday, 9\sphinxhyphen{}10am, \sphinxurl{https://duke.zoom.us/my/nickeubank} or Gross Hall 231

\item {} 
\sphinxAtStartPar
Katie: Tuesday 9\sphinxhyphen{}10am \sphinxurl{https://duke.zoom.us/j/7696790306}

\item {} 
\sphinxAtStartPar
Keon: Wednesday 3\sphinxhyphen{}4pm \sphinxurl{https://duke.zoom.us/j/97104029366}

\item {} 
\sphinxAtStartPar
Gunel: Thursday 3\sphinxhyphen{}4pm \sphinxurl{https://duke.zoom.us/j/95439693004}

\end{itemize}


\section{Texts Used}
\label{\detokenize{00_class_schedule/class_schedule:texts-used}}
\sphinxAtStartPar
This course will make use of readings from a handful of different sources. The main four, in order of the amount they will be used, are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{DS4H}: \sphinxhref{https://ds4humans.com}{Data Science for Humans} (This website, free.)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Cunningham}: \sphinxhref{https://www.amazon.com/Causal-Inference-Mixtape-Scott-Cunningham/dp/0300251688}{Causal Inference Mixtape} (\$30 new, \$20 used. \sphinxhref{https://ebookcentral.proquest.com/lib/duke/detail.action?docID=6425560}{Online Access through Duke Library})

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{KTX}: \sphinxhref{https://www.amazon.com/gp/product/1108724264/}{Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing} (\$40 new, \$30 used.)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GHV}: \sphinxhref{https://www.amazon.com/Regression-Stories-Analytical-Methods-Research/dp/1107676517}{Regression and Other Stories} (\$40. Too new to have used versions.)

\end{itemize}


\begin{savenotes}
\sphinxatlongtablestart
\sphinxthistablewithglobalstyle
\makeatletter
  \LTleft \@totalleftmargin plus1fill
  \LTright\dimexpr\columnwidth-\@totalleftmargin-\linewidth\relax plus1fill
\makeatother
\begin{longtable}{\X{4}{51}\X{15}{51}\X{25}{51}\X{7}{51}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Date
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Topic
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Do Before Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
In Class
\\
\sphinxmidrule
\endfirsthead

\multicolumn{4}{c}{\sphinxnorowcolor
    \makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} \textendash{} continued from previous page}}%
}\\
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Date
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Topic
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Do Before Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
In Class
\\
\sphinxmidrule
\endhead

\sphinxbottomrule
\multicolumn{4}{r}{\sphinxnorowcolor
    \makebox[0pt][r]{\sphinxtablecontinued{continues on next page}}%
}\\
\endfoot

\endlastfoot
\sphinxtableatstartofbodyhook

\sphinxAtStartPar
Th Jan 9
&
\sphinxAtStartPar
Course Overview
&
\sphinxAtStartPar
Nothing
&
\sphinxAtStartPar
Discuss logistics and structure
\\
\sphinxhline
\sphinxAtStartPar
Tu Jan 14
&
\sphinxAtStartPar
Solving Problems by Answering Questions
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/nickeubank/ds4humans/raw/refs/heads/main/98\_syllabus/Syllabus\_IDS701.pdf?download=}{Read, sign, submit syllabus on gradescope.}

\item {} 
\sphinxAtStartPar
\sphinxhref{../40\_in\_practice/00\_how\_to\_read\_this\_book.html}{How to Read}

\item {} \begin{description}
\sphinxlineitem{\sphinxhref{../10\_introduction/10\_solving\_problems\_with\_data.html}{Chpt 1} }\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Whenever I list a Chpt number, you may assume it includes all subparts unless I say otherwise.} So this includes all three big idea subsections.

\end{itemize}

\end{description}

\end{itemize}
&
\sphinxAtStartPar
\sphinxhref{../99\_exercises/exercise\_class2\_probs\_and\_questions.html}{Questions Ex}
\\
\sphinxhline
\sphinxAtStartPar
Th Jan 16
&
\sphinxAtStartPar
Stakeholder Management
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../20\_problems\_to\_questions/10\_solving\_the\_right\_problem.html}{Chpt 3}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.youtube.com/watch?v=kYMfE9u-lMo}{Why It’s Important To Get Your Question Right (30 min video)}

\item {} 
\sphinxAtStartPar
\sphinxhref{../20\_problems\_to\_questions/20\_stakeholder\_management.html}{Chpt 4}

\end{itemize}
&
\sphinxAtStartPar
\sphinxhref{../99\_exercises/exercise\_stakeholder\_management.html}{Stakeholder Ex}
\\
\sphinxhline
\sphinxAtStartPar
Tu Jan 21
&
\sphinxAtStartPar
Prescriptive v. Descriptive Questions
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../10\_introduction/40\_data\_science\_in\_historical\_context.html}{Chpt 2}

\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/05\_descriptive\_v\_prescriptive.html}{Chpt 5}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Th Jan 23
&
\sphinxAtStartPar
Exploratory Questions: Purpose, Internal and External Validity
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/10\_using\_exploratory\_questions.html}{Chpt 6}

\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/15\_answering\_exploratory\_questions.html}{Chpt 7}

\end{itemize}
&
\sphinxAtStartPar
\sphinxhref{../97\_project/00\_problem\_and\_exploratory.html}{Team Project}
\\
\sphinxhline
\sphinxAtStartPar
Tu Jan 28
&
\sphinxAtStartPar
Teams 1
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/nickeubank/unifyingdatascience/raw/master/team\_readings/project\_aristotle\_nytimes.pdf}{What Project Aristotle Learned}

\item {} 
\sphinxAtStartPar
Edmondson, The Fearless Organization, Chpt 1 (on Canvas)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Problem and Questions Due}

\end{itemize}
&
\sphinxAtStartPar
\sphinxhref{../99\_exercises/exercise\_teams1.html}{Debrief Ex}
\\
\sphinxhline
\sphinxAtStartPar
Th Jan 30
&
\sphinxAtStartPar
Teams 2
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/nickeubank/unifyingdatascience/raw/master/team\_readings/MIDS\%20Team\%20Charter\%20Assignment.docx}{Review Team Charter Assignment}

\item {} 
\sphinxAtStartPar
Edmondson, Teaming, Chpt 2 (on Canvas)

\end{itemize}
\begin{quote}

\sphinxAtStartPar
Optional: \sphinxhref{https://docs.google.com/document/d/1PsnDMS2emcPLgMLFAQCXZjO7C4j2hJ7znOq\_g2Zkjgk/export?format=pdf}{Fostering Psychological Safety Tips}
\end{quote}
&
\sphinxAtStartPar
\sphinxhref{../97\_project/10\_exploratory\_report.html}{Team Report}
\\
\sphinxhline
\sphinxAtStartPar
Tu Feb 4
&
\sphinxAtStartPar
Exploratory Questions: Internal Validity Concerns
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/16\_exploratory\_internal\_challenges.html}{Chpt 8}

\end{itemize}
&
\sphinxAtStartPar
\sphinxhref{../99\_exercises/exercise\_exploratory.html}{Exploratory Ex}
\\
\sphinxhline
\sphinxAtStartPar
Th Feb 6
&
\sphinxAtStartPar
Passive Prediction Questions: Purpose, Internal Validity 1
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/20\_using\_passive\_prediction\_questions.html}{Chpt 9}

\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/23\_passive\_internal\_alignment\_and\_bias.html}{Chpt 10}

\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/24\_passive\_internal\_errors.html}{Chpt 11} ONLY FIRST READING, not Fairness

\end{itemize}
&
\sphinxAtStartPar
\sphinxhref{../99\_exercises/exercise\_passive\_prediction.html}{Passive P Ex}
\\
\sphinxhline
\sphinxAtStartPar
Tu Feb 11
&
\sphinxAtStartPar
Passive Prediction Questions: Internal Validity 2
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/25\_passive\_fairness.html}{Chpt 11 Fairness Section}

\end{itemize}
&
\sphinxAtStartPar
\sphinxhref{../99\_exercises/exercise\_used\_car.html}{Car Buyer}
\\
\sphinxhline
\sphinxAtStartPar
Th Feb 13
&
\sphinxAtStartPar
Passive Prediction Questions: External Validity
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/27\_passive\_external\_general.html}{Chpt 12}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Exploratory Report Due}

\end{itemize}
&
\sphinxAtStartPar
\sphinxhref{../97\_project/20\_feedback.html}{Feedback}
\\
\sphinxhline
\sphinxAtStartPar
Tu Feb 18
&
\sphinxAtStartPar
Passive Prediction Questions: Interpretability
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/32\_passive\_interpretable\_models.html}{Chpt 13}

\item {} 
\sphinxAtStartPar
\sphinxhref{../40\_in\_practice/30\_giving\_feedback.html}{Giving Feedback}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Feedback Memos Should Be Shared by Midnight on 18th}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.youtube.com/watch?v=Z8MEFI7ZJlA}{Optional but fun!}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Th Feb 20
&
\sphinxAtStartPar
SNOW DAY
&&\\
\sphinxhline
\sphinxAtStartPar
Tu Feb 25
&
\sphinxAtStartPar
Feedback Class
&\begin{itemize}
\item {} 
\sphinxAtStartPar
Read feedback provided to your team. Be prepared to discuss.

\end{itemize}
&\begin{itemize}
\item {} 
\sphinxAtStartPar
Discuss Feedback

\end{itemize}
\\
\sphinxhline
\sphinxAtStartPar
Th Feb 27
&
\sphinxAtStartPar
Causal Questions: Purpose
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/30\_using\_causal\_questions.html}{Using Causal Questions}

\item {} 
\sphinxAtStartPar
\sphinxhref{../30\_questions/40\_answering\_causal\_questions.html}{Answering Causal Questions}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Tu Mar 4
&
\sphinxAtStartPar
Causal Questions: Potential Outcomes
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../35\_causal/10\_potential\_outcomes.html}{Potential Outcomes Framework}

\end{itemize}
\begin{quote}
\begin{description}
\sphinxlineitem{Optional:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Read Chpt 4 in Cunningham from start (different presentation of potential outcomes).

\item {} 
\sphinxAtStartPar
Note that Cunningham uses the term Simple Difference in Outcomes (SDO) instead of \$widehat\{ATE\}\$ (the term I use). They are perfectly interchangable.

\end{itemize}

\end{description}
\end{quote}
&\\
\sphinxhline
\sphinxAtStartPar
Th Mar 6
&
\sphinxAtStartPar
Causal Questions: Potential Outcomes
&\begin{itemize}
\item {} 
\sphinxAtStartPar
Cunningham, Chpt 4, pp 135 (“Independence Assumption”) \sphinxhyphen{} 148 (Stop at “Randomization Inference”)

\item {} 
\sphinxAtStartPar
\sphinxhref{../35\_causal/30\_interpreting\_indicator\_vars.html}{Indicator Variables}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Tu Mar 11
&
\sphinxAtStartPar
\sphinxstylestrong{NO CLASS}
&&\\
\sphinxhline
\sphinxAtStartPar
Th Mar 13
&
\sphinxAtStartPar
\sphinxstylestrong{NO CLASS}
&&\\
\sphinxhline
\sphinxAtStartPar
Tu Mar 18
&
\sphinxAtStartPar
Causal Questions: Experiments in Practice, Internal Validity
&
\sphinxAtStartPar
Experiments: Internal Validity (In Practice):
\begin{itemize}
\item {} 
\sphinxAtStartPar
KTX: Chpt 2 (End to End Example)

\item {} 
\sphinxAtStartPar
KTX: Chpt 3, “Threats to Internal Validity” (p. 42\sphinxhyphen{}47)

\item {} 
\sphinxAtStartPar
KTX: Chpt 19 (A/A Testing)

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Th Mar 20
&
\sphinxAtStartPar
Causal Questions: Experiments in Practice, External Validity
&
\sphinxAtStartPar
Overall Evaluation Criteria: KTX Chpt 7.

\sphinxAtStartPar
Finishing Internal Validity:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Different Randomizations: KTX Chpt 22

\end{itemize}

\sphinxAtStartPar
Experiments: External Validity (In Practice):
\begin{itemize}
\item {} 
\sphinxAtStartPar
KTX, Chpt 3, “Threats to External Validity” to end (p. 47\sphinxhyphen{}54)

\item {} 
\sphinxAtStartPar
Kohvai, Tang and Xu, Chpt 23 (Primacy Effects / Long Term Decay)

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Tu Mar 25
&
\sphinxAtStartPar
Causal Questions: Experiments in Practice, Design
&
\sphinxAtStartPar
Designing Experiments
\begin{itemize}
\item {} 
\sphinxAtStartPar
Statistical Power and Sample Sizes: GHV Ch 16

\item {} 
\sphinxAtStartPar
\sphinxhref{endogenous\_stopping.ipynb}{Don’t stop experiments early!}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Th Mar 27
&
\sphinxAtStartPar
Causal Questions: Experiments in Practice, P\sphinxhyphen{}Value Interpretation
&\begin{itemize}
\item {} 
\sphinxAtStartPar
Statistical Decision Theory (on Canvas). 550\sphinxhyphen{}556

\end{itemize}

\sphinxAtStartPar
(This is same as IDS 705 Lecture 8 Reading)
&\\
\sphinxhline
\sphinxAtStartPar
Tu Apr 1
&
\sphinxAtStartPar
AB Testing Review
&&\\
\sphinxhline
\sphinxAtStartPar
Th Apr 3
&
\sphinxAtStartPar
\sphinxstylestrong{MIDTERM}
&
\sphinxAtStartPar
\sphinxstylestrong{MIDTERM}
&\\
\sphinxhline
\sphinxAtStartPar
Tu Apr 8
&
\sphinxAtStartPar
Causal Questions: Regression
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../35\_causal/40\_causal\_inference\_beyond\_ab\_testing.html}{Causal Beyond Experiments}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Th Apr 9
&
\sphinxAtStartPar
Causal Questions: Matching
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{../35\_causal/80\_matching\_why.html}{The Why of Matching}

\item {} 
\sphinxAtStartPar
\sphinxhref{../35\_causal/90\_matching\_how.html}{The How of Matching Summary}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://youtu.be/tvMyjDi4dyg?t=910}{Methods for Matching}

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Watch the video above from about 15 minutes in (where link starts) till at least 45 minutes in, keep going if you want to learn about propensity score matching problems.}
&\\
\sphinxhline
\sphinxAtStartPar
Tu Apr 15
&
\sphinxAtStartPar
Causal Questions: Differences and Differences / Panels
&\begin{itemize}
\item {} 
\sphinxAtStartPar
Cunningham, Chpt 9 pp 406 (Difference in Differences) \sphinxhyphen{} pp 433 (Stop at “Importance of Placebos in Diff\sphinxhyphen{}in\sphinxhyphen{}Diff”)

\end{itemize}
\begin{quote}

\sphinxAtStartPar
\sphinxhyphen{}(Book page numbers at bottom of PDF on canvas. Full chapter is in PDF, more than you need to read.)

\sphinxAtStartPar
Optional but encouraged:
(dont need to follow everything, but here’s a real diff\sphinxhyphen{}in\sphinxhyphen{}diff)
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.cambridge.org/core/journals/american-political-science-review/article/enfranchisement-and-incarceration-after-the-1965-voting-rights-act/C68FA7BB8CA313BDD8D9A39BA666A21D}{Enfranchisement and Incarceration}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://netflixtechblog.com/key-challenges-with-quasi-experiments-at-netflix-89b4f234b852}{Diff\sphinxhyphen{}in\sphinxhyphen{}Diffs at Netflix}

\end{itemize}
\end{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Revised Exploratory Due along with Causal Proposal}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Wed, April 23
&
\sphinxAtStartPar
x
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Optional Due Date for Causal Rough Draft (11pm)}

\end{itemize}
&
\sphinxAtStartPar
x
\\
\sphinxhline
\sphinxAtStartPar
Wed, Apr 30
&&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Final Causal Report Due}

\end{itemize}
&
\sphinxAtStartPar
x
\\
\sphinxbottomrule
\end{longtable}
\sphinxtableafterendhook
\sphinxatlongtableend
\end{savenotes}

\sphinxstepscope


\part{Introduction}

\sphinxstepscope


\chapter{Solving Problems with Data}
\label{\detokenize{10_introduction/10_solving_problems_with_data:solving-problems-with-data}}\label{\detokenize{10_introduction/10_solving_problems_with_data::doc}}
\sphinxAtStartPar
Few fields have shown as much promise to address the world’s problems as data science. Today, data science is improving our understanding of and adaptation to climate change. It is being used in medicine to speed drug discovery, improve the quality of X\sphinxhyphen{}rays and MRIs, and ensure that patients receive appropriate medical care. It is used in courtrooms to fight for fair elections and electoral maps and by data journalists to document and communicate the injustices prevalent in our criminal justice system and issues in policing.

\sphinxAtStartPar
Data science also enables new technologies that may improve our lives. Autonomous drones are delivering blood and medical supplies to rural health clinics from Rwanda to \sphinxhref{https://www.theverge.com/2020/5/27/21270351/zipline-drones-novant-health-medical-center-hospital-supplies-ppe/}{North Carolina}, and driver\sphinxhyphen{}aid features continue to make progress in reducing the over 30,000 traffic deaths and millions of injuries that occur in the US alone every year. And nearly every facet of business — from the way businesses source materials and manage inventory to the way product offerings respond to customer behavior — has been reshaped by data science.

\sphinxAtStartPar
At the same time, it is also increasingly clear that today’s challenges will not be met by “throwing data science at the problem” or “just adding AI.” According to a 2020 MIT/BCG survey, \sphinxhref{https://www.wired.com/story/companies-rushing-use-ai-few-see-payoff/}{only 11\% of businesses that had piloted or employed Artificial Intelligence (AI) had reaped a sizeable return on their AI investments.} Businesses and regulators are also coming to appreciate the potential of data science tools to reinforce racial and gender inequities. Algorithms at Amazon have been found to \sphinxhref{https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G}{discriminate against female job applicants}. Medical algorithms have been found to prioritize White patients over Black patients \sphinxhref{https://www.wired.com/story/how-algorithm-blocked-kidney-transplants-black-patients/}{for kidney transplants} and \sphinxhref{https://www.washingtonpost.com/health/2019/10/24/racial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients/}{preventative care}. In the criminal justice system, algorithms have been found to \sphinxhref{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}{incorrectly identify Black defendants than White defendants as being a “danger to society” when providing risk assessments to judges deciding on pre\sphinxhyphen{}trial release, bail and sentencing}. And even Meta’s own research has shown its algorithms drive political polarization and division among users, and push users into extremist groups.%
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxhref{https://www.wsj.com/articles/facebook-knows-it-encourages-division-top-executives-nixed-solutions-11590507499}{Recent reporting by the Wall Street Journal} has shown that Facebook’s own research has confirmed what many outside experts have long argued: the way its recommendation engines prioritize content that results in “user engagement” (clicks, shares, comments) ends up promoting partisan, polarizing, sensationalist, or extreme content. In addition, their own research has also shown that group recommendations are contributing to extremism. According to one internal presentation, “64\% of all extremist group joins are due to our recommendation tools” like \sphinxstyleemphasis{Groups You Should Join} and other discovery tools.
%
\end{footnote}

\sphinxAtStartPar
How, then, should a burgeoning data scientist approach this discipline, full of such promise and peril? And why have so many data science endeavors failed to deliver on their promise?


\section{The Three Big Ideas}
\label{\detokenize{10_introduction/10_solving_problems_with_data:the-three-big-ideas}}
\sphinxAtStartPar
This book is my answer to these questions, and it is organized around three big ideas:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data science is about solving problems.} All too often, data scientists get lost in the technical details of models and lose sight of the bigger picture. Data science is not about maximizing accuracy or AUC scores — it’s about using data and quantitative methods to solve problems, and at the end of the day the only “metric” that matters is whether your work has solved the problem you set out to address.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data scientists solve problems by answering questions, and the question you are asking determines what tool is appropriate.} At their core, all data science tools are tools for answering questions, whether you realize it or not. Learning to recognize how data scientists use questions to solve problems — and exactly what questions are being answered by the tools you use every day — is key to navigating the ambiguity of real\sphinxhyphen{}world problem solving.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reasoning rigorously about uncertainty and errors is what differentiates good data scientists from great data scientists.} Data science isn’t just about minimizing classification errors and uncertainty — it’s also about deciding how unavoidable errors should be distributed, how to weigh the risks and trade\sphinxhyphen{}offs inherent in probabilistic decision\sphinxhyphen{}making rigorously and in a manner that takes into account the problem you are trying to solve, and to take uncertainty into account when acting on data.

\end{enumerate}

\sphinxAtStartPar
From the summary of these three ideas, it should be immediately obvious that this book is different from many other data science books you may have read. Where most data science books are designed to teach specific data science techniques or methods, the aim of this book is to provide readers with a framework for thinking about their goals and how to achieve them using data science methods. It is, in a sense, about everything you need to know \sphinxstyleemphasis{beyond} the technicalities of model fitting. It is about what comes \sphinxstyleemphasis{before} and \sphinxstyleemphasis{after} you fit your model: it will help you decide what questions to answer, what data to collect, and what models to consider using \sphinxstyleemphasis{before} you actually fit your model, and it will help you learn to evaluate whether a result is likely to generalize, whether a model is safe to deploy, and how to communicate those facts to a stakeholder \sphinxstyleemphasis{after} model fitting.

\sphinxAtStartPar
The importance of these skills is often underestimated by data science students, and for understandable reasons. Data science curricula usually begin with coding, statistics, and model evaluation techniques. As a result, the hardest part of data science classes is often mastering the technical details of model implementation. Moreover, the limited time available to instructors and the need to support full classes of students means data science exercises almost always have to come with clear directions and problem scaffolding to ensure students meet their learning goals.

\sphinxAtStartPar
But real\sphinxhyphen{}world problems don’t come with directions. Indeed, a problem that is clearly defined and for which a solution is obvious isn’t a problem anyone will pay you very much to solve. No, classroom exercises are carefully structured to foster learning and to make it possible for instructors to grade and provide feedback at scale. But real problems — the kind you will encounter in industry, government, or research — are hard to even articulate clearly, never mind solve. And that is why, as we will see, what really sets exceptional professional data scientists apart is not their ability to get a high AUC — \sphinxstylestrong{it’s their ability to navigate and thrive in the face of ambiguous problems and goals.}


\section{This Is A Book About The Forest, Not The Trees}
\label{\detokenize{10_introduction/10_solving_problems_with_data:this-is-a-book-about-the-forest-not-the-trees}}
\sphinxAtStartPar
The goal of this book, to frame things a little differently, is to help young data scientists maintain perspective. Students spend so much time learning individual techniques that they are unable to see the forest for trees. But this is not a book about individual techniques — it’s about learning to think strategically about how to use those techniques to solve real problems.

\sphinxAtStartPar
To maintain its focus on the “forest,” this book \sphinxstyleemphasis{takes as given} that you have already been introduced to statistical inference and machine learning and know how to faithfully fit a model in a robust manner. That means that topics like hypothesis testing, cross\sphinxhyphen{}validation, how to use train\sphinxhyphen{}test splits, and how to evaluate a model’s AUC will be treated as assumed knowledge.%
\begin{footnote}[2]\sphinxAtStartFootnote
If you are a Duke student reading this, it’s ok if you are not yet familiar with all of these topics so long as you are taking a good machine learning course — like IDS 705 — concurrently.
%
\end{footnote} This is in no way meant to suggest these topics aren’t important — we will reference them constantly — just that I will not attempt to teach them here, both to maintain focus on the goals of this book, and also because there already exist many other resources that introduce these topics better than I could.


\section{Introduction Structure}
\label{\detokenize{10_introduction/10_solving_problems_with_data:introduction-structure}}
\sphinxAtStartPar
The remainder of this introductory chapter contains an overview of the Big 3 ideas of the book. All concepts discussed here will also be covered in greater detail in future readings, but before we dive into them in detail, it’s helpful to get a sense of the overall approach we will be taking!


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Idea 1: Solving Problems}
\label{\detokenize{10_introduction/20_solving_problems:idea-1-solving-problems}}\label{\detokenize{10_introduction/20_solving_problems::doc}}
\sphinxAtStartPar
Given the focus of data science coursework on the math, statistics, and programming that is required to use data science tools, one would be forgiven for thinking that understanding these tools is the ultimate goal of data science.

\sphinxAtStartPar
At the end of the day, however, your success as a data scientist will not be evaluated by whether you write good code or whether your classification model’s AUC score is high. No, your success as a data scientist will always be evaluated based on a much simpler criterion: \sphinxstylestrong{did you make your stakeholder’s life better by solving a problem they faced?}

\begin{sphinxShadowBox}

\sphinxAtStartPar
Your success as a data scientist will \sphinxstyleemphasis{always} be evaluated by one simple criterion: \sphinxstylestrong{did you make your stakeholder’s life better by solving a problem they faced?}
\end{sphinxShadowBox}

\sphinxAtStartPar
On the last page, I said that that one of the key goals of this book is to help young data scientist’s understand the broader context of their work. There is perhaps no better way to do that then to always keep this one fact front of mind. The things you practice in class exercises — writing good code and finding a classification model with a high AUC — are things that may \sphinxstyleemphasis{help} you achieve the goal of solving your stakeholder’s problem, but they are never sufficient in and of themselves. Indeed, it is hard to overstate how common it is to see talented young data scientists write brilliant, performant code and fit extremely accurate models that predict the wrong property, solve a problem that isn’t actually core to the stakeholder’s needs, require data not available in deployment, or work in training but won’t generalize to the stakeholder’s deployment context. The data scientists on these projects often deployed the skills they learned in technical classes magnificently, but this execution was not well\sphinxhyphen{}directed in service of the stakeholder — the classic “not seeing the forest for the trees” mistake.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Throughout this book, I will frequently use the term “stakeholder” to refer to the person whose problem that you, the data scientist, is seeking to address. I use this term because, as a young data scientist, you will often be in the position of having to use your data science skills to help someone else. Thus your stakeholder may be your manager, your CEO, or someone at another company you are advising.

\sphinxAtStartPar
However, if you’re lucky enough to \sphinxstyleemphasis{not} be directly answerable to someone else, either because you work for yourself or because you’re in a field that gives you substantial autonomy like academia, you can simply think of your “stakeholder” as yourself.

\sphinxAtStartPar
If you’re interested in developing a consumer\sphinxhyphen{}facing product (e.g., you’re an independing developer whose thinking of creating a new data\sphinxhyphen{}science\sphinxhyphen{}based web app), you may also find it useful to think of your customer of the stakeholder, since very few products are successful if they don’t solve a problem customers face.
\end{sphinxadmonition}


\subsection{Specifying The Problem}
\label{\detokenize{10_introduction/20_solving_problems:specifying-the-problem}}
\sphinxAtStartPar
How, then, can we be sure our hard work as data scientists serves the interests of our stakeholder?

\sphinxAtStartPar
The first step in solving any problem is \sphinxstyleemphasis{always} to carefully specify the problem. While this may seem obvious, properly articulating the core problem one seeks to address can be remarkably difficult. Moreover, because everything you will do \sphinxstyleemphasis{after} articulating your problem is premised on having correctly specified your objective, it is \sphinxstyleemphasis{the} greatest determinant of the success of your project. The most sophisticated, efficiently executed, high precision, high recall model in the world isn’t worth a lick of good if the results it generates don’t solve the problem you or your stakeholder need solved.

\sphinxAtStartPar
Specifying your problem not only ensures that your subsequent efforts are properly directed, but it can also radically simplify your task. Many times problems only \sphinxstyleemphasis{appear} difficult because of how they are presented. As Charles Kettering, Head of Research at General Motors from 1920 to 1947 once said, “A problem well stated is a problem half solved.”

\sphinxAtStartPar
How do you know if you’ve “clearly articulated the problem,” and how should you go about refining your problem statement with your stakeholder? Those are topics we will discuss in detail in the coming chapters, as well as strategies for using data to help inform this process through iterative refinement of your understanding of the contours of the problem space.


\subsection{Stakeholder Management and Domain Expertise}
\label{\detokenize{10_introduction/20_solving_problems:stakeholder-management-and-domain-expertise}}
\sphinxAtStartPar
Application is, in many ways, what sets data science apart from the disciplines of mathematics, computer science, and statistics. And to apply the tools of data science tools effectively, by definition, requires an understanding of the domain to which your tools are being deployed. This fact makes many data scientists uncomfortable — after all, many young data scientists do not even know the industry in which they will be employed when the graduate, never mind feel they can lay claim to being a “domain expert” in any specific substantive field.

\sphinxAtStartPar
A consequence of this discomfort is that “domain expertise” is that the term “domain expertise” is often invoked as a way of abdicating responsibility for part of an analysis as “somebody else’s problem” (a dangerously powerful construction, as explained by the \DUrole{xref,myst}{incomparable Douglas Adams}).

\sphinxAtStartPar
But it would be a mistake to view domain expertise as beyond the responsibility of the data scientist for two reasons. First, engaging with the details of a substantive domain to tailor the application of data science methods to solve a specific problem isn’t ancillary to the job of a data scientists — it’s a data scientist’s comparative advantage. Data scientists (generally) are not the most technically skilled mathematicians, statisticians, or programmers — we are professionals who specialize in taking the best insights from all three of these skills sets and adapting them to fit the specific needs of a specific problem. So embrace the role of “domain knowledge!”

\sphinxAtStartPar
The second reason is that while you are unlikely to be a “domain expert” yourself, learning to solicit important information about a domain from true domain experts is a skill unto itself. As we will detail in our readings on “stakeholder management,” stakeholders may be experts in their particular substantive domain, but because they (usually) won’t understand data science, they are unlikely to ever provide you with the domain knowledge you need to do your job successfully (and you can’t get away with just saying “yes, this project failed, but it was because you didn’t tell me X!” See the discussion above of the single criterion used to evaluate data scientists). So learning to \sphinxstyleemphasis{partner} with your stakeholder and domain experts to understand a problem is a key part of understanding it properly, and thus eventually solving it.

\sphinxstepscope


\section{Idea 2: Solving Problems by Answering Questions}
\label{\detokenize{10_introduction/22_question_types:idea-2-solving-problems-by-answering-questions}}\label{\detokenize{10_introduction/22_question_types::doc}}
\sphinxAtStartPar
Once you’ve clearly articulated the problem you wish to solve, the next step is… to solve it! But how best to do so?

\sphinxAtStartPar
As data scientists, we are somewhat restricted in the types of solutions to which we have access. Nobody expects a data scientist to discover a new semiconductor manufacturing technique or solicit donors for cancer research funds.

\sphinxAtStartPar
But what data scientists can do is \sphinxstylestrong{answer questions about the world.} And while that may not seem inspiring at first, it’s a remarkably powerful ability. That’s because whether it’s trying to answer the question “does this patient have disease X?”, “what would happen to profits if we changed our pricing strategy?”, or “do universal income programs reduce long\sphinxhyphen{}term poverty?”, it turns out that a great many problems faced by both businesses and society as a whole become much easier to solve if we can better understand the consequences of our actions or reduce our uncertainty about the world.

\sphinxAtStartPar
In light of that fact, we can reframe the challenge of a data scientist from the more amorphous task of “figuring out how to solve the problem” to the more concrete “what question, if answered, would make it easier to solve this problem?” Once we’ve articulated a question to answer we can turn to choosing the best tool for generating an answer. But it is worth emphasizing this point — it is only at this stage of our project—not at the beginning!—that we start thinking about what statistical method, algorithm, or model is appropriate for the task.

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Answering Questions}

\sphinxAtStartPar
The challenge of a data scientist is to figure out “what question, if answered, would make it easier to solve this problem?”
\end{sphinxShadowBox}

\sphinxAtStartPar
But what if my stakeholder wants me to do something other than answer a question about the world? What if they want me to write a model to automatically read x\sphinxhyphen{}rays, or detect fraudulent transactions?

\sphinxAtStartPar
While it may not be immediately obvious how these fit in the “data scientists solve problems by answering questions about the world” framework, I can assure you they do. Indeed, anything you can think of that a data scientist might do ends up fitting this model because — as we’ll discuss in detail below and later in the book — \sphinxstyleemphasis{all} data science models and algorithms can be understood as instruments designed to answer very specific, very concrete questions about data. And once you come to appreciate that fact, not only will this frame make more sense, but you will probably also find you understand many of the models you work with regularly more intuitively than you did before.




\subsection{Types of Questions}
\label{\detokenize{10_introduction/22_question_types:types-of-questions}}
\sphinxAtStartPar
There are three types of questions we will explore in this book, each of which helps to solve problems in a different way:%
\begin{footnote}[1]\sphinxAtStartFootnote
Careful readers may notice that these categories do not include \sphinxstyleemphasis{should questions}, which are sometimes referred to as “prescriptive” or “normative” questions. As we will discuss in detail in an upcoming reading, that is because while data science is an amazing tool for characterizing the world around us, it cannot, on its own, answer questions about how the world \sphinxstyleemphasis{should} be. Answering “should questions” requires evaluating the desirability of different possible states of the world, and that can only be done with reference to a system of values, making them inherently subjective. Data science can help us predict the \sphinxstyleemphasis{consequences} of different courses of action, but it cannot tell us whether those consequences make a given course of action \sphinxstyleemphasis{preferable}.
%
\end{footnote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Exploratory Questions: Questions about patterns and structure in the world.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Help us to understand the problem space better and prioritizing subsequent efforts.

\end{itemize}

\item {} 
\sphinxAtStartPar
Passive Prediction Questions: Questions about likely outcomes for individual observations or entities.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Useful for targeting individuals for additional attention or automating certain tasks.

\end{itemize}

\item {} 
\sphinxAtStartPar
Causal Questions: Questions about the consequences of actions or interventions being considered.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Useful for deciding on appropriate courses of action.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
Each of these can play a different but important role in solving problems, and any effort to answer a question of each type will raise similar issues that need to be considered. As summarized next, by recognizing the \sphinxstyleemphasis{class} of questions we are seeking to answer, we can significantly narrow both the set of data science tools that are appropriate to consider and provide a short list of common considerations to think through.


\subsection{Exploratory Questions}
\label{\detokenize{10_introduction/22_question_types:exploratory-questions}}
\sphinxAtStartPar
Once you have settled on a problem you wish to address, the next step is often to use data science to better understand the contours of the problem in order to better prioritize and strategize your efforts. As data scientists, our best strategy for this type of investigation is to ask questions about general patterns related to your problem — what I call \sphinxstyleemphasis{Exploratory Questions.}

\sphinxAtStartPar
Why is this necessary? Well, as we’ll discuss in a future reading on “stakeholder management,” you would be \sphinxstyleemphasis{shocked} at how often stakeholders have only a vague sense of the patterns surrounding their problem. This makes refinement of your problem statement (and thus prioritization of your subsequent efforts) impossible. So before you get too far into any data science project, it’s important to ask Exploratory Questions to improve your understanding of how best to get at your problem.

\sphinxAtStartPar
To illustrate, suppose a company hired you because they were having trouble recruiting enough high\sphinxhyphen{}quality employees. You \sphinxstyleemphasis{could} ask for their HR data and immediately try to train a neural network to… well, I’m not even sure what you’d want to train it to do right off that bat! And that’s a big part of the problem. Getting more high\sphinxhyphen{}quality employees is a very general problem, and you could imagine addressing it in any number of ways — you could try and get more people to apply for the job in the first place, you could try and get a \sphinxstyleemphasis{different type} of candidate to apply then is currently applying, you could try and get more high\sphinxhyphen{}quality people who are given job offers to accept those offers, or you could help try to increase the number of people who are hired who turn out to be successful hires! But which should you do first?

\sphinxAtStartPar
To help answer this question, we can start by asking a series of Exploratory Questions that, when answered, will aid in your efforts to solve your stakeholder’s problem:
\begin{itemize}
\item {} 
\sphinxAtStartPar
How many job applications are you receiving when you post a job?

\item {} 
\sphinxAtStartPar
What share of your current job applicants are of high quality?

\item {} 
\sphinxAtStartPar
If your current applicants come from different sources (online ads, services like Indeed, outreach to colleagues for recommendations, etc.), what share of job applicants from each of these sources are of high quality?

\item {} 
\sphinxAtStartPar
What share of employees you try to hire accept your offer?

\item {} 
\sphinxAtStartPar
What share of employees you do hire turn out to be successful employees?

\end{itemize}

\sphinxAtStartPar
Suppose, for example, only 10\% of applicants who receive job offers accept. Then clearly that would seem a place where intervention would be likely to substantially increase the number of high\sphinxhyphen{}quality employees being hired. If, by contrast, 95\% of applicants accept offers, then that is clearly not a place where you would want to focus.

\sphinxAtStartPar
Similarly, if most applicants are high quality and there just aren’t enough of them, then you would probably want to focus your efforts on increasing the number of people who apply in the first place. But if only 2\% of applicants seem appropriate to the company, then maybe focus should be put on changing \sphinxstyleemphasis{who} is applying for positions with an eye towards increasing the average quality of applicants.

\sphinxAtStartPar
Answering these questions will likely not, on its own, make it clear exactly where to focus your efforts. Your stakeholder may look at the fact that only 2\% of applicants are appropriate and say “That’s fine — we have so many applications that the \sphinxstyleemphasis{absolute number} of quality applicants is actually high enough, and it’s easy to filter out the bad applicants.” But these are numbers you can bring back to your stakeholder to discuss and use to zero in on the specific facet of their problem that is most amenable to an impactful solution.

\sphinxAtStartPar
Generating answers to these types of Exploratory Questions doesn’t have the same “coolness factor” as using expensive GPUs to train deep learning models. But it is precisely this type of analysis that will help ensure that when if you \sphinxstyleemphasis{do} later run up a giant bill renting GPUs, at least that money will have been spent addressing a part of your stakeholder’s problem that matters.

\sphinxAtStartPar
So how does one go about answering Exploratory Questions? As we’ll discuss in later chapters, at times Exploratory Questions can be answered with simple tools, like scatter plots, histograms, and the calculation of summary statistics like means and medians. Other times, however, it may require more sophisticated methods, like clustering or other unsupervised machine learning algorithms that can, say, identify “customer\sphinxhyphen{}types” in a large dataset of customer behavior.

\sphinxAtStartPar
Regardless of the tool used, however, the goal is always to identify patterns in the world that are salient to understanding your stakeholder’s problem.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Answering Exploratory Questions is \sphinxstyleemphasis{not} synonymous with “Exploratory Data Analysis” (EDA). As it is commonly understood and practiced by students, EDA refers to the process of poking around in a new data set before fitting a more complicated statistical model. It entails learning what variables are present, how they are coded, and \sphinxstyleemphasis{sometimes} looking at general patterns in the data prior to model fitting. Crucially, it is generally defined by what it is \sphinxstyleemphasis{not} — it is what you do \sphinxstyleemphasis{before} you fit a complicated model — and by the tools used — EDA consists of plotting distributions or cross\sphinxhyphen{}tabulations, not fitting models or doing more sophisticated analyses.

\sphinxAtStartPar
Answering Exploratory Questions, by contrast, is about achieving a substantive goal — learning about patterns \sphinxstyleemphasis{in the world} — not the tools used to do it, or what it comes before. Answering an important Exploratory Question may require you to actively seek out new data, merge data from different sources together, and potentially do novel data collection. It may also entail model fitting or use of unsupervised machine learning algorithms to uncover latent patterns.

\sphinxAtStartPar
{\hyperref[\detokenize{30_questions/07_eda::doc}]{\sphinxcrossref{\DUrole{std,std-doc}{We will discuss the conceptual issues surrounding EDA in more detail in a later reading}}}}.
\end{sphinxadmonition}


\subsection{Passive Prediction Questions}
\label{\detokenize{10_introduction/22_question_types:passive-prediction-questions}}
\sphinxAtStartPar
Answering Exploratory Questions helps you to prioritize your efforts and improve your understanding of your stakeholder’s problem. Often you will bring the answers you generate to Exploratory Questions back to your stakeholder and use them to refine your problem statement in an iterative loop. “Yes, your broader problem is your profit margins are too thin, but after looking at your cost structure, it seems that the biggest driver of costs is the labor that goes into product packaging. So let’s focus on minimizing that.” But what then? Simple! We return to asking “What question, if answered, would help solve my problem?”

\sphinxAtStartPar
The second class of questions which, if answered, often help solve stakeholder problems are \sphinxstyleemphasis{Passive Prediction Questions}. Passive Prediction Questions are questions about the future outcomes or behaviors of \sphinxstyleemphasis{individual entities} (people, stocks, stores, etc.). This type of question may take the form “Given this new customer’s behavior on my website, are they likely to spend a lot over the next year?” or “Given the symptoms of this patient and their test results, how likely are they to develop complications after surgery?”  (Don’t worry about the “passive” part of the name — we’ll circle back to why that’s there below. For the moment, you may find it helpful to just think of these as “Prediction Questions.”).

\sphinxAtStartPar
Because Passive Prediction Questions are questions about individual entities, they don’t necessarily have one “big” answer. Rather, Passive Prediction Questions are answered by fitting or training a model that can take the characteristics of an individual entity as inputs (e.g., this patient is age 67, has blood pressure of 160/90, and no history of heart disease) and spitting out an answer \sphinxstyleemphasis{for that individual} (given that, her probability of surgical complications is 82\%). This differentiates Passive Prediction Questions from Exploratory Questions, which are about global patterns, not individual level predictions.

\sphinxAtStartPar
With Passive Prediction Questions, the first question to ask is often one of feasibility: “Given data on new customer behavior on my website, \sphinxstylestrong{can I} predict how much they are likely to spend a lot over the next year?” But you then answer that question by training a model that can answer the question you really care about for any given customer: “Given this new customer’s behavior on my website, are they likely to spend a lot over the next year?”

\sphinxAtStartPar
This ability to make predictions about future outcomes is obviously of tremendous use to stakeholders as it allows them to tailor their approach at the individual level. A hospital that can predict which patients are most likely to experience complications after surgery can allocate their follow\sphinxhyphen{}up care resources accordingly. A business that knows which customers are more likely to be big spenders can be sure that those customers are given priority by customer care specialists.

\sphinxAtStartPar
But the meaning of the term “Prediction” in Passive Prediction Questions extends beyond “predicting the future”. Passive Prediction Questions also encompass efforts to predict how a third party \sphinxstyleemphasis{would} behave or interpret something about an individual if given the chance.

\sphinxAtStartPar
For example, suppose our hospital stakeholder wanted to automate the reading of mammograms so that rural hospitals without full\sphinxhyphen{}time radiologists could give patients diagnoses more quickly (or, more cynically, pay fewer radiologists).%
\begin{footnote}[2]\sphinxAtStartFootnote
Mammograms are x\sphinxhyphen{}rays of breast tissue used for the detection of breast cancer.
%
\end{footnote} How does a model that reads mammograms “answer a question”? Well, in a very meaningful way, if you train your model by feeding it a dataset of mammograms that have been labelled as “normal” or “abnormal” by radiologists, then what that model is learning to do is answer the question: “if a radiologist looked at this particular scan, would they conclude it is abnormal?”

\sphinxAtStartPar
The value of this type of prediction to stakeholders is likely also self\sphinxhyphen{}evident, as it opens the door for automation and scaling of tasks that would otherwise be too costly or difficult for humans. Indeed, answering this question is the type of task for which machine learning has become most famous. Spam filtering amounts to answering the question “If the user saw this email, would they tag it as spam?” Automated content moderation amounts to answering “Would a Meta contractor conclude the content of this photo violates Facebook’s Community Guidelines?” Indeed, even Large Language Models (LLMs) like chatGPT, Bard, and LLaMA can be understood in this way, as we will discuss later.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Thinking of training an algorithm to read a mammogram as a model that answers the question “if a radiologist looked at this particular scan, would they conclude it is abnormal?” may seem a little strange at first. But this framing is both more accurate and more conceptually useful than the more common frame of “this is a a model that detects abnormal mammograms.” That’s because when a data science problem is solved by training a model on data on past human behavior, it isn’t actually learning to “detect cancer” — it’s learning to emulate the behavior of the humans in the training data.

\sphinxAtStartPar
This distinction is subtle, but it is important because it helps us to understand why any model we train in this way will inherit all of the biases and limitations of the radiologists who created the data used to train the algorithm. If, for example, our radiologists were less likely to see cancer in denser breast tissue, that bias would also be inherited by the algorithm.

\sphinxAtStartPar
(We call the inevitable existance of some difference between between what we \sphinxstyleemphasis{want} the algorithm to do — in this case, detect cancer — and \sphinxstyleemphasis{what it is actually being trained to do} — predict how a radiologist would interpret the scan — an “alignment problem.”)
\end{sphinxadmonition}

\sphinxAtStartPar
It is worth emphasizing that the distinction between Exploratory Questions and Passive Prediction Questions is a distinction \sphinxstyleemphasis{in purpose}, not necessarily a distinction in the statistical tools that are most appropriate to the task. A linear regression, for example, may be used for answering either type of question, but in different ways. To answer an Exploratory Question, we might look at the coefficients in a linear regression to understand the partial correlations between variables in the data. To answer a Passive Prediction Question, we might only look at the predicted values from the regression model.

\sphinxAtStartPar
But even if the same \sphinxstyleemphasis{type} of model can be used for both purposes, how one \sphinxstyleemphasis{evaluates} the model depends entirely on the purpose to which it is being put. When answering an Exploratory Question through the interpretation of regression coefficients, the size of the standard errors on the coefficients is critical. When making predictions, by contrast, one may not care about the coefficients of a model at all! So long as the R\sphinxhyphen{}squared is high enough (and other diagnostics seem good), one can simply use the predicted values the regression generates without ever looking “inside the box.”

\sphinxAtStartPar
As such, there’s no simple mapping between statistical or machine learning methods and the type of questions you aim to answer. With that said, the study of how to answer Passive Prediction Questions is often referred to as “supervised machine learning.”


\subsection{Causal Questions}
\label{\detokenize{10_introduction/22_question_types:causal-questions}}
\sphinxAtStartPar
The last category of question that data scientists are commonly called upon to answer are \sphinxstyleemphasis{Causal Questions}: questions about what the \sphinxstyleemphasis{effect} might be if a certain action is taken. For example, “What would be effect be of patients with disease X taking medication Y?” or “what would the effect of changing the interface of my app be on user retention?”

\sphinxAtStartPar
Causal Questions most often arise when a stakeholder wants to do something — buy a Superbowl ad, change how a recommendation engine works, authorize a new medical device — but they fear the action they are considering might be costly and not actually work. In these situations, stakeholders will often turn to a data scientist in the hope that the scientist can “de\sphinxhyphen{}risk” the stakeholder’s decision by providing guidance on the likely effect of the action \sphinxstyleemphasis{before} the action is undertaken at full scale.

\sphinxAtStartPar
Causal Questions, therefore, take the form of “What is the effect of an action X on an outcome Y?”—or more usefully, “If I do X, how will Y change?”. Nearly anything can take the place of X and Y in this formulation: X could be something small, like changing the design of a website, or something big, like giving a patient a new drug or changing a government regulation. Y, similarly, could be anything from “how long users stay on my website” or “how likely are users to buy something at my store” to “what is the probability that the patient survives”.

\sphinxAtStartPar
In my view, Causal Questions are perhaps the hardest to answer for two reasons. The first is that when we ask a Causal Question, we are fundamentally interested in \sphinxstyleemphasis{comparing} what our outcome Y would be in two states of the world: the world where we do X, and the world where we don’t do X. But as we only get to live in one universe, we can never perfectly know what the value of our outcome Y would be in \sphinxstyleemphasis{both} a world where we do X and one where we don’t do X—a problem known as the \sphinxstylestrong{Fundamental Problem of Causal Inference} (causal inference is just what people call the study of how to answer Causal Questions).

\sphinxAtStartPar
But the second reason is Causal Questions land on the desk of data scientists when a stakeholder wants to know the likely consequences of an action \sphinxstyleemphasis{before they actually undertake the action at full scale.} This may seem obvious, but it bears repeating — not only is answering Causal Questions hard because we never get to measure outcomes in both a universe where our treatment occurs and also a universe where it does not (the Fundamental Problem of Causal Inference), but answering Causal Questions is \sphinxstyleemphasis{also} hard because stakeholders want to know about the likely consequences of an action they aren’t ready to actually undertake!

\sphinxAtStartPar
As a result, the job of a data scientist who wants to answer a Causal Question is to design a study that not only measures the effect of a treatment but also does so in a setting that is enough like the context in which the stakeholder wants to act that any measured effect will generalize to the stakeholder’s context.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Now that we’ve discussed Causal Questions, we can discussion the term “passive” in “Passive Prediction Questions.” The term “Passive” in “Passive Prediction Questions” is meant to differentiate situations where a stakeholder wants to predict things about individuals they do not yet know \sphinxstyleemphasis{in a world where the status quo prevails and our behavior doesn’t change.}

\sphinxAtStartPar
For example, when answering the Passive Prediction Question “Given their case history, how likely is this patient to experience post\sphinxhyphen{}surgical complications?” we don’t actually want to know how likely they are to experience complications — we want to know how likely they would be to experience complications \sphinxstyleemphasis{absent any intervention.} Our hope, after all, is that by learning that a certain patient is likely to experience complications we can act to prevent that outcome!

\sphinxAtStartPar
Obviously, we might then want to build on the insights provided by that model and ask a followup Causal Question: “What would the effect be of assigning an extra nurse to patients predicted to be more likely to have post\sphinxhyphen{}surgical complications?”

\sphinxAtStartPar
But because these questions are different in their goals, we would also go about answering them in very different ways. For example, we might answer our Passive Prediction Question using historical patient data and a logistic regression or decision tree. And to answer our Causal Question, we might run a randomized experiment in which half the patients we predict as likely to experience complications are assigned an extra nurse and half are not, and we later compare patient outcomes.
\end{sphinxadmonition}


\subsection{An Example}
\label{\detokenize{10_introduction/22_question_types:an-example}}
\sphinxAtStartPar
In this introductory chapter alone, we’ve already covered a substantial amount of material. We’ve discussed the importance of problem articulation, the idea that the way data scientists solve problems is by answering questions, and the three types of questions data scientists are likely to encounter.

\sphinxAtStartPar
It’s easy to see how this framework might result in a sequential development of a project. First, a hospital comes to you concerned about the cost of surgical complications. So you:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Work with them to more clearly define the problem (“Surgical complications are extremely costly to the hospital and harm patients. We want to reduce these complications in the most cost\sphinxhyphen{}effective manner possible.”)

\item {} 
\sphinxAtStartPar
You answer some Exploratory Questions (“Are all surgical complications equally costly, or are there some we should be most concerned about?”).

\item {} 
\sphinxAtStartPar
You develop a model to answer a Passive Prediction Question (“Given data in patient charts, can we predict which patients are most likely to experience complications?”) so the hospital can marshal its limited nursing resources more effectively.

\item {} 
\sphinxAtStartPar
The hospital then comes back to you to ask the Causal Question “Would a new program of post\sphinxhyphen{}discharge nurse home visits for patients identified as being at high risk of complications reduce complications?”

\end{enumerate}

\sphinxAtStartPar
In reality, however, while it is important that some steps come before others (if you don’t start by defining your problem, where do you even start?), real projects are never so linear. The reality is that you will constantly find yourself moving back and forth between different types of questions, using new insights gained from answering one question to refine your problem statement and articulate new questions.

\sphinxAtStartPar
Nevertheless, by using this framework as a starting point, and using this taxonomy to help you recognize (a) the type of question you are asking, and (b) the reason you are seeking to answer a given question even when iterating through a project, you will see tremendous gains in your ability to please your stakeholders by staying focused on the problems they need addressed.


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Idea 3: Being Thoughtful About Being Wrong}
\label{\detokenize{10_introduction/23_mistakes:idea-3-being-thoughtful-about-being-wrong}}\label{\detokenize{10_introduction/23_mistakes::doc}}
\sphinxAtStartPar
The third big idea of this book is that one of the biggest differentiators of good data scientists and great scientists is not their ability to maximize the accuracy of their models, but to think rigorously about their models limitations and the errors they will inevitably commit.

\sphinxAtStartPar
To illustrate the type of fallacy that young data scientists fall prey to in their quest to maximize model performance, let us consider that most intuitive of metrics, \sphinxstyleemphasis{accuracy} — the share of classifications a model makes that are correct. In my role as the Admissions Chair for the Duke Masters of Interdisciplinary Data Science (MIDS) program, I read hundreds of essays a year from aspiring data scientists, and I long ago lost count of the number of times I have seen some version of the following passage with little to no additional context:
\begin{quote}

\sphinxAtStartPar
While working in {[}person{]}’s lab, I fit a {[}logit/XGBoost/Random Forest/Deep Learning{]} model to the data and achieved an accuracy of 96\%.
\end{quote}

\sphinxAtStartPar
Or, in the applicant’s resume, they report attaining an accuracy score in the 90s with some model.

\sphinxAtStartPar
I assume that these authors believe that this type of declaration reflects well on them — after all, 96\% accuracy means only 4\% of observations were mis\sphinxhyphen{}classified! But the truth is that reporting a high accuracy absent addition information about model performance actually tells me more about the author’s failure to understand this third Big Idea of the book than it tells me about their modelling prowess.

\sphinxAtStartPar
To illustrate why, suppose I told you I had developed a model that could predict breast cancer in mammograms with an accuracy of over 90\%. That’d be amazing, right? Mammograms cost roughly \$10 \sphinxstyleemphasis{billion} dollars a year in the US alone,%
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxhref{https://pmc.ncbi.nlm.nih.gov/articles/PMC4142190/}{“Aggregate Cost of Mammography Screening in the United States: Comparison of Current Practice and Advocated Guidelines”}, \sphinxstyleemphasis{Annals of Internal Medicine}, February 2014.
%
\end{footnote} so any reduction in need for skilled radiologists to review mammograms would be a huge win for women’s health and healthcare costs, right?

\sphinxAtStartPar
Now suppose I told you that the model I wrote was this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}cancer\PYGZus{}detection\PYGZus{}model}\PYG{p}{(}\PYG{n}{mammogram}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous} \PYG{o}{=} \PYG{k+kc}{False}
    \PYG{k}{return} \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous}
\end{sphinxVerbatim}

\sphinxAtStartPar
How does that have an accuracy of 90\%? Simple — according to the Susan G. Komen Society, roughly 90\% of routine mammograms are normal and require no followup. Thus a “model” that reports all routine mammograms are normal will immediately achieve an accuracy score of 90\%. In fact, the true accuracy of the model is actually higher than 90\%, since most mammograms flagged as “abnormal” are determined to not be indicative of cancer after followup (e.g., after biopsies).

\sphinxAtStartPar
So, are you still impressed by my model’s 90\% accuracy?

\sphinxAtStartPar
Of course not. And to be clear, the problem with this model is \sphinxstyleemphasis{not} that its accuracy is only 90\% and not, say 95\%. The problem with this model is that \sphinxstyleemphasis{it has a False Negative Rate (the share of cases that are positive — in this case, mammograms from women with cancer — that are classified as cancer free) of 100\%.} And since the thing we care about most in cancer screenings is not telling a patient with cancer they’re fine (a False Negative), that’s a huge problem!

\sphinxAtStartPar
OK, if what we care about is the False Negative Rate, shouldn’t we just minimize the False Negative Rate? Not so fast! Consider this model:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}no\PYGZus{}false\PYGZus{}negative\PYGZus{}model}\PYG{p}{(}\PYG{n}{mammogram}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous} \PYG{o}{=} \PYG{k+kc}{True}
    \PYG{k}{return} \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous}
\end{sphinxVerbatim}

\sphinxAtStartPar
Oh dear. Yes, that model has a 0\% False Negative Rate, but it also has a 100\% False \sphinxstyleemphasis{Positive} Rate, meaning anyone subject to this screening would be told they might have cancer and needs followup diagnostic procedures! That’s no good either.

\sphinxAtStartPar
No, a \sphinxstyleemphasis{good} model for reading mammograms needs to maximize accuracy while also balancing the desire to not subject healthy women to unnecessary procedures (minimize False Positives) \sphinxstyleemphasis{and} the desire to not fail to flag potentially cancerous scans. And that’s what I mean when I say that a \sphinxstyleemphasis{great} data scientist is one who is thoughtful about how their models make mistakes. Determining what model is “best” requires more than optimizing a simple objective function — it requires thinking critically about the problem one is trying to solve (Big Idea 1), the substantive impact of different types of model errors in the context of that problem, and using that to inform model selection and evaluation.




\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{What \sphinxstyleemphasis{is} Data Science: An Historical Perspective}
\label{\detokenize{10_introduction/40_data_science_in_historical_context:what-is-data-science-an-historical-perspective}}\label{\detokenize{10_introduction/40_data_science_in_historical_context::doc}}
\sphinxAtStartPar
Given how often the term “data science” gets thrown around, you would be excused for thinking that the meaning of the term was clearly understood. The reality, however, is that if you were to ask ten people working in the field you will almost certainly get ten different descriptions of what it is and what they do.

\sphinxAtStartPar
Part of that is deliberate obfuscation—data science is \sphinxstyleemphasis{so} trendy that everyone wants to claim that what they’re doing is data science in order to woo venture capitalists or to win research grants. Indeed, it has been said (half\sphinxhyphen{}joking, half\sphinxhyphen{}seriously): “Data science is \sphinxstyleemphasis{Artificial Intelligence} when you’re raising money, \sphinxstyleemphasis{Machine Learning} when you’re hiring, and it’s \sphinxstyleemphasis{Logistic Regression} when you actually have to get the job done.

\sphinxAtStartPar
But the ambiguity that surrounds the term “data science” is also the result of the fact that data science is not a mature discipline in the way that computer science, economics, or mechanical engineering are mature disciplines. And, as a young data scientist, that immaturity is important for you to understand, as it is both the source of some of the most exciting opportunities and also some of the biggest challenges you will face.


\section{The Organization of Academia, Data Science, and You}
\label{\detokenize{10_introduction/40_data_science_in_historical_context:the-organization-of-academia-data-science-and-you}}
\sphinxAtStartPar
To explain what the term data science means in practice, we have to start by discussing a bit of the inside\sphinxhyphen{}baseball%
\begin{footnote}[1]\sphinxAtStartFootnote
“\sphinxhref{https://en.wikipedia.org/wiki/Inside\_baseball\_(metaphor)}{Inside baseball}” refers to the discussion of the idiosyncracies and details of how an institution or system operates internally, something that is often not of interest to people who aren’t part of the system.
%
\end{footnote} of how academia operates. This may feel esoteric, but it’s important to understand because the way academia is organized has shaped the professional training — and thus the language and thought patterns — of most people you will encounter in the data science space. Understanding academia better, as a result, will not only help you understand the material you are exposed to in data science classes better, but also help you relate to your future peers and colleagues.

\sphinxAtStartPar
The idea that academia is deeply fragmented often surprises students, and understandably so. Universities \sphinxstyleemphasis{love} to pay lip service to the importance of interdisciplinarity and are quick to highlight successful interdisciplinary collaborations. But successful interdisciplinary collaborations are so notable precisely because they are the exception, not the rule. The reality is that academic research is starkly divided into disciplinary silos (e.g., computer science, statistics, political science, economics, and engineering). This isn’t because researchers aren’t \sphinxstyleemphasis{interested} in interdisciplinary collaborations, but rather that their professional imperatives push them to focus their attention on the priorities and language of their own departments and disciplines.%
\begin{footnote}[2]\sphinxAtStartFootnote
Nearly all university faculty are hired by established departments like statistics or economics, faculty submit their research to journals specific to their discipline, those journals in turn ask fellow members of the discipline to evaluate their work for publication, and promotions and tenure reviews are managed by the faculty in a faculty member’s own department.
%
\end{footnote}

\sphinxAtStartPar
Thus, while the past several decades have seen an unprecedented emergence of new methods across all of academia, the lack of intellectual cross\sphinxhyphen{}pollination across academic silos has resulted in disciplines failing to take full advantage of discoveries from other disciplines. Over time, each discipline has developed a perspective on computational methods that emphasizes its own intellectual priorities.

\sphinxAtStartPar
To illustrate, suppose we were interested in using patient data to reduce heart attacks. A computer scientist looking at this problem might use their discipline’s methods to \sphinxstyleemphasis{predict} which patients are most likely to experience a heart attack in the future using current patient data; a social scientist might focus on trying to understand the \sphinxstyleemphasis{effect} of giving patients a new drug on heart attack risk; and a statistician might focus on understanding \sphinxstyleemphasis{how confident} we should be in the conclusions reached by the computer scientist and social scientist.

\sphinxAtStartPar
This fragmentation has also resulted in a fragmentation of \sphinxstyleemphasis{language} around data science methodologies. Disciplines often come up with different terminology for the same phenomena, adding another layer of difficulty to efforts to work across departmental silos.

\sphinxAtStartPar
The result is a situation analogous to the Buddhist parable of the blind men and the elephant, wherein a group of blind people come upon an elephant, and upon laying hands on different parts of the elephant, they come to different conclusions about what lies before them. The person touching the tail declares “we have found a rope!”, while the person touching the leg declares “we have found a tree!”

\sphinxAtStartPar
\sphinxincludegraphics{{blindmenelephant}.jpg}

\sphinxAtStartPar
(\sphinxstyleemphasis{Note}: Not sure of original source of this image. \sphinxhref{https://pursuitofresearch.org/2011/01/19/the-blind-men-and-the-elephant/}{Found it here}, but need to figure out rights prior to anything about this becoming commercial! Lots of pics in public domain if needed, but not blindfolded scientists.)

\sphinxAtStartPar
And yet, as the poet John Godfrey Saxe wrote in his poem \sphinxhref{https://en.wikipedia.org/wiki/Blind\_men\_and\_an\_elephant\#John\_Godfrey\_Saxe}{\sphinxstyleemphasis{The Blind Men and the Elephant}} about this parable many centuries later:
\begin{quote}

\sphinxAtStartPar
And so these men of Indostan,
Disputed loud and long,
Each in his own opinion
Exceeding stiff and strong,
Though each was partly in the right,
And all were in the wrong!
\end{quote}

\sphinxAtStartPar
In recent years, however, there has been a growing appreciation of what can be gained from pulling together the insights that have been developed in different fields, despite the challenges of language and professional imperatives to such collaborations. And, at least amongst those who are serious about the development of data science as a discipline and not just a buzzword to use when raising money, is the promise of data science: to unify the different perspectives and methods for analyzing data. Or, to put it more succinctly: to finally see the whole elephant.

\sphinxAtStartPar
While the field is making progress towards “seeing the elephant as a whole,” however, as a result of this fragmented origin story, \sphinxstyleemphasis{most} people you will encounter in the world doing data science were trained in one of these academic silos. That means that depending on who you are working with and how they were trained, you may find your future colleagues using terms you’ve never heard before. And when that happens, it’s important to remember that while that \sphinxstyleemphasis{may} be because they’re talking about a concept you’ve yet to encounter, it may also simply be because they’re using different language for something you know. Similarly, you may also find senior colleagues unfamiliar with concepts that seem basic to you simply because you were exposed to perspectives that were alien to your colleague’s academic silo at the time they were trained. Indeed, given that data science education \sphinxstyleemphasis{is} finally becoming more unified, you should probably expect to learn a lot of ideas that even your more senior colleagues (or rather, especially your more senior colleagues!) were never exposed to.

\sphinxAtStartPar
And therein also lies some of the greatest opportunities. Precisely because of this intellectual fragmentation, there are \sphinxstyleemphasis{lots} of opportunities for taking insights from one intellectual silo and using them to solve problems in another — a kind of “intellectual arbitrage,” if you will.


\section{The Data Analyst / Software Engineering Distinction}
\label{\detokenize{10_introduction/40_data_science_in_historical_context:the-data-analyst-software-engineering-distinction}}
\sphinxAtStartPar
In addition to this broader intellectual fragmentation, the world of data science also often feels oddly fragmented around the way people use the tools of data science.

\sphinxAtStartPar
One model of data science is what we will call the “data analyst” approach. Data scientists doing this type of work often collect data to answer specific questions—what is the effect of expanded government health insurance subsidies on mortality? what type of customer should we target with our new advertising campaign?. As a result, when they write code, they write it to be run against a specific set of data to answer a specific question.

\sphinxAtStartPar
The other model is what we will call the “software engineering” approach. Data scientists doing this type of work write software they plan to \sphinxstyleemphasis{deploy} to thousands or millions of users. This is the type of work that gets embedded in the apps on your phone, or that generates your movie recommendations at Netflix. As a result, when these data scientists write code, they are writing more sophisticated and generalizable programs.

\sphinxAtStartPar
To be clear, most data scientists do at least a bit of both types of work—data analysts may often write small programs or packages to aid in types of analysis they do a lot, and software engineers have to prototype and test new programs before they write a version that can be deployed broadly. But most people will eventually choose to specialize in one direction or another, and when you see data science resources in the world—especially ones about programming for data science—bear in mind that depending on \sphinxstyleemphasis{your} proclivities towards on approach or another, not all resources will be well suited to your interests.

\sphinxAtStartPar
I also want to draw attention to this distinction because it’s remarkable how dismissive most data scientists will be of the “other” type of data science, and I want to encourage you to both (a)not be so tribal yourself (both flavors of data science have their place, and help solve real world problems!), and (b) not be too surprised when you encounter people with irrationally strong opinions about which approach is the “right” approach to doing data science.


\bigskip\hrule\bigskip


\sphinxstepscope


\part{Problems to Questions}

\sphinxstepscope


\chapter{Solving the Right Problem}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:solving-the-right-problem}}\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem::doc}}
\sphinxAtStartPar
If data science is the study of how to solve problems using quantitative methods, then the first — and arguably most important — stage in any data science project is to define the problem to be solved.

\sphinxAtStartPar
While this may seem simple, it is often far from it. Indeed, as noted in the introduction, problems often only appear complicated because they are poorly understood. Reframing or rearticulating a difficult problem is often the key to figuring out how to solve it, which is why the adage “A problem well stated is a problem half solved” has remained popular for so long.

\begin{sphinxShadowBox}

\sphinxAtStartPar
A problem well stated is a problem half\sphinxhyphen{}solved.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Charles Kettering, Head of Research at General Motors

\end{itemize}
\end{sphinxShadowBox}

\sphinxAtStartPar
In this chapter, we will discuss what happens when data scientists fail to appreciate the importance of this stage of a project. With that established, we will then turn to advice on how to approach problem articulation.

\sphinxAtStartPar
This chapter will be written as if you are the sole actor on a data science project, responsible for everything from problem articulation to execution. But of course, this will rarely be the case in your professional life as a data scientist, especially early in your career. With that in mind, in the next chapter we will turn to the topic of “Stakeholder Management” — the practice of refining your understanding of the problem to be solved with a stakeholder.


\section{What \sphinxstyleemphasis{is} a “Stakeholder Problem?”}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:what-is-a-stakeholder-problem}}
\sphinxAtStartPar
Before I dive into the nuances of problem articulation, I should probably start by defining what I mean by “your stakeholder’s problem.”

\sphinxAtStartPar
A problem, in this context, is something about the world that your stakeholder would like to be different. They have to make a decision about whether to authorize a new drug, but they aren’t sure what the consequences of authorization would be. They are losing customers to a competitor. They are a hospital and are being regularly sued for patient falls. All of these are things in your stakeholder’s world that they wish would change. And we could say their problem has been addressed when that state of the world changes, and your stakeholder is happier as a result.

\sphinxAtStartPar
Critically, a problem isn’t just something your stakeholder \sphinxstyleemphasis{wants} to be different; a stakeholder problem is something they are willing to \sphinxstyleemphasis{pay} to make different. After all, the time of a data scientist isn’t cheap, so no stakeholder is going to be willing to hire you unless there’s something they are willing to allocate resources to resolve.

\sphinxAtStartPar
Sometimes, a stakeholder problem will clearly indicate what the remedy will look like — a government regulator charged with deciding whether a drug should be made publically available who isn’t sure about the consequences of authorization would be… wants to know the consequences of drug authorization. But this will not always be the case — often it’s easy to identify a problem (“we’re losing customer’s to Bob’s Burgers down the street!”) in a way that doesn’t immediately imply what form a solution would take. And other times, as we’ll discuss below, your stakeholder will \sphinxstyleemphasis{assume} they know what form a solution would take, and part of your job will be to differentiate between the actual problem (that you do need to address) and the assumed solution (which you should question, because it \sphinxstyleemphasis{may} be the best way to resolve the problem, but also may not).


\section{If You Don’t Articulate The Problem}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:if-you-don-t-articulate-the-problem}}
\sphinxAtStartPar
There are many examples in the world of data science of projects going awry because the team behind them failed to properly articulate the problem they wished to solve. Rather than start with one of those true stories, however, I’d like to begin with one of my favorite fictional (and humorous) examples of the phenomenon.

\sphinxAtStartPar
In Douglas Adams’ comedic sci\sphinxhyphen{}fi classic \sphinxstyleemphasis{Hitchhiker’s Guide to the Galaxy}, a race of hyperintelligent pandimensional beings set out to build a massive supercomputer the size of a city to solve the mysteries of the cosmos once and for all. When they turned on the computer, named Deep Thought, they announced that:
\begin{quote}

\sphinxAtStartPar
“The task we have designed you to perform is this. We want you to tell us… the Answer!”

\sphinxAtStartPar
“The Answer?” said Deep Thought.

\sphinxAtStartPar
“The Answer to what?”

\sphinxAtStartPar
“Life!” urged one designer.

\sphinxAtStartPar
“The Universe!” said another.

\sphinxAtStartPar
“Everything!” they said in chorus.

\sphinxAtStartPar
Deep Thought paused, then answered, “Life, the Universe, and Everything. There is an answer. But,” Deep Thought added, “I’ll have to think about it.”
\end{quote}

\sphinxAtStartPar
Seven and a half million years later, when Deep Thought had \sphinxstyleemphasis{finally} finished its calculations, the descendants of those designers assembled to learn the result of their ancestors’ work.
\begin{quote}

\sphinxAtStartPar
“Er …good morning, O Deep Thought,” said Loonquawl {[}one descendant{]} nervously, “do you have … er, that is …”

\sphinxAtStartPar
“An answer for you?” interrupted Deep Thought majestically. “Yes. I have.”

\sphinxAtStartPar
The two {[}descendants{]} shivered with expectancy. Their waiting had not been in vain.

\sphinxAtStartPar
“There really is one?” breathed Phouchg {[}the other descendant{]}.

\sphinxAtStartPar
“There really is one,” confirmed Deep Thought.

\sphinxAtStartPar
“To Everything? To the great Question of Life, the Universe and Everything?”

\sphinxAtStartPar
“Yes. {[}…{]} Though I don’t think,” added Deep Thought, “that you’re going to like it.”

\sphinxAtStartPar
{[}…{]}

\sphinxAtStartPar
“All right,” said the computer, and settled into silence again.

\sphinxAtStartPar
The two fidgeted.

\sphinxAtStartPar
The tension was unbearable.

\sphinxAtStartPar
“Forty\sphinxhyphen{}two,” said Deep Thought, with infinite majesty and calm.

\sphinxAtStartPar
“Forty\sphinxhyphen{}two!” yelled Loonquawl. “Is that all you’ve got to show for seven and a half million years’ work?”

\sphinxAtStartPar
“I checked it very thoroughly,” said the computer, “and that quite definitely is the answer. I think the problem, to be quite honest with you, is that you’ve never actually known what the question is.”

\sphinxAtStartPar
“But it was the Great Question! The Ultimate Question of Life, the Universe and Everything,” howled Loonquawl.

\sphinxAtStartPar
“Yes,” said Deep Thought with the air of one who suffers fools gladly, “but what actually is it?”

\sphinxAtStartPar
A slow stupefied silence crept over the men as they stared at the computer and then at each other.

\sphinxAtStartPar
“Well, you know, it’s just Everything … everything …” offered Phouchg weakly.

\sphinxAtStartPar
“Exactly!” said Deep Thought. “So once you do know what the question actually is, you’ll know what the answer means.”%
\begin{footnote}[1]\sphinxAtStartFootnote
Yes, I recognize that it is wildly indulgent to open a chapter with such a long epigraph. But it’s my book, and if there’s anything to be indulgent about its quotes from \sphinxstyleemphasis{Hitchhiker’s Guide to the Galaxy}, damn it!
%
\end{footnote}
\end{quote}

\sphinxAtStartPar
In addition to establishing the premise for one of the greatest comedic science fiction novels in human history,%
\begin{footnote}[2]\sphinxAtStartFootnote
Not least for being the only 5\sphinxhyphen{}book trilogy of which I am aware!
%
\end{footnote} I feel this passage perfectly exemplifies the three reasons most data science projects fail.

\sphinxAtStartPar
\sphinxstylestrong{The first is our absolute faith that technology will save us.} All we have to do to solve any problem is feed it into the newest, shiniest AI/LLM/ML model available. Sure, there may be technical challenges associated with configuring the environment, formatting the data, etc., but fundamentally, all that lies between us and success is putting the problem into technology’s hands.

\sphinxAtStartPar
The second is that when a data science project fails, it’s rarely because the technology itself failed. Rather, \sphinxstylestrong{projects usually fail because people failed to ensure that the task they asked their model to accomplish would actually solve their problem.} Technology doesn’t care if the task it’s been given is useful to the user. It will do what it has been asked to do — no more and no less. Garbage in, garbage out.

\sphinxAtStartPar
Finally, this passage also gives a nod to the third — and perhaps most important reason — that data science projects fail: \sphinxstylestrong{data scientists are far too deferential to their stakeholders.} In this passage, we can see that Deep Thought recognizes the idiocy of the request it has been given by its stakeholders — Loonqual and Phouchg — but does nothing about it. And to the degree to which we can think of the Deep Thought personality as a stand\sphinxhyphen{}in for the data scientist, this is a troubling realistic illustration of how many data science interactions go. The stakeholder says they want something and the (usually younger) data scientist assumes their only responsibility is to ensure the stakeholder’s request is implemented.

\sphinxAtStartPar
But as discussed in the introduction of this book, your success as a data scientist will \sphinxstyleemphasis{always} be evaluated in terms of whether you’ve made your stakeholder’s life better. And if doing precisely what they ask does not improve their lives, that’s the only thing that will matter. No one is blaming Deep Thought in this story, but they sure aren’t excited about what it did, either.


\section{Articulating and Reframing Your Problem}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:articulating-and-reframing-your-problem}}
\sphinxAtStartPar
The preceding story gives (an admittedly extreme) example of how things can go wrong when a data science project is not well motivated — in short, large amounts of energy and sweat can go into creating technically impressive results that, in the end, in no way solve a problem or improve the lives of anyone involved.

\sphinxAtStartPar
How, then, can you avoid that fate? Every problem is different, but here are some strategies to employ when faced with a problem to aid in refinement, reframing, and articulation. As we work through these, we will also work a few examples based on real cases designed to help you get a feel for what this process entails and why it’s so important.


\subsection{1. How do you know if you’ve been successful?}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:how-do-you-know-if-you-ve-been-successful}}
\sphinxAtStartPar
One of the most helpful questions to ask when trying to understand a problem is “how would we know if we’ve successfully solved this problem?” This question is powerful because it abstracts away the question of \sphinxstyleemphasis{how} you might solve a problem, and instead focuses attention on the \sphinxstyleemphasis{goal} one is trying to achieve. In other words, the objective of this question is to figure out how you might \sphinxstyleemphasis{measure} success.

\sphinxAtStartPar
Crucially, the answer to this question shouldn’t be something like “we have a good model for doing X” — that answer has largely just shifted the question to what constitutes “good.” Rather, try to come up with an answer in terms of the \sphinxstyleemphasis{metric} or \sphinxstyleemphasis{behavior} that, if observed, would tell you that you’ve been successful.

\sphinxAtStartPar
To illustrate the potential power of this question, let’s consider an example I’ve encountered in the real world (with some details changed to protect the anonymity of everyone involved).

\sphinxAtStartPar
The stakeholder is an online auction site for used construction equipment (front loaders, backhoes, etc.). They want to improve the algorithm that suggests prices to people selling equipment, but they’ve realized that while they have good data on how machinery type impacts prices, their algorithm can’t take into account the condition of equipment being listed. So they’ve hired a data science team to train an image classification model on listings to estimate the condition of each listed piece of machinery.

\sphinxAtStartPar
So: how would you know if you’ve been successful in solving this stakeholder’s problem?

\sphinxAtStartPar
At first blush, you might say I would know I was successful if I have a model that has high classification accuracy for equipment condition when fed photos of equipment listed in the past.

\sphinxAtStartPar
But look more carefully, is that really what the stakeholder cares about? No — because “image classification model” is something young data scientists are familiar with, they tend to latch on to it immediately. “Oh, great, image classification! I can do that. Let’s go!”

\sphinxAtStartPar
No, the problem motivating the stakeholder is that their price suggestion algorithm doesn’t take into account the condition of equipment going up for sale. Given that, you would know your model was successful if the predicted prices from the model were more in line with final sale prices for equipment of all conditions. \sphinxstyleemphasis{That’s} the actual goal.

\sphinxAtStartPar
Yes, an image classification model might allow the stakeholder to estimate equipment condition which could then be used as training data to revise the pricing model. But is that the best approach?

\sphinxAtStartPar
To train a model that can differentiate construction machinery in good or bad condition, the first thing you’d need to do is have a person go through and label a lot of old listings as being in good or bad condition. And you’d probably need a lot of them — after all, your model would have to be able to differentiate between indicators of wear that are superficial (dirt, superficial rust, and peeling paint) and indicators of wear that are substantial (cracks, patch welds, etc.), and do so on a range of different types of construction equipment. Then you’d use that to train an image classifier, then you’d have to tune and validate that image classifier, then you could use its classifications to improve the pricing model.

\sphinxAtStartPar
Alternatively, you could just (a) have someone label some old listings and use those labels \sphinxstyleemphasis{directly} to refine the pricing model, and (b) add a drop\sphinxhyphen{}down menu that asks people listing equipment to report the condition of their equipment (so that data is available to the pricing model when someone goes to list a product). Congratulations! You’ve just \sphinxstyleemphasis{entirely} removed image classification from this task.


\subsection{2. Abstract the Problem}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:abstract-the-problem}}
\sphinxAtStartPar
A second strategy for better understanding a problem is to try to reframe it at a higher level of generality/abstraction. When you do, you may find your thorny problem is actually just a specific example of a more general type of problem, one that people smarter than either of us have already figured out how to solve.

\sphinxAtStartPar
Alternatively, you may realize that you or your stakeholder has (often unknowingly) introduced constraints to the problem that aren’t actually constraints.

\sphinxAtStartPar
Perhaps my favorite example of this phenomenon comes from a talk given by \sphinxhref{https://youtu.be/kYMfE9u-lMo?t=1281\&amp;si=haO8mlmO5tB4OC9k}{Vincent Warmerdam at PyData 2019.}

\sphinxAtStartPar
The World Food Program (WFP) is a global leader in food aid provision. As Vincent tells the story — which he reports having heard at an Operations Research Conference — the WFP was struggling with an extremely difficult data science problem: how best to get food from the places it was being grown/stored to the people who needed it most. Essentially, the WFP would receive reports of needs from communities facing food insecurity. One community might report a need for bread and beef, while another might request lentils and chicken. The WFP would compile these requests and then set about trying to determine the most efficient way to meet these needs.

\sphinxAtStartPar
This type of logistics problem is an example of a notoriously difficult problem (essentially a version of the Traveling Salesman Problem, which is NP\sphinxhyphen{}Complete, if that means anything to you) that companies like FedEx and UPS buy supercomputers to address. But this particular problem was made extra challenging by all the different types of food the WFP was trying to provide communities.

\sphinxAtStartPar
What the WFP realized was that they didn’t actually need to provide bread to the village asking for bread. See, humans don’t need \sphinxstyleemphasis{bread} to avoid starvation — they need a certain number of calories, a certain amount of protein, and a handful of other nutrients.%
\begin{footnote}[3]\sphinxAtStartFootnote
As I understand it, calcium, iron, vitamins A, B1, B2, C, and niacin.
%
\end{footnote} So when a village asks for bread, rice, or wheat, those requests can be \sphinxstyleemphasis{converted} into requests for carbohydrates. And when a village asks for beef or beans, those requests can be converted into requests for protein and iron. So by simply abstracting the task from “How best can we meet all these food requests?” to “How best can we meet the nutritional needs indicated by these requests?” the WFP was able to \sphinxstyleemphasis{dramatically} reduce the number of constraints being imposed on the logistical optimization problem WFP needed to solve, making its task \sphinxstyleemphasis{far} simpler.

\sphinxAtStartPar
How far should you abstract things? To the level that feels most useful. Yes, in the private sector you \sphinxstyleemphasis{can} abstract almost any problem to “how do we maximize the lifetime discounted profits of the company,” but I don’t think it would be controversial to say that that formulation is not particularly useful if you’re trying to pick the best box sizes for your company to stock for online orders. But there’s also no real cost to thinking about your problem at that level of generality for a little while, so my advice would be: if you haven’t gotten to a level of abstraction that feels silly, you probably haven’t abstracted enough.


\subsection{3. Let Your Articulation Change}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:let-your-articulation-change}}
\sphinxAtStartPar
My last suggestion is to allow your understanding of the problem you are trying to solve evolve. As your project develops, you will learn new things about your problem context, and you should allow those to inform how you are thinking of your problem. This is true not only for you — the data scientist — but also your stakeholder. We’ll discuss stakeholder management more in our next chapter, but you should regularly be asking them: “does the way we’ve articulated the problem we’re trying to solve still feel right to you given everything we’ve learned?”


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Solving The Wrong Problem: Examples}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:solving-the-wrong-problem-examples}}\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem::doc}}
\sphinxAtStartPar
In the last section, we discussed \sphinxstyleemphasis{why} understanding your problem is so important, reasons data scientists often fail to do so, and a few suggestions for ways to improve your understanding of the problem you are trying to solve.

\sphinxAtStartPar
In this section, we will work a longer example to illustrate what “getting the problem wrong, then adapting and getting it right” looks like in practice.


\subsection{Pizza Ltd. Advertising}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:pizza-ltd-advertising}}
\sphinxAtStartPar
Pizza Ltd. is a (fictitious) pizza delivery chain interested in improving their online sales. Last year they increased their online advertising budget three\sphinxhyphen{}fold, but saw almost no change in their online sales, despite increasing in\sphinxhyphen{}store sales.

\sphinxAtStartPar
You have been hired to help improve the effectiveness of their advertising. Pizza Ltd. provides you with data on their previous advertising campaigns, including information on ad impressions and clicks broken down by user demographics, geography, and past interactions with Pizza Ltd.

\sphinxAtStartPar
“Well,” you reason, “maybe the problem is that Pizza Ltd’s ads aren’t being shown to the right people. After all, it seems unlikely that any ad for pizza—no matter how appealing—is likely to draw a click if it’s shown to a 75\sphinxhyphen{}year\sphinxhyphen{}old at 7 am.” And sure enough, the data provided by Pizza Ltd shows that they are not doing a lot of ad targeting — their ads are being shown to an extremely diverse set of users, including many who probably aren’t that interested in pizza!

\sphinxAtStartPar
Using the data provided, you train a model to answer the question, “given a user’s demographics and online behavior, how likely are they to click on a Pizza Ltd. ad?” You try out a few different models, tune the model parameters, and eventually settle on a neural network model with extremely high precision \sphinxstyleemphasis{and} recall. Hooray!

\sphinxAtStartPar
As expected, the model shows that Pizza Ltd. was showing too many ads to people who were probably not even that interested in pizza, when they should have been targeting people who have ordered pizza from Pizza Ltd in the past, people searching for “Pizza Ltd,” and people who live close to Pizza Ltd locations.

\sphinxAtStartPar
You hand over your model to Pizza Ltd, who immediately reallocate their ads based on their models. Within a week, Pizza Ltd. sees that the share of ad impressions that result in clicks and pizza purchases has increased five\sphinxhyphen{}fold. Everyone congratulates you, and you move on to the next project feeling very smug.


\subsubsection{The Other Shoe Drops}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:the-other-shoe-drops}}
\sphinxAtStartPar
A few months later, you are called into a meeting with the Pizza ltd advertising team, online sales team, and the company’s Chief Financial Officer (CFO). They’ve been looking over the numbers, and despite the huge rise in ad clicks, ad clicks per impression, ad clicks per dollar spent, \sphinxstyleemphasis{and} clicks that result in sales, when they crunch their quarterly sale numbers they find that, to their surprise, overall online sales haven’t risen at all. Moreover, in\sphinxhyphen{}store sales are stable, searches for Pizza Ltd. haven’t declined, and social media sentiment and posting rates all seem stable, suggesting the fact overall sales haven’t risen isn’t related to a decline in overall demand.

\sphinxAtStartPar
So, what went wrong?
\begin{quote}

\sphinxAtStartPar
OK, this is the place in most books where the authors ask you that question, and you look up at the ceiling for a minute, shrug, and then read on.

\sphinxAtStartPar
But I’m really, \sphinxstyleemphasis{really} serious about this: close your laptop, stand up, set a 5\sphinxhyphen{}minute timer on your phone, and go for a walk. Ponder this example. See if you can figure out what’s going on. This is \sphinxstyleemphasis{precisely} the kind of problem you will soon face as a professional data scientist, so why not practice trying to think through the problem?
\end{quote}


\subsubsection{Using Our Problem Refinement Skills}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:using-our-problem-refinement-skills}}
\sphinxAtStartPar
To help us work through this problem, let’s begin by asking the questions we learned in our last reading.

\sphinxAtStartPar
First, \sphinxstylestrong{how will we know if we’re successful?} Implicitly, we were assuming that we would know we were successful if the number of impressions that resulted in clicks rose. But as is clear from the concerns raised by Pizza Ltd’s advertising team, online sales team, and CFO, the fact that clicks rose did \sphinxstyleemphasis{not} indicate success.

\sphinxAtStartPar
What would indicate success? As indicated by both the problem statement at the top of this example and by the description of the concerns raised by the Pizza Ltd. CFO, \sphinxstyleemphasis{we would know we were successful if we saw an increase in online sales.}

\sphinxAtStartPar
OK, but… we got people to click the ads, right? We show ads, we hope people click. And we did a great job of figuring out how to show the ads to people who would click the ads! How is this our fault?

\sphinxAtStartPar
Well, \sphinxstyleemphasis{was} our problem that not enough people were clicking the ads? What is the goal of an advertisement — online or in the real world? Is it to be clicked on?

\sphinxAtStartPar
No — often clicking on an ad is an indicator the ad has worked, but in focusing on that immediate (and easy to measure) outcome, we’re missing the point of ads. So let’s abstract our idea of what we’re trying to accomplish. Are we trying to get people who see one of our ads online to click on that ad? No. Are we trying to get people who see one of our ads online to click that ad and buy a pizza? Closer, but still no.

\sphinxAtStartPar
No, let’s get away from all the specifics of clicks, and clicks that convert into sales. Those are specifics that are distracting us. \sphinxstylestrong{Fully abstracted and generalized}, our problem is that we don’t know how to deploy our ads to \sphinxstylestrong{increase} online sales.

\sphinxAtStartPar
And the way to \sphinxstyleemphasis{increase} sales is to show the ads to the people whose likelihood of buying a pizza will increase the most as a result of seeing the ad. In other words, we want to show our ads to the people on whom they will have the largest \sphinxstyleemphasis{effect} on the likelihood of buying a pizza.

\sphinxAtStartPar
How is this different from maximizing clicks or clicks that turn into sales? Simple — consider a person who \sphinxstyleemphasis{has already decided to buy a pizza from Pizza Ltd.} If they happen to see an ad on their way to buying their pizza, they may click on it to save a few dollars (if there’s a coupon in the ad) or a few keystrokes (have you ever typed the name of a company into google and clicked the top link to get to their homepage — a link that was actually an ad the company paid for?%
\begin{footnote}[1]\sphinxAtStartFootnote
To be clear, I’m not saying that paying for ads at the top of Google for the name of one’s own company are always a bad idea — they may prevent a competitor pizza chain from buying that spot and convincing the searcher to change their plans and order from the competitor instead. Basically, these ads sometimes amount to paying Google to not sell your business to someone else. But that’s a nuance that’s mostly a distraction at this point.
%
\end{footnote}).

\sphinxAtStartPar
Even though that customer clicked the ad, and even though that user bought a pizza, the ad didn’t \sphinxstyleemphasis{cause} them to buy a pizza. In fact, the ad had no effect on the likelihood they’d buy a pizza.%
\begin{footnote}[2]\sphinxAtStartFootnote
If you’ve taken any causal inference courses, you’re reconize that while I’m describing is an “always\sphinxhyphen{}taker” — someone who is going to engage in a behavior regardless of whether they are subject to a treatment of interest (here, encountering an ad) or not.
%
\end{footnote} And if the ad included a coupon, then not only has the ad not increased online sales, but it’s reduced profits from the sale because of the coupon \sphinxstyleemphasis{and} you had to pay for that ad impression and click!

\sphinxAtStartPar
Even though this is \sphinxstyleemphasis{not} someone you want to show an ad too, however, this is precisely the type of user that a naive model designed to target the people most likely to click an ad would suggest targeting. Not because the statistical model did the wrong thing, but because in answering the question you asked it to answer — “what kind of users are most likely to click on a Pizza Ltd. ad?” — wasn’t a question whose answer helped solve your problem.


\subsection{Counter\sphinxhyphen{}Factual Advertising}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:counter-factual-advertising}}
\sphinxAtStartPar
So how should Pizza Ltd. have approached solving their problem? The answer — as we’ll explore in detail in later readings — is that they should have run an A/B experiment. Track a group of users, and show a random subset of those users a Pizza Ltd. ad. Then measure the effect of the ads by comparing online purchase rates between the group that saw ads and the group that didn’t.

\sphinxAtStartPar
This data can then be used to improve targeting by looking at the difference in purchase rates between the group that saw the ad and the group that didn’t for different demographic sub\sphinxhyphen{}populations (younger users, men versus women, users in different geographic areas, etc.). And of course this strategy can also be used to test different ads to figure out what ad is most effective.

\sphinxAtStartPar
This idea — that the goal of ads is to have an \sphinxstyleemphasis{effect} on consumer purchase behavior, not to be clicked on — is often referred to as “counter\sphinxhyphen{}factual advertising,”%
\begin{footnote}[3]\sphinxAtStartFootnote
I’m not sure of the first citation for this idea in relation to online advertising, but \sphinxhref{https://dl.acm.org/doi/10.5555/2567709.2567766}{this is one well\sphinxhyphen{}cited early paper on the topic}.
%
\end{footnote} and it’s the basis for how nearly all major advertising platforms work today.

\sphinxAtStartPar
It’s also why companies like Meta and Google are so eager to track user behavior across apps and websites. To demonstrate the effectiveness of ads, these companies need to be able to not only track users after they click an ad (to see whether they eventually make a purchase), but also track users who \sphinxstyleemphasis{haven’t} seen an ad (so they can establish a behavioral baseline for the “control” group of users who haven’t seen an ad). This allows these companies to estimate the true effect of ads on sales, data they use to improve ad targeting \sphinxstyleemphasis{and} justify higher prices to advertisers.




\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{Stakeholder Management}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:stakeholder-management}}\label{\detokenize{20_problems_to_questions/20_stakeholder_management::doc}}
\sphinxAtStartPar
In our previous two readings, I detailed why it’s critically important to spend as much time understanding the problem you wish to solve as actually developing a solution. But throughout that discussion, I implicitly simplified the discussion by treating you — the reader — as the key stakeholder.

\sphinxAtStartPar
If you’re fortunate enough to be your own stakeholder — either because you are working in a startup, or are an academic doing your own research — congratulations! You get to apply the preceding lessons directly to solve the problem(s) you care about most.

\sphinxAtStartPar
But for everyone else, it’s time to engage with one of the most difficult aspects of being a data scientist: stakeholder management.

\sphinxAtStartPar
For most data scientists, between you and the problem you are being asked to solve will almost always sit a group of key stakeholders. In most cases, the problem you are being asked to solve will actually be \sphinxstyleemphasis{their} problem, they will be the source of most of the domain knowledge you will need to solve their problem successfully, and they will also be the actors who will be responsible for evaluating you at the end of the project.

\sphinxAtStartPar
In this chapter, we will discuss principles and strategies for working effectively with stakeholders. This is a critically important skill — as noted above, these will generally be the people responsible for evaluating you at the end of the project! — but one many data scientists fail to recognize is categorically different from the “professor management” they have been practicing for years. It requires deliberate cultivation, reflection, and a recognition that as a professional data scientist, it is now incumbent upon you to see yourself as a \sphinxstyleemphasis{partner} and \sphinxstyleemphasis{colleague} to the stakeholders you work with, not a simple subordinate.


\section{Principles for Stakeholder Management}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:principles-for-stakeholder-management}}

\subsection{Principle 0: Your Job is to Help Solve Your Stakeholder’s Problem}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:principle-0-your-job-is-to-help-solve-your-stakeholder-s-problem}}
\sphinxAtStartPar
The first Principle of stakeholder management is that you will be evaluated on whether, at the end of your engagement, you’ve solved your stakeholder’s problem, \sphinxstylestrong{not} whether you did precisely what they asked you to do. I’ve made this point twice before, but it is important enough it bears repeating a third time.

\sphinxAtStartPar
That means that if you just follow your stakeholders directions — even when your expertise suggests to you that those directions aren’t the best way to solve a problem — you will \sphinxstyleemphasis{not} find your stakeholder will be delighted by your skill despite finding they are no better off for having hired you.


\subsection{Principle 1: Your Job is to be a Problem\sphinxhyphen{}Solving \sphinxstyleemphasis{Partner}}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:principle-1-your-job-is-to-be-a-problem-solving-partner}}
\sphinxAtStartPar
The biggest mistake that young data scientists make when working with stakeholders for the first time is being overly deferential to their stakeholders. This is certainly an understandable mistake, but a mistake nevertheless, given Principle 0.

\sphinxAtStartPar
Where does this deference come from? I feel is borne from three misperceptions.


\subsubsection{Confusing Domain Expertise for Global Expertise}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:confusing-domain-expertise-for-global-expertise}}
\sphinxAtStartPar
Most stakeholders you work with will be domain experts in their field, and — at least early in your career — will often be more senior. As a result, there is a natural tendency for young data scientists to err on the side of deference.

\sphinxAtStartPar
While it is important to be respectful of your stakeholder’s domain expertise, it is just as important to recognize \sphinxstylestrong{that data science is about \sphinxstyleemphasis{pairing} domain expertise with computational methods}. And while your stakeholder will almost certainly have greater domain knowledge than you, it is very unlikely they will also know more than you about cutting edge quantitative methods. Indeed, if they did, they probably wouldn’t be hiring you!%
\begin{footnote}[1]\sphinxAtStartFootnote
Obviously there are exceptions to this — if you work for a mature tech company like Google or Meta, you may very well end up working under a manager who knows both sides of a problem significantly better than you. In my experience, however, this circumstance is the exception, not the rule.
%
\end{footnote}

\begin{sphinxShadowBox}

\sphinxAtStartPar
Data science is about \sphinxstyleemphasis{pairing} domain expertise with computational methods and quantitative insights, and neither you nor your stakeholder is likely to have expertise in \sphinxstyleemphasis{both} the substantive domain in question \sphinxstyleemphasis{and} cutting edge quantitative methods.
\end{sphinxShadowBox}

\sphinxAtStartPar
One factor that feeds into this misconception is that most stakeholders know \sphinxstyleemphasis{just enough} data science to throw around lots of jargon, giving young data science students the (mistaken) impression that their stakeholders know just as much about data science as they know about their domain. But, again, this is almost never the case.


\subsubsection{Confusing Your Stakeholder for a Professor}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:confusing-your-stakeholder-for-a-professor}}
\sphinxAtStartPar
Because they’ve never been trained to do anything else, many young data scientists treat their stakeholders the same way they would their professors — as people who know what the right answer looks like, and who should thus be deferred to in determining the right way to solve a problem. But this is a mistaken understanding of both (a) what a professor does in class, and (b) what makes someone a stakeholder.

\sphinxAtStartPar
A professor (in theory) is someone whose job is to help you achieve a set of specific learning goals. And when they give you an assignment, that assignment has (again, in theory) been developed to help you achieve those learning goals. Assignments from a professor, in other words, are designed with a clear goal in mind and a structure designed to achieve that goal. As a result, professors do generally know everything (or close to everything) there is to know about how to approach an assignment.

\sphinxAtStartPar
\sphinxstylestrong{A stakeholder, by contrast, is just a person with a problem.} They may have some \sphinxstyleemphasis{ideas} for how to solve their problem, but whatever possible solutions they bring to the table are just that: \sphinxstyleemphasis{ideas}.

\sphinxAtStartPar
Remember: your time as a data scientist is \sphinxstyleemphasis{valuable and expensive}. If your stakeholder already knew how to solve their problem, they wouldn’t be paying you for your time. No, the reason your stakeholder is coming to you is that they don’t know what the solution to their problem will look like, and they need help figuring that out.

\sphinxAtStartPar
That doesn’t mean that a stakeholder won’t come to you with a set of suggestions for how you might start to address a problem, but as we’ll discuss below, these should be viewed as suggestions from a colleague, not directions like those from a professor in a class.


\subsubsection{Not Having Confidence In Your Own Knowledge}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:not-having-confidence-in-your-own-knowledge}}
\sphinxAtStartPar
It is often said that “The more you know, the more you realize you don’t know.”%
\begin{footnote}[2]\sphinxAtStartFootnote
This is commonly attributed to Aristotle, though I don’t think it’s more of a paraphrase of things he wrote than an actual quote.
%
\end{footnote} And I’d be hard\sphinxhyphen{}pressed to find another field where I think this is more common than in data science. The field is expanding so rapidly and in so many directions, that it is only by learning a lot that one can really appreciate the range of frontiers of knowledge that one has only begun to explore.

\sphinxAtStartPar
And on the other side of the spectrum, \sphinxstyleemphasis{far} too many people have fit a regression tree in scikit\sphinxhyphen{}learn and gotten a high accuracy score, and have taken that as evidence that they are expert data scientists.

\sphinxAtStartPar
All of which is to say: young data scientists tend to approach the world with an appropriate humility that comes from serious study. But don’t confuse that appropriate humility for a lack of expertise. And just as importantly, don’t confuse the apparent confidence of others for evidence that they know more than you — all too often, the opposite will be true (the fact confidence and knowledge are often negatively correlated is a cognitive bias referred to as the \sphinxhref{https://en.wikipedia.org/wiki/Dunning\%E2\%80\%93Kruger\_effect}{Dunning\sphinxhyphen{}Kruger effect})!


\subsection{Principle 2: Don’t Assume Your Stakeholder Knows What They Need}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:principle-2-don-t-assume-your-stakeholder-knows-what-they-need}}
\sphinxAtStartPar
A corollary to Step 0 is to not assume your stakeholder understands what they need. So when I say “helping your stakeholder understand their problem is a core part of the job,” I don’t only mean that it’s part of your job \sphinxstyleemphasis{if the stakeholder admits to deep uncertainty about their problem}.” Odds are your stakeholder will come to you with a strong statement of what they think they want, but you should take that as a starting point for discussion, not your mandate.

\sphinxAtStartPar
This is particularly true if your stakeholder comes to you with really specific technical suggestions. Often you will be approached by a stakeholder who, rather than laying out a problem, announces they would like you to do X using some data science tool Y. Occasionally the stakeholder doing this knows exactly what they’re talking about, and you should use Y to do X.

\sphinxAtStartPar
More often, however, you’re dealing with a stakeholder with just enough knowledge to be dangerous (and to drop buzzwords), but not enough to know how best to solve their problem.

\sphinxAtStartPar
Most people ask data scientists for help because they don’t know much about data science (or, worse, they \sphinxstyleemphasis{think} they know about data science but don’t). Again, different rules apply if you’re at Google or Apple, but in most contexts, it’s a good idea to treat implementation details provided by the client as a red herring. Focus on the stakeholder’s \sphinxstyleemphasis{needs}. Only get into implementation details once you feel you understand the problem well.

\begin{sphinxShadowBox}

\sphinxAtStartPar
Focus on the stakeholder’s \sphinxstyleemphasis{needs}. Only get into implementation details once you feel you understand the problem well.
\end{sphinxShadowBox}


\subsection{Principle 3: Stakeholder Time Is Precious}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:principle-3-stakeholder-time-is-precious}}
\sphinxAtStartPar
It is always good to go into meetings with your stakeholder with a clear sense of your objectives — what you hope to communicate, and what information and feedback you need to get before the meeting ends. When your stakeholder is someone you don’t get to meet with regularly, it’s good practice to detail these objectives and provide them — in writing — to your stakeholder in advance of your meeting. This will not only ensure that you and your teammates are on the same page (as you will all have reviewed the document before sending it to your stakeholder), but also ensure that your stakeholder has adequete time to reflect on any questions or issues you wish to raise.

\sphinxAtStartPar
When it comes to your \sphinxstyleemphasis{first} meeting, however, this practice can feel impractical as you may feel so uncertain about the project that you only know the first few questions you want to ask.

\sphinxAtStartPar
But even in a first meeting, preparation is key. Rather than laying out the new issues you wish to raise and questions you want answered, for a first meeting it’s helpful to write out a full \sphinxstyleemphasis{tree} of lines of inquiry you may wish to propose. In other words, for every question you wish to pose to your stakeholder, try to anticipate some likely responses they make provide, then write down a few followup questions to ask if they provide one of those responses.

\sphinxAtStartPar
Time with your stakeholder is \sphinxstyleemphasis{precious}, especially early in a project, make the most of that face time through preparation.


\subsection{Principle 4: Iterate}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:principle-4-iterate}}
\sphinxAtStartPar
And here’s the last but perhaps most important step: \sphinxstylestrong{iterate.} Bring your work back to your stakeholder as often as possible.

\sphinxAtStartPar
Many stakeholders find the idea of data science mysterious and abstract and will struggle to understand what is and is not feasible. By bringing them intermediate results, the whole process will start to become more concrete for the stakeholder, and it will help them provide you with better feedback.

\sphinxAtStartPar
The way this book is organized suggests a natural flow from problem articulation to answering Exploratory Questions to prioritize efforts, to answering Passive\sphinxhyphen{}Prediction Questions to target individuals for extra attention or automate tasks, and finally to Causal Questions to better understand the effects of that extra attention/automation. In reality, however, a good data scientist is always coming back to the stakeholder, updating their plan, and jumping back in the sequence when new questions arise.


\section{Strategies for Stakeholder Communication}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:strategies-for-stakeholder-communication}}

\subsection{Step 3: Ask Questions (Especially Quantitative Ones!)}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:step-3-ask-questions-especially-quantitative-ones}}
\sphinxAtStartPar
Be sure to ask a lot of questions of your stakeholder. In particular, I would suggest two types: questions about what success would look like, and questions about the problem itself.


\subsection{Questions About Success}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:questions-about-success}}
\sphinxAtStartPar
Getting a sense of where the goalposts are for your stakeholder will both help you know what to target and also help you better understand your stakeholder’s understanding of the problem. Make sure to ask questions like:
\begin{itemize}
\item {} 
\sphinxAtStartPar
How are you measuring the problem? What would you measure to help you know if you were successful in solving the problem?

\item {} 
\sphinxAtStartPar
How big, in quantitative terms, is this problem?

\item {} 
\sphinxAtStartPar
How much would you need the current situation to change to call this a success?

\end{itemize}


\subsection{Questions About the Problem}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:questions-about-the-problem}}
\sphinxAtStartPar
The more you know about your client’s needs the better, so ask anything that comes to mind. If the client can answer your question, it will help you better understand the situation; if the client can’t answer your question you may find that they are suddenly really interested in knowing the answer, and you immediately have some of your first Exploratory Questions to try to resolve.

\sphinxAtStartPar
In the example of the company that wanted to improve recruitment of high\sphinxhyphen{}quality employees in the introduction of this book, I suggested that some of the first exploratory questions you might want to investigate would be things like:
\begin{itemize}
\item {} 
\sphinxAtStartPar
How many job applications are you receiving when you post a job?

\item {} 
\sphinxAtStartPar
What share of your current job applicants are of high quality?

\item {} 
\sphinxAtStartPar
What share of employees you try to hire accept your offer?

\item {} 
\sphinxAtStartPar
What share of employees you do hire turn out to be successful employees?

\end{itemize}

\sphinxAtStartPar
These are all questions that I would ask my stakeholder in one of our first meetings.


\subsection{Strategy 1: Propose Questions You Might Answer}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:strategy-1-propose-questions-you-might-answer}}
\sphinxAtStartPar
As a data scientist, answering questions about the world is the instrument you have to solve problems. So once you think you have a sense of your stakeholder’s needs, turn around and propose a handful of questions and ask them if answering those questions would help solve their problem.

\sphinxAtStartPar
This is important because many people have only a vague sense of what they are likely to get as a “deliverable” from the data scientist. They usually have a vague sense that they will get some type of magic machine (a “magic model” or “magic algorithm”) that will just make their problem go away. By concretely framing your deliverable as the answer to a question (or a model that would answer a specific question for each entity like a customer or patient that it encounters), you can get much more valuable feedback before you dive into a problem.


\subsection{Strategy 2: Make Your Questions Specific and Actionable}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:strategy-2-make-your-questions-specific-and-actionable}}
\sphinxAtStartPar
{[}tend to {]}

\sphinxAtStartPar
In developing your questions, it is important to make them specific and actionable. A specific and actionable question makes it very clear what you need to do next. For example, suppose an international aid organization told you they were worried that urbanization in Africa, Asia, and Latin America was impacting efforts to reduce infant mortality. Some examples of specific, actionable questions are: “Is infant mortality higher among recent migrants to urban centers, controlling for income?” or “Are the causes of infant mortality among recent migrants to urban centers different from those living in rural areas?” Reading those questions, you can probably immediately think of what data you’d need to collect, and what regressions you’d want to run to generate answers to those questions.

\sphinxAtStartPar
Vague questions would be “Is urbanization impacting efforts to reduce infant mortality?”, or “Does urbanization affect infant mortality?” Note that when you read these, they don’t seem to obviously imply a way forward.

\sphinxAtStartPar
Perhaps the best way to figure out if your question is answerable is to write down what an answer to your question would look like. Seriously – try it. Can you write down, on a piece of paper, the graph, regression table, or machine learning diagnostic statistics (complete with labels on your axes, names for variables, etc.) that would constitute an answer to your question? If not, it’s probably too vague.


\section{Reading Reflection Questions}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:reading-reflection-questions}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Why should you care if your stakeholder misspecifies their problem?

\end{itemize}


\bigskip\hrule\bigskip


\sphinxstepscope


\part{Types of Questions}

\sphinxstepscope


\chapter{Descriptive v. Prescriptive Questions}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:descriptive-v-prescriptive-questions}}\label{\detokenize{30_questions/05_descriptive_v_prescriptive::doc}}
\sphinxAtStartPar
Welcome to Part 3: Types of Questions! Now that we’ve discussed how data science is all about solving problems, and how refining your understanding of the problem you wish to solve is key to success, it’s time to turn our attention to how we, as data scientists, solve problems: by using quantitative methods to answering questions about the world!

\sphinxAtStartPar
In the following chapters, we will be diving into the intricacies of Exploratory, Passive\sphinxhyphen{}Prediction, and Causal Questions in detail. We will discuss the role each of these question types plays in solving problems, and the issues that arise when trying to answer them.

\sphinxAtStartPar
First, however, we need to discuss another important concept: the distinction between “descriptive” and “prescriptive” questions (also referred to as “positive” and “normative” questions, respectively, in some circles, such as the social sciences).


\section{Descriptive Questions}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:descriptive-questions}}
\sphinxAtStartPar
Up until this point, all the questions we’ve discussed have been examples of Descriptive Questions. Descriptive Questions are questions about the empirical state of the world, and answering Descriptive Questions is the bread and butter of data science. “Do high\sphinxhyphen{}income and low\sphinxhyphen{}income countries emit similar amounts of carbon dioxide?,” “What is the likelihood a 78\sphinxhyphen{}year\sphinxhyphen{}old male patient undergoing kidney surgery will experience a post\sphinxhyphen{}surgical infection?,” and “What kinds of users are clicking our ads?” are all examples of Descriptive Questions.

\sphinxAtStartPar
Because Descriptive Questions are about the empirical state of the world around us, they have right and wrong answers. Actually generating answers to these questions may be difficult, of course, and due to differences in things like empirical strategies or study populations, different data scientists may generate different answers to the same Descriptive Question. At least in principle, however, if you are willing to agree there exists a single shared empirical reality around us,%
\begin{footnote}[1]\sphinxAtStartFootnote
If you know enough epistemology to object to my asserting the existence of an “objective state of the world,” then I assume you can also understand the point I’m trying to get across in this chapter and will forgive me for this philosophical slight.
%
\end{footnote} then Descriptive Questions can be thought of as having right and wrong (or at least more or less “accurate”) answers.

\sphinxAtStartPar
But Descriptive Questions are not the only type of question you will encounter as a data scientist. Stakeholders, corporate strategists, executives, and doctors may often ask you for a recommendation — in other words, they are asking you what they should do. And when they ask you that question, they are no longer asking you a \sphinxstyleemphasis{Descriptive Question}, they are now asking you a \sphinxstyleemphasis{Prescriptive Question}.


\section{Prescriptive Questions}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:prescriptive-questions}}
\sphinxAtStartPar
A \sphinxstyleemphasis{Prescriptive Question} is a question about what someone should do or what someone ought to do. Unlike Descriptive questions, Prescriptive Questions are not purely empirical in nature — rather, they require the person answering the question to make a value judgment about the desirability of different potential outcomes. And because there is no single “correct” value system in the world everyone agrees on, there are also no universally right and wrong answers to Prescriptive Questions.

\sphinxAtStartPar
To illustrate the role of values in answering Prescriptive Questions, suppose you have been asked whether the US Food and Drug Administration should authorize a new drug. This drug is 95\% effective in reducing the incidence of a chronic respiratory condition that prevents sufferers from engaging in strenuous sports. But this drug also causes permanent paralysis in 1\% of patients who take the drug. Should (there’s that magic “should” word?) this drug be authorized?

\sphinxAtStartPar
In answering this question, note that we’ve started with a description of some empirical facts — the drug is 95\% effective, and the drug also causes permanent paralysis in 1\% of people who take the drug. You can think of this component as the answer to the Descriptive Question “What effects — positive and negative — does this drug have on the people who take it?” There is, most of us would agree, a right answer to this question. And assuming the clinical trial that generated those results was well run, we would expect another clinical trial — one conducted on similar patients, with the same dosing regime, similar quality, etc. — to generate similar results, because most of us believe there exists an \sphinxstyleemphasis{empirical truth} about the effect of the drug.%
\begin{footnote}[2]\sphinxAtStartFootnote
I recognize that in asserting the existence of “empirical truth” I may be offending some readers. As will the discussion above, I suspect that if you are familiar with the philosophical issues raised by me invoking an empirical truth, you will are also probably comfortable with the distinction between prescriptive and descriptive questions and will forgive me this simplification.
%
\end{footnote}

\sphinxAtStartPar
But is there a “correct” answer to the question “should the FDA authorize this drug?” I would argue not — I don’t think I would have too much difficulty finding people who would be quite willing to take both sides of that question. Why? Because different people likely hold different views on the relative value of patients being able to engage in sports and exercise versus the value of 1\% of patients being permanently paralyzed. Different people likely also hold different views on the relative value of patients being allowed to make those choices for themselves is greater than the value of avoiding the possibility that some patients may make the decision to take the drug without fully understanding what life with paralysis entails, and who would later regret their decision when they come to better understand the risk they were taking.


\subsection{Prescriptive Questions and Ethics}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:prescriptive-questions-and-ethics}}
\sphinxAtStartPar
If you look up Prescriptive Questions, you will probably find a set of examples that emphasize things we think of as ethical or moral questions. “Should higher income and lower income countries be expected to meet the same carbon emission reduction standards?” or “Do high\sphinxhyphen{}income countries have a moral obligation to provide tuberculosis drugs to developing countries for free (or at cost)?”

\sphinxAtStartPar
As you will note, however, in the drug example above I’ve avoided asking what is the \sphinxstyleemphasis{moral} choice, and instead emphasized that one’s answer depends on the relative \sphinxstyleemphasis{value} one places on different things (“how do you \sphinxstyleemphasis{value} a patient being able to live a full life that includes exercise?”). That’s because when one encounters terms like “ethical question” or “moral question,” one immediately jumps to really deep philosophical questions of life, suffering, altruism, and inequality.

\sphinxAtStartPar
But questions that have nothing to do with those heavy topics can be Prescriptive Questions too. For example, suppose you’ve been asked to choose between:
\begin{itemize}
\item {} 
\sphinxAtStartPar
(a) a business strategy that increases profits in the short run, but may harm how consumers view your brand, and as a result reduce long term profits, or

\item {} 
\sphinxAtStartPar
(b) a business strategy that protects how consumers view your brand, foregoing some profits in the short run to protect long term value.

\end{itemize}

\sphinxAtStartPar
This doesn’t \sphinxstyleemphasis{feel} like the same type of question as “should the FDA authorize this new drug.” Just like that question, however, this question asks you to decide the relative \sphinxstyleemphasis{value} you place on long term profit versus the \sphinxstyleemphasis{value} you place on short term profit. Or, since the future is uncertain, it may as you to decide the relative \sphinxstyleemphasis{value} you place on relatively certain short term profits versus the \sphinxstyleemphasis{value} you place on less certain but potentially larger long\sphinxhyphen{}term profits.

\sphinxAtStartPar
And that’s what makes a Prescriptive Question different from a Descriptive Question: Descriptive Questions ask you to measure the world, while Prescriptive Questions ask you to decide on the \sphinxstyleemphasis{relative values} of different outcomes.

\sphinxAtStartPar
Does that mean not all Prescriptive Questions are ethical questions, like you may see written around the internet? Well… I think a moral philosopher would probably argue that how we assign value to different things (which is how we decide what we \sphinxstyleemphasis{should} or \sphinxstyleemphasis{ought} to do) is at the root of all moral and ethical systems. Consequently, this is probably more a matter of semantics than substance — all Prescriptive Questions require weighing choices and assigning value to different outcomes. A philosophy would probably say that does make them all ethical, while a layperson may not.


\subsection{Trade\sphinxhyphen{}Offs, Decisions, Descriptive Questions, and Prescriptive Questions}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:trade-offs-decisions-descriptive-questions-and-prescriptive-questions}}
\sphinxAtStartPar
The two examples given above illustrate a more general feature of Descriptive and Prescriptive Questions when it comes to making decisions:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Descriptive Questions help clarify the likely \sphinxstyleemphasis{consequences} of different courses of action, but cannot tell you which is preferable, while

\item {} 
\sphinxAtStartPar
Prescriptive Questions are about assigning values to different outcomes and deciding on their desirability accordingly.

\end{itemize}


\section{Descriptive and Prescriptive Questions in Data Science}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:descriptive-and-prescriptive-questions-in-data-science}}
\sphinxAtStartPar
The focus of this book is on Descriptive Questions. This is not because Prescriptive Questions are unimportant — indeed, one can easily make the argument that they are \sphinxstyleemphasis{more} important than Descriptive Questions. Moreover, as we will discuss in future chapters, they will arise frequently in your career as a data scientist.

\sphinxAtStartPar
No, the reason that Descriptive Questions are the focus of this book is that data science tools can \sphinxstyleemphasis{only} answer Descriptive Questions. No neural network, logistic regression, or clustering algorithm can ever decide on the relative value you should place on 1/100 patients being paralyzed versus 95/100 being cured. They can help you better understand things like whether the cost of caring for paralyzed patients outweigh the saved cost on patients with respiratory issues for the health system (a Descriptive Question), but not how to weigh those financial considerations against patient quality of life.

\sphinxAtStartPar
To be clear, this does not mean that the answers you generate as a data scientist will not have a \sphinxstyleemphasis{bearing} on how people answer Prescriptive Questions. Data science would be a very dull field indeed if it could not speak to the ethical issues of our day. Data science is powerful precisely because it can inform how we answer Prescriptive Questions by helping us understand the relevant stakes. Data science tools can help decision\sphinxhyphen{}makers understand the likely \sphinxstyleemphasis{consequences of different courses of action}, information that can help people make \sphinxstyleemphasis{informed} decisions about what outcomes they feel are most desirable. But they cannot tell you what course of action is best, and anyone who tells you otherwise hasn’t been thoughtful enough about how they make decisions.


\subsection{Then Why’d You Bring Them Up?}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:then-why-d-you-bring-them-up}}
\sphinxAtStartPar
If the focus of this book is Descriptive Questions, why this digression on Prescriptive Questions?

\sphinxAtStartPar
To help you begin to recognize when the question you are being asked is a Descriptive Question — with a right or wrong answer, and where your expertise as a data scientist put you in a unique position of authority — or a Prescriptive Question, where answers depend on how one chooses to value different outcomes, and where one’s expertise as a data scientist really \sphinxstyleemphasis{doesn’t} provide you with any unique standing or authority.


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Descriptive and Prescriptive Questions In Practice}
\label{\detokenize{30_questions/06_descriptive_prescriptive_examples:descriptive-and-prescriptive-questions-in-practice}}\label{\detokenize{30_questions/06_descriptive_prescriptive_examples::doc}}
\sphinxAtStartPar
Below are two vignettes of groups struggling to answer important questions about the world. Let’s read them to see how Descriptive and Prescriptive Questions interact in the context of real world problems, and how you (as a data scientist) can wield your skills to be especially impactful.


\subsection{Opioid Reductions}
\label{\detokenize{30_questions/06_descriptive_prescriptive_examples:opioid-reductions}}
\sphinxAtStartPar
You have been hired by a medical regulatory board concerned about the rise in opioid overdoses. They are debating whether they \sphinxstyleemphasis{should} (there’s that magic word!) make it harder for patients to get opioids. Fundamentally, however, they worry that while restrictions on opioids may reduce overdoses and addiction, they may also prevent some patients with very real pain conditions from getting the care they need.

\sphinxAtStartPar
Why are they stuck?
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
\sphinxAtStartPar
They may be unsure how to value overdose prevention relative to ensuring appropriate patient access to opioids, and/or

\item {} 
\sphinxAtStartPar
they may also be unsure about \sphinxstyleemphasis{how much} opioid regulations that reduce overdoses by a certain amount would limit access for patients in need.

\end{enumerate}

\sphinxAtStartPar
The first of these sources of uncertainty is about a Prescriptive Question — if you could prevent one overdose death at the expense of preventing 10 patients in pain from getting the opioids they need, would you accept that trade\sphinxhyphen{}off? You may have an opinion on that question, but unless you have experience with overdoses, addiction, or chronic pain, you probably don’t have much to offer to a room full of doctors who have extensive experience with how these things impact patients and their families.

\sphinxAtStartPar
But the second source of uncertainty could be resolved by answering a Descriptive Question — what is the effect of opioid regulations on appropriate patient opioid access? That’s a question which you — the data scientist — is uniquely positioned to answer! You could study policies that have been implemented in the past and come up with a rigorous estimate of how much opioid regulations that reduce overdoses also reduce access for patients in need. You could also evaluate different kinds of policies to figure out which is most efficient — maybe some policies (like not allowing any opioid prescriptions at all) are good at stopping overdose deaths but also \sphinxstyleemphasis{really} limit appropriate access, while other policies are similarly good at reducing overdoses but have a much smaller effect on limiting access.


\subsection{The Example of Carbon Emissions}
\label{\detokenize{30_questions/06_descriptive_prescriptive_examples:the-example-of-carbon-emissions}}
\sphinxAtStartPar
A profoundly difficult Prescriptive Question in debates over carbon reduction is whether developing countries should be held to the same emission reduction targets as more developed countries. On the one hand, developing countries like China and India are the source of most current growth in carbon emissions, and so policies that do not apply to developing countries are unlikely to prevent many of the worst climate change outcomes. On the other hand, these countries produce radically less carbon \sphinxstyleemphasis{per capita} than Europe or the United States, and the industrial growth creating those emissions has been a major factor in lifting billions of people out of extreme poverty.

\sphinxAtStartPar
Hard choices indeed! How does one weigh the improvements in the quality of life of those in extreme poverty against the possible consequences of even greater climate catastrophes?

\sphinxAtStartPar
While that question is, in part, a Prescriptive Question that no regression can answer, data scientists \sphinxstyleemphasis{can} bring data to bear on this question indirectly by helping everyone understand the potential consequences of different carbon targets for developing countries and the feasibility of different strategies for carbon reduction. A data scientist could, for example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Evaluate the effectiveness of different messages politicians in the US and Europe could use to convince their constituents to support greater carbon reduction targets,

\item {} 
\sphinxAtStartPar
Quantify the magnitude of the effect on global warming caused by different emissions targets for developing countries to help politicians in developing countries weigh the poverty\sphinxhyphen{}reducing benefits of carbon\sphinxhyphen{}intensive industrialization against the likely direct effect of flooding, droughts, or more severe storms on their own citizens, or

\item {} 
\sphinxAtStartPar
Estimate the cost\sphinxhyphen{}effectiveness of developed countries sharing lower emissions industrial technologies with developing countries to ameliorate the tradeoff between poverty reduction and emissions.

\end{itemize}

\sphinxAtStartPar
In each of these cases, the data scientist is only answering Descriptive Questions, but in doing so they are helping everyone better understand the consequences of their decisions, and in doing so (hopefully) help the world to make more informed decisions about the trade\sphinxhyphen{}offs they are making.


\subsection{Recap}
\label{\detokenize{30_questions/06_descriptive_prescriptive_examples:recap}}
\sphinxAtStartPar
Answering Descriptive Questions — questions about how the world is or would be in different scenarios — is the core competency of the data scientist. In the chapters that follow, we will explore in detail three different kinds of Descriptive Questions: Exploratory, Passive\sphinxhyphen{}Prediction, and Causal Questions.

\sphinxAtStartPar
While these are the only types of questions that data science tools can answer directly, it is important for you, the data scientist, to also recognize when you encounter Prescriptive Questions — that is, questions about how the world \sphinxstyleemphasis{should} be, or what we \sphinxstyleemphasis{ought} to do. These questions can only be answered with respect to a system of values, and as such, do not have right or wrong answers, and cannot be answered by statistical means. Nevertheless, as a data scientist, you are well\sphinxhyphen{}prepared to help others (and yourself!) make more informed choices when they decide how to answer Prescriptive Questions for themselves.

\sphinxstepscope


\chapter{Using Exploratory Questions}
\label{\detokenize{30_questions/10_using_exploratory_questions:using-exploratory-questions}}\label{\detokenize{30_questions/10_using_exploratory_questions::doc}}
\sphinxAtStartPar
In Chapters 3 and 4, we covered a number of strategies for better understanding your problem. We discussed how trying to abstract or generalize the problem can often help one realize the problem one faces isn’t as unique as you might have thought, and how focusing on what would constitute success can help you think about a problem free from implementation details you may not have realized you were assuming were inescapable. We also talked about how to extract information from your stakeholder about the problem context. But what \sphinxstyleemphasis{then}? Those are all great strategies for thinking about the problem differently, but you can only get so far just thinking about the problem. What can you \sphinxstyleemphasis{do}?

\sphinxAtStartPar
Enter \sphinxstyleemphasis{Exploratory Questions}. Exploratory Questions are questions designed to elicit information about our problem space and aid us in prioritizing our efforts and refining our goals. Exploratory Questions are questions about patterns in the world. “What types of customers spend the most in our stores?,” “which hospitals have the highest and lowest complication rates?,” “what share of greenhouse emissions come from short, exceptional periods of peak energy demand and what share come from baseload generation?” are all examples of Exploratory Questions.

\sphinxAtStartPar
Of the three classes of questions we detail in this book, answering Exploratory Questions often (though not always) requires the least technical sophistication. As a result, Exploratory Questions also often get the least respect. But because of their critical role in improving our understanding of our objectives, learning to ask and answer Exploratory Questions will have a huge influence on your effectiveness as a data scientist.


\section{Code Optimization: An Example}
\label{\detokenize{30_questions/10_using_exploratory_questions:code-optimization-an-example}}
\sphinxAtStartPar
One of my favorite examples of the value of Exploratory Questions to give data scientists relates to the practice of \sphinxstyleemphasis{code optimization} — the process of adjusting code to reduce the amount of time it takes to run. Code optimization may feel a long way from figuring out how to minimize surgical complications or target advertisements, but it is very familiar to young data scientists, and illustrates some important aspects of how to solve underdefined problems effectively.

\sphinxAtStartPar
Data science is full of computationally intensive tasks that, if approached incorrectly, can leave a data scientist staring at their computer for hours, days, or even weeks (if they allow it). As a result, most data scientists will go through a phase in their development when they start constantly worrying about how to make every line of code they write as fast as possible. They bend over backward to write unnatural, unreadable code to ensure that they aren’t wasting a single CPU clock cycle.

\sphinxAtStartPar
The problem with this is that humans have \sphinxstyleemphasis{incredibly} bad intuition about what tasks take a computer a long time. It turns out that even in programs that take huge amounts of time to run, it is often the case that \sphinxstyleemphasis{most} of the program’s runtime is taken up by a single function or loop. As a result, programmers who fixate on ensuring every line of code they write is optimized for speed end up not only wasting their \sphinxstyleemphasis{own} time, but also writing code that is less natural, harder to maintain, and more likely to contain errors for effectively no benefit.

\sphinxAtStartPar
Indeed, no less a figure than \sphinxhref{https://en.wikipedia.org/wiki/Donald\_Knuth}{Donald Knuth}, one of the greatest programmers in history and author of the famous \sphinxhref{https://en.wikipedia.org/wiki/The\_Art\_of\_Computer\_Programming}{\sphinxstyleemphasis{The Art of Computer Programming}}, famously said of this trying to optimize each line of code at the time it is being written (“premature optimization”):
\begin{quote}

\sphinxAtStartPar
The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; \sphinxstylestrong{premature optimization is the root of all evil (or at least most of it) in programming.} {[}emphasis added{]}
\end{quote}

\sphinxAtStartPar
So what is a programmer interested in performance to do?

\sphinxAtStartPar
Ask Exploratory Questions of their code, of course!

\sphinxAtStartPar
First, programmers write code in as natural a way as possible. Then, \sphinxstyleemphasis{if} the result is code that is slower than they would like, ask the Exploratory Question: “What lines of code are contributing most to this program taking so long to run?” Then, once the programmer has identified the problematic parts of their code, they seek to optimize it for performance.

\sphinxAtStartPar
This is accomplished using a tool called a \sphinxstyleemphasis{profiler}, which dips into a running program every few milliseconds to see what functions are currently running. Then after the program has finished running, it reports how often each part of the program code was found to be running, giving the user a sense of the overall distribution of time spent running different parts of the code.

\sphinxAtStartPar
But the details of \sphinxstyleemphasis{how} profilers answer the question of “what part of my code is contributing most to my program’s run time?” is not the important thing here: it’s the way in which this Exploratory Question — which helps the programmer understand how much time different lines of code take to run \sphinxstyleemphasis{in relation to one another} — helps guide everything the programmer does next.

\sphinxAtStartPar
A programmer who fails to ask and answer this question may instead find themselves spending hours re\sphinxhyphen{}writing functions that are complicated in terms of the structure of their code, but which actually contribute only trivially to the overall runtime of the program. And that is also the same fate that awaits the data scientists who dives into “solving” a problem too quickly without first ensuring they understand the lay of the land.




\section{Using Exploratory Questions to Prioritize Efforts}
\label{\detokenize{30_questions/10_using_exploratory_questions:using-exploratory-questions-to-prioritize-efforts}}
\sphinxAtStartPar
If the preceding example feels too niche — you want to be a data scientist, after all, not a software engineer! — let’s consider a different example. Suppose you’ve been hired by a new non\sphinxhyphen{}profit interested in helping reduce energy use in buildings in the United States. They know that fixed structures (factors, stores, houses, etc.) are responsible for a huge share of US energy consumption, and are interested in figuring out how to drive down that energy use by helping building owners improve the energy efficiency of their buildings (by providing information on things like government subsidies for efficiency improvements and the potential value of energy efficient windows, better heating and cooling, etc.).

\sphinxAtStartPar
You \sphinxstyleemphasis{could} start out by trying to build a fancy supervised machine learning model that tried to predict the energy use of every building in the US based on infrared satellite data and weather information. Indeed, that may even be what you were asked to do! (See our discussion of how {\hyperref[\detokenize{30_questions/10_using_exploratory_questions:../20_problems_to_questions/30_solving_the_right_problem}]{\sphinxcrossref{\DUrole{xref,myst}{stakeholders will often have somewhat wild ideas of what is feasible and what would help most.}}}}).

\sphinxAtStartPar
But given this is a new non\sphinxhyphen{}profit, it sounds like their real need is probably to figure out how to target their efforts to be most effective. So maybe we should step back and start by trying to answer a few Exploratory Questions that would help the organization decide where to focus its attention:
\begin{itemize}
\item {} 
\sphinxAtStartPar
What \sphinxstyleemphasis{type} of buildings (industrial, residential, commercial) consume the most power in the US?
\begin{itemize}
\item {} 
\sphinxAtStartPar
The answer to this question can help you prioritize the \sphinxstyleemphasis{types} of buildings on which to focus your efforts. For example, if industrial or commercial buildings only represent a few percent of all energy consumed by buildings, you don’t need to worry about addressing their needs!

\end{itemize}

\item {} 
\sphinxAtStartPar
In what \sphinxstyleemphasis{region} of the US are buildings consuming the most power?
\begin{itemize}
\item {} 
\sphinxAtStartPar
If most energy is being consumed in a specific area, perhaps the non\sphinxhyphen{}profit should start by focusing its efforts regionally.

\end{itemize}

\item {} 
\sphinxAtStartPar
Is there a \sphinxstyleemphasis{region} of the US where buildings are generating the most CO2?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Not all power is created equal when it comes to climate change! Maybe buildings in California consume a lot of energy, but because they have cleaner power plants, those buildings are indirectly generating less CO2 than those in states in the US South?

\end{itemize}

\item {} 
\sphinxAtStartPar
Does the \sphinxstyleemphasis{average energy use per building} vary by region or building type?
\begin{itemize}
\item {} 
\sphinxAtStartPar
If the non\sphinxhyphen{}profit plans to approach building owners, it may be easier to have an impact working with a few owners of large buildings than lots of residential homeowners. But of course, that also depends on the answer to our previous question about what types of buildings are using the most power/generating the most CO2!

\end{itemize}

\item {} 
\sphinxAtStartPar
In what season is most building energy consumed? Is more energy consumed by heating or AC needs, or do the two use similar amounts of power?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Again, this may impact both the regions the non\sphinxhyphen{}profit may wish to focus on, and also the types of efficiency retrofits they may wish to prioritize.

\end{itemize}

\item {} 
\sphinxAtStartPar
Where is power most expensive?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Building owners are most likely to be interested in efficiency retrofits when power is expensive.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
While answering these questions is likely to require some significant detective work, and may require some thoughtful data wrangling, none require deeply sophisticated statistical machinery. But that doesn’t mean answering these questions wouldn’t provide \sphinxstylestrong{huge} value to the stakeholder.


\section{Goals, Not Methods}
\label{\detokenize{30_questions/10_using_exploratory_questions:goals-not-methods}}
\sphinxAtStartPar
What makes a question an Exploratory Question? Is it the fact that you can answer it with a simple summary statistic, or a scatter plot?

\sphinxAtStartPar
No — what makes something an Exploratory Question is its \sphinxstyleemphasis{purpose}, not the tools we use to serve that purpose. Some Exploratory Questions will be answered through simple cross\sphinxhyphen{}tabulations. But if the patterns one seeks to understand are more subtle or contingent, they may only be answerable with regressions or unsupervised machine learning algorithms.


\subsection{Collecting, Merging, and Creating New Data}
\label{\detokenize{30_questions/10_using_exploratory_questions:collecting-merging-and-creating-new-data}}
\sphinxAtStartPar
The work of answering Exploratory Questions is also often not in the modelling, but rather in the data collection and merging. Because Exploratory Questions are about understanding how different features relate to one another, collecting and merging datasets that have not previously been pulled together is often key to generating answers. Sometimes this data collection requires no more than finding people who already have the data you need, getting it, and finding a way to merge different data sources (e.g., data on power plant CO2 emissions and data on building energy use), while in other situations this will entail building new datasets yourself by doing things like using Natural Language Processing to make collections of documents (contracts, patient files, public records) analyzable systematically.


\subsection{But Where Do I Get Data?}
\label{\detokenize{30_questions/10_using_exploratory_questions:but-where-do-i-get-data}}
\sphinxAtStartPar
It’s hard to overstate how much data is public these days, but to give you a sense, \sphinxhref{https://www.unifyingdatascience.org/html/public\_data.html}{here’s a quick summary of a few terrific sources.}

\sphinxstepscope


\section{EDA: The Most Pernicious Term in Data Science}
\label{\detokenize{30_questions/07_eda:eda-the-most-pernicious-term-in-data-science}}\label{\detokenize{30_questions/07_eda::doc}}
\sphinxAtStartPar
After finishing the last section on Exploratory Questions, some readers may be rolling their eyes thinking “yeah, yeah, EDA {[}Exploratory Data Analysis{]}. I learned about EDA in my first stats class. When do we get to the good stuff?”

\sphinxAtStartPar
If that’s you, or if you just don’t feel clear on the distinction between answering Exploratory Questions and “Exploratory Data Analysis,” commonly referred to by the acronym “EDA,” this section is for you.

\sphinxAtStartPar
In my view, and that of many of my colleagues, the practice commonly referred to as “EDA” and students understanding of its role in problem\sphinxhyphen{}solving is one of the greatest failures in data science education.

\sphinxAtStartPar
The problem with the term EDA is that, if you asked most data scientists what it means, they probably couldn’t actually give you a straight answer. If you pressed them further, they would probably say something like “exploring your data before you start fitting your models.”

\sphinxAtStartPar
While the idea that data scientists should “get to know their data” before fitting a model is well\sphinxhyphen{}meaning (you \sphinxstyleemphasis{absolutely} should!), the ubiquitous but uncritical use of the term has given young data scientists the sense that the undirected poking at data is worthy of a capitalized three world title, complete with a universally recognized acronym.

\sphinxAtStartPar
This is problematic because \sphinxstyleemphasis{any} activity that involves data but lacks a clear motivation is doomed to be unending and unproductive. Data science has emerged precisely because our datasets are far too complex for us to understand directly; indeed, I would argue that the job of a data scientist can be summed up, in part, as a person who identifies \sphinxstylestrong{meaningful} patterns in our data and makes them comprehensible.

\sphinxAtStartPar
But therein lies the problem — without a clear motivation for \sphinxstyleemphasis{why} the data scientist is poking at their data, what makes a pattern meaningful is undefined. And without a clear purpose from which a concept of meaningfulness can be derived, there is no end to the ways one can slice and dice the data with no way of knowing when to stop or what is useful.

\sphinxAtStartPar
I would argue that what most people call Exploratory Data Analysis (EDA) can actually be decomposed into three activities.

\sphinxAtStartPar
The first activity people call EDA is what I call “learning the structure of your \sphinxstyleemphasis{dataset}” (emphasis on learning about your \sphinxstyleemphasis{dataset}, not using your data to learn about the world). This consists of answering questions about your dataset like “what constitutes a single observation in this dataset?,” “what variables are included in this dataset?,” “how many observations are there?,” “how are variables coded?,” and “what population is represented in this data?” These are questions about \sphinxstyleemphasis{the specific dataset} you are working with, \sphinxstyleemphasis{not} the real world, and answers are likely to be found in the dataset documentation and through basic tools for data introspection.%
\begin{footnote}[1]\sphinxAtStartFootnote
In \sphinxcode{\sphinxupquote{pandas}}, this would be things like \sphinxcode{\sphinxupquote{df.columns}} to see what variables are in the data, \sphinxcode{\sphinxupquote{df.info()}} to get a sense of how data is being represented and the number of rows, and simple tools for tabulating unique values like \sphinxcode{\sphinxupquote{df{[}"first column"{]}.value\_counts()}}.
%
\end{footnote}

\sphinxAtStartPar
The second activity that often falls under the label EDA is what I call “\sphinxstyleemphasis{validating} your dataset.” It’s a poor data scientist who takes the validity of their data on blind faith, so when faced with a new dataset, one should begin with a few “sanity checks” just to make sure things look reasonable. Does the number of observations seem reasonable given what you know about how the data was collected and who is supposed to be represented in the data? If there are date variables in the data, does their range match what should be in this data? And given the specifics of the data, does the range of variables make sense? For example, if you have data on registered voters 18 and over, you should probably check that the age variable has a minimum value of 18 and a maximum value of something sensible (e.g., not 225).

\sphinxAtStartPar
The third and final activity people call EDA is… everything one does with the data before they fit a statistical or machine learning model. This is the second major reason that I feel the very concept of EDA has had a pernicious influence on data science — it implicitly devalues anything done with data that doesn’t entail a complicated model as “lesser” or “just a stop on the way towards the “real” analysis,” when nothing could be further from the truth.

\sphinxAtStartPar
This type of data analysis — looking at summary statistics, calculating distributions of variables, computing tabulations and cross\sphinxhyphen{}tabulations of different things to improve one’s understanding of the world — is categorically different from “learning the structure of your data,” because it is inquiry in the service of better understanding the world, not the structure of your dataset. But it is \sphinxstyleemphasis{not} categorically different analyzing data using statistical models, not just because in many cases generating cross\sphinxhyphen{}tabulations or calculating group averages are essentially equivalent to using a statistical method like linear regression, but also because they are both examples of the same enterprise: attempting to answer questions about the world using data in the service of solving problems.

\sphinxAtStartPar
And just as one cannot properly fit or tune a model without a clear sense of the question one is seeking to answer and how that answer is meant to be used, nor can one know what cross\sphinxhyphen{}tabulations to compute without having a sense of purpose to make clear what constitutes “meaningfulness.”

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
“But I do EDA all the time without a clear question!” I hear you cry. “Sometimes I just want to see what patterns there are in the data.”

\sphinxAtStartPar
To you I say: you may not have realized you had questions in mind, but most of your data explorations have been \sphinxstyleemphasis{implicitly} motivated by a sense of questions you thought might relate to your stakeholder’s problem.

\sphinxAtStartPar
Perhaps you were looking at a store’s retail sales data and decided to see how sales volumes varied by customer age or gender. That may not seem obviously question\sphinxhyphen{}motivated, but I put it to you that you had in mind that those are customer demographics to which the store could target advertising or product stocking decisions. And had someone suggested “why don’t you look at how sales volumes vary by customer birth month or whether their name starts with a letter in the first half of the alphabet,” you would have looked at them funny and said “why on Earth would I do that?”

\sphinxAtStartPar
But the problem with approaching your data with \sphinxstyleemphasis{implicit} motivations is that (a) it’s hard to reflect on them or evaluate whether they rest on solid assumptions about the stakeholder problem, and (b) without an explicit goal, there’s no way to know when you’ve reached your destination, making it \sphinxstyleemphasis{really}  easy to get lost in the data.
\end{sphinxadmonition}


\subsection{Am I Just Being a Curmudgeon?}
\label{\detokenize{30_questions/07_eda:am-i-just-being-a-curmudgeon}}
\sphinxAtStartPar
No — this is a view held by many people who work with data. Not only is it a common (informal) complaint among colleagues, but others have also written on the topic in different forms. In a \sphinxhref{https://medium.com/@eytanadar/banning-exploration-in-my-infovis-class-9578676a4705}{wonderful Medium post}, for example, Eytan Adar discusses his decision to ban his students from using the term EDA entirely:
\begin{quote}

\sphinxAtStartPar
I got to have lunch with John Tukey many years ago. We talked about birding. I wish we talked about “Exploratory Data Analysis.” For all the clever names he created for things (software, bit, cepstrum, quefrency) what’s up with EDA? The name is fundamentally problematic because it’s ambiguous. “Explore” can be both transitive (to seek something) and intransitive (to wander, seeking nothing in particular). Tukey’s book seems to emphasize the former {[}…{]}. The problem is that students think he meant the latter.

\sphinxAtStartPar
Somehow that term has given students, and some professionals, the license to be totally imprecise about what they were building, and (more critically) how to evaluate whether it worked. If you’re not seeking anything in particular, any tool that lets you meander through data is perfectly reasonable. It makes the job of deriving insight completely the responsibility of the end\sphinxhyphen{}user. In that world, any decision is a reasonable one, evaluation is unnecessary, and there is no grade but an A. But that’s not the real world and so I’ve banned “explore.”

\sphinxAtStartPar
Exploration is too unbounded in the context of building a tool. We need to be able to decide when exploration terminates. Forcing students to tell me what they want the end\sphinxhyphen{}user to find and/or what decisions they want to enable has led to better projects.
\end{quote}

\sphinxAtStartPar
Obviously in this book I’ve endorsed the term “Exploratory” as an adjective, rather than a verb — in my few, once you’ve articulated a concrete Exploratory Question, I think it \sphinxstyleemphasis{does} help motivate subsequent

\sphinxAtStartPar
\sphinxhref{https://flowingdata.com/2017/04/28/data-exploration-banned/}{Nathan Yau} makes a similar point about EDA data visualizations:
\begin{quote}

\sphinxAtStartPar
Data exploration with visualization is good, but when someone describes their project as an exploration tool, it often means it lacks focus or direction. Instead it looks like generic graphs that don’t answer anything particular and leave all interpretation to the reader.
\end{quote}


\subsection{Recap}
\label{\detokenize{30_questions/07_eda:recap}}
\sphinxAtStartPar
Despite its ubiquity, few data scientists could actually tell you what constitutes Exploratory Data Analysis (EDA). Moreover, some of what people might call EDA in practice — answering questions about the world without complex modeling — should not be called EDA, but rather… well, that’s just data science.

\sphinxAtStartPar
So in this book, we will acknowledge the important (but distinct!) goal of two purposeful activities often called EDA:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Learning the structure of your dataset (what constitutes a unit of observation, what variables are in the dataset),

\item {} 
\sphinxAtStartPar
Validating your dataset (does the data pass the sniff test? Does it exhibit the basic properties you would expect given what it claims to be?)

\end{itemize}

\sphinxAtStartPar
But I will \sphinxstyleemphasis{not} use the term EDA itself, and when I differentiate between data science enterprises, I will do so by emphasizing differences in the \sphinxstyleemphasis{end goals} of those activities (answering Exploratory Questions, Passive\sphinxhyphen{}Prediction Questions, or Causal Questions), not the methods used to achieve those ends.


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{Answering Exploratory Questions}
\label{\detokenize{30_questions/15_answering_exploratory_questions:answering-exploratory-questions}}\label{\detokenize{30_questions/15_answering_exploratory_questions::doc}}
\sphinxAtStartPar
In our previous readings, we’ve had a chance to engage with two of the three big ideas of this book: data science is about solving problems, and the way data scientists solve problems is by answering questions about the world. Now it is time to engage with the third big idea in earnest: what differentiates good and great data scientists is their ability to be thoughtful about the ways in which they are likely to be wrong.

\sphinxAtStartPar
Anyone can fit a model that scores well on a target evaluation metric. \sphinxhref{https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml?view=azureml-api-2}{Microsoft}, \sphinxhref{https://cloud.google.com/automl}{Google}, and \sphinxhref{https://docs.aws.amazon.com/sagemaker/latest/dg/use-auto-ml.html}{Amazon} long ago rolled out tools that could take input data, a target feature, and an evaluation metric and run through a huge number of different types of models to find one that would perform best on that metric. What makes a data scientist valuable is the ability to figure out what they need to model to take in the first place to solve a real world problem, and to be thoughtful about the strengths \sphinxstyleemphasis{and limitations} of different modeling choices \sphinxstyleemphasis{with respect to the real world problem one is trying to solve}.

\sphinxAtStartPar
Throughout Part 3 of this book, we will explore the types of issues that are likely to arise when answering each type of question, starting with discussion of the issues likely to arise when answering Exploratory Questions in this chapter. Elements of these discussions will be similar across question types — such as the idea of \sphinxstyleemphasis{internal} and \sphinxstyleemphasis{external validity} we are about to introduce — but answers to different types of questions tend to be susceptible to different types of problems, so much will also be question specific.


\section{Internal Validity and External Validity}
\label{\detokenize{30_questions/15_answering_exploratory_questions:internal-validity-and-external-validity}}
\sphinxAtStartPar
An organizing principle for all discussion of model limitations is the distinction between internal and external validity.

\sphinxAtStartPar
\sphinxstylestrong{Internal validity} is about how well you have answered your question \sphinxstyleemphasis{in the context of the data and population being studied.} Internal validity should be very familiar to anyone who has taken a statistics of machine learning course — is your regression properly specified? Did you calculate your standard errors correctly? Are the assumptions underlying your model met by the data?

\sphinxAtStartPar
\sphinxstylestrong{External validity}, by contrast, is about how well the conclusions and parameter estimates from your analysis will generalize to a different context. This drug worked well in a study of young adults in California; will it also work for elderly patients in Nepal? Our stores in Argentina added this new dish to their menu and sales skyrocketed; would the same thing happen if we added it to menus in Brazil? In current data, users were more satisfied with search results to websites that had more text; if we start up\sphinxhyphen{}ranking websites with more text, will that make customers happier (or will Search Engine Optimization (SEO) operations just start filling spam sites with LLM\sphinxhyphen{}generated slop)?


\section{Why External Validity Often Gets Short Shrift}
\label{\detokenize{30_questions/15_answering_exploratory_questions:why-external-validity-often-gets-short-shrift}}
\sphinxAtStartPar
Understanding both internal and external validity is critically important to being a successful data scientist. In general, however, my experience has been that data science students tend to be better trained to evaluate the internal validity of an analysis than its external validity.

\sphinxAtStartPar
One reason for this may be that it is harder to evaluate external validity using statistical methods, causing some instructors to think of it as “outside the scope” of a statistics or machine learning course. But the second reason I suspect this occurs is more subtle: while a given analysis may be said to have good or bad \sphinxstyleemphasis{internal} validity, it cannot be said to have good or bad \sphinxstyleemphasis{external} validity. Rather, the external validity of a study can only be evaluated \sphinxstyleemphasis{with respect to the specific context to which one wishes to generalize the findings.}

\begin{sphinxShadowBox}

\sphinxAtStartPar
The external validity of a study can only be evaluated with respect to the specific context to which one wishes to generalize the findings.
\end{sphinxShadowBox}

\sphinxAtStartPar
This means that external validity is different from internal validity in an important way: when faced with the same facts about a study, everyone should \sphinxstyleemphasis{generally} agree on the internal validity of a study, but the external validity of a study really depends on how you want to use the results. A study of medical care provided to Duke undergraduates may have very \sphinxstyleemphasis{good} external validity with respect to undergraduate students at Emory, UNC, Vanderbilt, and other elite universities with associated research hospitals (that is, the conclusion of the study of Duke students is likely to also be valid for those other students). But that same study may have \sphinxstyleemphasis{poor} external validity with respect to lower income students attending community colleges that have less comprehensive student health insurance and no top\sphinxhyphen{}tier associated hospital system. And it would certainly have terrible external validity with respect to all Americans, never mind people living in other countries.


\section{Interval v. External Validity: An Example}
\label{\detokenize{30_questions/15_answering_exploratory_questions:interval-v-external-validity-an-example}}
\sphinxAtStartPar
To illustrate what is meant by these terms, suppose you’ve been hired by a car manufacturer. Through extensive market research, they have determined that if they can improve the safety of their cars, they could dramatically improve the image of their brand. They have turned to you, their in\sphinxhyphen{}house data scientist, to help them determine what safety enhancements they should focus on developing.

\sphinxAtStartPar
To help them prioritize their efforts, you decide it would be useful to begin by better understanding the predominant causes of major accidents. After all, if 95\% of accidents were caused by mechanical failures, bad weather, and drunk drivers, then even a perfect system for preventing drowsy driving accidents could only reduce accidents by 5\% at best!%
\begin{footnote}[1]\sphinxAtStartFootnote
It is worth noting that if the goal of the company is to improve brand image and sales — rather than improve safety as much as possible — then what matters is not what \sphinxstyleemphasis{actually} causes the most accidents, but rather what customer \sphinxstyleemphasis{perceive} causes the most accidents. But let us assume — for the sake of this exercise — that your employer’s interest in reducing accidents is sincere. If there is a divergence between customer perceptions and the reality of accident causes, that could be addressed with additional information embedded in any advertisements of the feature being developed.
%
\end{footnote}

\sphinxAtStartPar
The best source of accident investigations of which you are aware is the \sphinxhref{https://www.nhtsa.gov/data}{US National Highway Traffic Safety Administration’s} National Center for Statistics and Analysis (NCSA). Using their \sphinxstyleemphasis{Crash Reporting Sampling System}, the NCSA collects and publishes data from “nationally representative sample of police\sphinxhyphen{}reported traffic crashes, which estimates the number of police\sphinxhyphen{}reported injury crashes and property\sphinxhyphen{}damage\sphinxhyphen{}only crashes in the United States,” as well as data on all fatal accidents collected through their \sphinxstyleemphasis{Fatality Analysis Reporting System}.

\sphinxAtStartPar
The NCSA provides you with data on police\sphinxhyphen{}reported accidents from the Crash Reporting Sampling System and all fatal accidents from the Fatality Analysis Reporting System from the past 5 years (2018\sphinxhyphen{}2023). As you turn to using this data to analyze the causes of severe accidents for your stakeholder, what internal and external validity concerns might you have in the back of your mind?


\subsection{Internal Validity Concerns}
\label{\detokenize{30_questions/15_answering_exploratory_questions:internal-validity-concerns}}
\sphinxAtStartPar
Internal validity concerns, in this context, relate to your ability to properly characterize the causes of severe accidents in this data. The first of these concerns are those discussed in our previous two readings — namely, whether the summarizations of accident cause you generate from this data are meaningful and faithful representations of the patterns in the data.

\sphinxAtStartPar
But internal validity concerns also extend to things like concerns over the accuracy with which things are measured. For example, it’s reasonable to ask “How well do police accident reports capture the true causes of an accident?” Some factors — like whether a driver was intoxicated — are easy to verify after the fact in major accidents. Other factors, however, may have contributed to accidents — and be easier to address with driver assistance systems — but may have been too hard to verify for the police to put in their reports. For example, maybe weak headlights prevented the driver from seeing a change in the speed limit (causing the documented cause of the accident: speeding). Or perhaps one driver was unable to see an approaching vehicle on a cross street due to width of the A\sphinxhyphen{}piller (that piece of metal running vertically between the front windshield and front side window). Or “weather” may be invoked as a cause in an accident when what was really at play was that the driver had the wipers on too low of a setting (and automatic wipers would have adjusted more quickly, preventing the accident).

\sphinxAtStartPar
All of these are question about whether our summarization of the \sphinxstylestrong{data} provides a proper characterization of \sphinxstylestrong{the world} for the period and group we think are covered.


\subsection{External Validity Concerns}
\label{\detokenize{30_questions/15_answering_exploratory_questions:external-validity-concerns}}
\sphinxAtStartPar
But worrying about whether we really understand the world covered by our data is only half the battle. After all, our stakeholders aren’t planning to go back in time 5 years and add a new driver safety system to their cars starting in 2018; they’re thinking of developing this safety system to use \sphinxstyleemphasis{in the future}. So what do we think is the \sphinxstyleemphasis{external validity} of findings from 2018\sphinxhyphen{}2023 to the next ten years?

\sphinxAtStartPar
To answer this question, we need to consider how the next ten years might differ from the last five \sphinxstyleemphasis{in ways that are relevant to the phenomenon we care about} (here, the causes of car accidents).

\sphinxAtStartPar
First, since time will have passed between when the crash data was collected and when the new feature rolls out, one might reasonably assume that trends in accident causes during the 2018\sphinxhyphen{}2023 period are likely to continue. In light of that, a thoughtful data scientist may wish to pay extra attention to whether there are clear \sphinxstyleemphasis{trends} in accident causes, and in what direction they are trending.

\sphinxAtStartPar
But one might also pause to ask whether there was anything exceptional about the “data generating process” during the 2018\sphinxhyphen{}2023 period that would not obtain in later years. Something like… a pandemic?

\sphinxAtStartPar
And indeed, one would be right to worry. The figure below shows Vehicle Miles Travelled (VMT) and Fatalities in 2019 (pre\sphinxhyphen{}pandemic) and 2020 (the pandemic began in force in March, if you’ve forgotten). As the figure shows, driving plummeted during this period, and so too did Fatalities.

\sphinxAtStartPar
\sphinxincludegraphics{{2020_vmt_fatalities}.png}

\sphinxAtStartPar
Source: Taken from the US Department of Transportation NHTSA’s \sphinxhref{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813266}{\sphinxstyleemphasis{Overview of Motor Vehicle Crashes in 2020}}

\sphinxAtStartPar
Given that, a thoughtful data scientist may wish to be sure that any patterns identified in this five\sphinxhyphen{}year period are robust to exclusion of 2020\sphinxhyphen{}2022 before making any predictions about the likelihood these patterns would persist in later years.


\subsubsection{Other Contexts}
\label{\detokenize{30_questions/15_answering_exploratory_questions:other-contexts}}
\sphinxAtStartPar
The preceding discussion assumed an interest in US accidents, but of course most car companies that sell cars in the US also sell cars in Asia and Europe. The issues raised above are examples of what I would contend are relatively \sphinxstyleemphasis{small} threats to the external validity of findings based on data from 2018\sphinxhyphen{}2023 with respect to near\sphinxhyphen{}future US accidents (at least provided the patterns aren’t driven by the pandemic period).

\sphinxAtStartPar
But external validity to Asian or European markets would be a much bigger concern. Because traffic laws, speed limits, alcohol laws, and how roads are constructed and laid out are all very different in different regions of the world, it seems quite unlikely that patterns identified in US accident data would have much external validity to Asian or European auto markets.


\section{Conclusion}
\label{\detokenize{30_questions/15_answering_exploratory_questions:conclusion}}
\sphinxAtStartPar
Internal and External Validity are both key concepts for being an effective data scientist, and they are concepts to which we will return regularly in this book. Moreover, they are goals that are sometimes in tension — the more control one has over a study context, the more likely one is to have good Internal Validity; but that control can often create an artificiality to limits External Validity. Thus Internal and External Validity should not necessarily be thought of as things to try and simultaneously maximize at all costs; rather, they are best thought of as distinct features of any analysis that should always be considered. I would also add that, of the two, I would argue that External Validity — while not more important in and of itself — is the more often overlooked.


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{Answering Exploratory Questions: Internal Validity}
\label{\detokenize{30_questions/16_exploratory_internal_challenges:answering-exploratory-questions-internal-validity}}\label{\detokenize{30_questions/16_exploratory_internal_challenges::doc}}
\sphinxAtStartPar
While some aspects of internal validity are things you have almost certainly encountered in statistics and machine learning course — is the model overfit to the data? are your standard errors correctly calculated? are you omitting important variables from your regression? — most statistics courses fail to discuss internal validity in more holistic terms. In this section, I wish to take a step back to discuss what we are really trying to accomplish when we answer Exploratory Questions, and how that impacts how we evaluate “internal validity.”

\sphinxAtStartPar
Whether one uses simple summary statistics (means and medians), plots, or more sophisticated algorithms from the domains of statistical inference and unsupervised machine learning, answering Exploratory Questions always boils down to the same challenge:

\sphinxAtStartPar
\sphinxstylestrong{Generating answers that are (1) understandable, (2) faithfully represent patterns in the data, and (3) are relevant given the problem one is seeking to solve.}

\sphinxAtStartPar
What is meant by these three components exactly? Let’s take each in turn.















\sphinxstepscope


\section{(1) Understandable}
\label{\detokenize{30_questions/17_exploratory_internal_understandable:understandable}}\label{\detokenize{30_questions/17_exploratory_internal_understandable::doc}}\begin{quote}

\sphinxAtStartPar
Generating answers that are \sphinxstylestrong{(1) understandable,} (2) faithfully represent patterns in the data, and (3) are relevant given the problem one is seeking to solve.
\end{quote}

\sphinxAtStartPar
Answering Exploratory Questions effectively is all about making the patterns in our incomprehensively complicated world legible to people. To do so, we take large datasets that are too complicated to understand in their raw form, identify salient structure and patterns in this data, and summarize them in a way that allows us to communicate those patterns to other people. The \sphinxstyleemphasis{method} by which we make structures in the data understandable will vary across applications — summary statistics, regression coefficients, scatter plots, or other visualizations — but the goal of identifying and communicating information about salient patterns is always the same.

\sphinxAtStartPar
Professionals from different disciplines often use different terminology to describe this process of summarization. Some like to refer to it as “separating the signal (the thing that’s important) from the noise (all the other variation that doesn’t matter),” others talk about “dimensionality reduction” (basically linear algebra speak for summarization), while still others may talk about “modeling the underlying data generating process that gave rise to the observed data.” Regardless of the terminology one uses, however, these all boil down to the same thing: filtering and discarding the variation the data scientist deems to be irrelevant to make it easier to see and understand the variation deemed important.

\sphinxAtStartPar
The importance of researcher discretion in deciding what variation to discard as noise and what variation to foreground as “important” is one of the defining challenges of answering Exploratory Questions. Other types of questions — like Passive Prediction Questions — often involve using more mathematically sophisticated modeling tools, and consequently are viewed as more challenging. In my experience, however, learning to understand the stakeholder’s problem context \sphinxstyleemphasis{and} the variation in a data set well enough to exercise this discretion effectively is actually one of the things young data scientists struggle with most. It requires both good domain knowledge to understand what is \sphinxstyleemphasis{meaningful} (as we will discuss below), and also for the data scientist to spend a lot of time exploring the data thoughtfully and from different perspectives. This is a hard skill to learn,%
\begin{footnote}[1]\sphinxAtStartFootnote
Although I am far from convinced that the discipline has tried particularly hard to teach it ({\hyperref[\detokenize{30_questions/17_exploratory_internal_understandable:07_eda.ipynb}]{\sphinxcrossref{\DUrole{xref,myst}{see my screed against “EDA”}}}}).
%
\end{footnote} but with intentionality, patience, and practice, it is a talent that once learned will helps set you apart from the average pytorch\sphinxhyphen{}jockey.

\sphinxAtStartPar
Summarizations created to answer Exploratory Questions can differ radically in their ambition. At one end of the spectrum are simple summary statistics, like means, median, and standard deviations. These seek to provide a simple characterization of a single feature of a single variable. Slightly more ambitious are various basic data visualizations — like histograms (which are substantially richer than the aforementioned summary statistics) or scatter plots and heatmaps (which provide substantial granularity and communicate information about the relationship between different variables). And the most ambitious efforts make use of multivariate regressions and unsupervised machine learning algorithms to make inferences about the \sphinxstyleemphasis{Data Generating Process} (DGP) — the actual physical or social processes that gave rise to the data you observe, and which (hopefully) can be represented in a relatively parsimonious manner, much as the relatively simple laws of physics give rise to the orbits of the planets and the complexity of life.

\sphinxAtStartPar
To illustrate what I mean by trying to deduce something about the data\sphinxhyphen{}generating process, suppose you are a medical researcher interested in a poorly understood disease like Chronic Fatigue Syndrome (CFS). It is generally agreed that CFS is more of a label for a constellation of symptoms than an understood physical ailment, and you have a hypothesis that the symptoms of CFS aren’t actually caused by a single biological dysfunction, but rather that multiple distinct biological dysfunctions give rise to similar symptoms that we have mistakenly grouped under this same umbrella term. In other words, you think that the data\sphinxhyphen{}generating process that gives rise to patients diagnosed with Chronic Fatigue Syndrome consists of two distinct diseases.

\sphinxAtStartPar
You’re fortunate enough to have detailed patient data on people diagnosed with the condition, but it’s impossible for you to just look at these gigabytes of thousands of patient records and “see” any meaningful patterns. You need a way to filter out irrelevant data to identify the “signal” of these two conditions. To aid you in this question, you decide to ask “If you were to group patients into two groups so that the patients in each cluster looked as similar as possible, but patients in different clusters looked as \sphinxstyleemphasis{dissimilar} as possible, how would you group these patients?”

\sphinxAtStartPar
This, you may recognize, is precisely the question clustering algorithms (a kind of unsupervised machine learning algorithm) are designed to answer! So you apply your clustering algorithm to the patient data and get back a partition of the patients into two distinct groups. This, in and of itself, doesn’t constitute a particularly \sphinxstyleemphasis{understandable} summarization of your data, but it provides a starting point for trying to investigate \sphinxstyleemphasis{diagnostically and biologically relevant} differences that exist between these populations. If one cluster included more patients reporting fatigue when doing any exercise, while another cluster reported they felt better when they exercised, but felt a high level of baseline fatigue that didn’t respond to sleep, that might suggest that the \sphinxstyleemphasis{data\sphinxhyphen{}generating process} for these patients was actually driven by two different biological processes. \sphinxstyleemphasis{And} it gives you a great starting point to prioritize your subsequent investigations into what might explain these differences!


\bigskip\hrule\bigskip


\sphinxstepscope


\section{(2) Faithful to the Data}
\label{\detokenize{30_questions/18_exploratory_internal_faithful:faithful-to-the-data}}\label{\detokenize{30_questions/18_exploratory_internal_faithful::doc}}\begin{quote}

\sphinxAtStartPar
Generating answers that are (1) understandable, \sphinxstylestrong{(2) faithfully represent patterns in the data,} and (3) are relevant given the problem one is seeking to solve.
\end{quote}

\sphinxAtStartPar
What do means, medians, standard deviations, linear regressions, logistic regressions, generalized additive models (GAMs), singular value decomposition (SVD), principal component analyses (PCAs), clustering algorithms, and anomaly detection algorithms all have in common?

\sphinxAtStartPar
Answer: unless your dataset is extremely degenerate, you can point \sphinxstyleemphasis{any} of these tools at your data and they will return a relatively easy\sphinxhyphen{}to\sphinxhyphen{}understand characterization of the structure of your data.

\sphinxAtStartPar
At first, that may seem extremely exciting. But if you think about it a little longer you will realize the problem: all of these are designed to give you a relatively understandable summary of radically different properties of your data, and even though they will all provide you with a result, these results can’t all possibly be faithful representations of the dominant patterns in your data.

\sphinxAtStartPar
To illustrate the point, suppose I told you that in one university math course, the average grade was a B\sphinxhyphen{}. You might infer that students were doing pretty well! But now suppose I told you that in a different university math course, 20\% of the students had gotten a 0 on the midterm and on the final—you would probably infer something was going seriously wrong in that class. And yet those two statistics could both be true of the same class—the only difference is what patterns in the data \sphinxstyleemphasis{I}, the data scientist, have decided are meaningful to communicate to you, the reader.

\sphinxAtStartPar
The example of the math class in which the average grade was a B\sphinxhyphen{} and 20\% of the students were failing also illustrates one of the great dangers of tools for data summarization: they are so eager to please, they will \sphinxstyleemphasis{always} provide you with an answer, whether that answer is meaningful or not. I think most readers would agree that learning that the average grade in the class was a B\sphinxhyphen{} actually misleads more than it informs (since for the class to have an average grade of 80\% and a 20\% fail rate, the grade distribution would need to be something like 20\% 0’s and 80\% 100’s). Indeed, it’s worth emphasizing that while hearing “the average grade is a B\sphinxhyphen{}” makes the reader think that most kids are doing ok\sphinxhyphen{}ish, the reality is that \sphinxstyleemphasis{no one} in the class is doing ok\sphinxhyphen{}ish! They’re either doing horribly or terrifically!


\subsection{The Case of \sphinxstyleemphasis{Aimovig}}
\label{\detokenize{30_questions/18_exploratory_internal_faithful:the-case-of-aimovig}}
\sphinxAtStartPar
Less that feel like a contrived example, consider the case of Aimovig, a drug authorized by the FDA in 2018 for treating chronic migraines that was heralded as a “game changer.”

\sphinxAtStartPar
To get Aimovig authorized, the pharmaceutical companies developing (Amgen and Novartis) had to run a clinical trial in which a random sample of people with chronic migraines was given Aimovig (the treatment group) and a random sample was a placebo (the control group). Patients in the clinical trial self\sphinxhyphen{}reported how their migraine frequency changed when in the trial, and the effectiveness of Aimovig was then evaluated by comparing the decrease in self\sphinxhyphen{}reported migraines for those taking Aimovig (on average, a decrease of 6\sphinxhyphen{}7 migraines a month) to the decrease in self\sphinxhyphen{}reported migraines for those taking a placebo (on average, a decrease of 4 migraines a month).%
\begin{footnote}[1]\sphinxAtStartFootnote
A placebo is a “fake” treatment given to patients in clinical trials. Despite not being biologically active — placebos are often simple saline or sugar pills — most patients on placebos see their condition improve when dealing with subjective conditions, like pain.
%
\end{footnote} This difference of 2\sphinxhyphen{}3 migraines a month — called the “Average Treatment Effect” of the trial — was found to be positive and statistically significant, and so the drug was authorized. Indeed, if you see an ad for Aimovig, you’ll probably see the average effect of the drug reported in the same way:

\sphinxAtStartPar
\sphinxincludegraphics{{migraine_average_effect}.png}

\sphinxAtStartPar
That’s great! Chronic migraines can be a crippling disability, so any improvement in treatment is exciting. But you would be excused for asking why people were getting \sphinxstyleemphasis{so} excited about what seems like a relatively small reduction in migraines.

\sphinxAtStartPar
The answer, as it turns out, is that almost nobody experiences this “average effect.” Instead, \sphinxstyleemphasis{most} people who take Aimovig see little to no benefit, but \sphinxstyleemphasis{some} (depending on your criteria, something like 40\%) see their migraine frequency fall by 50\% or more. Amgen and Novartis don’t yet know how to identify who will benefit and who will not before they try the drug, and we don’t allow drug companies to “move the goalposts” after a clinical trial has already started by changing the way they plan to measure the effectiveness of a drug (for fear they will hunt through the data till they find a spurious correlation that makes it look like the drug works when it really doesn’t), so this average effect remains the only statistic that Amgen and Novartis are allowed to report in their advertising.

\sphinxAtStartPar
But if you’re a \sphinxstyleemphasis{doctor} or a \sphinxstyleemphasis{patient}, it seems clear that this simple average effect — a reduction of 2\sphinxhyphen{}3 migraines a month — really does not provide a \sphinxstyleemphasis{faithful} summary of the underlying variation.


\subsubsection{But… I Thought Unsupervised Machine Learning Always Found The “Best”}
\label{\detokenize{30_questions/18_exploratory_internal_faithful:but-i-thought-unsupervised-machine-learning-always-found-the-best}}
\sphinxAtStartPar
“Fine,” I hear you say, “that makes sense for simple summary statistics. Those are computed by simple formulas. But what about unsupervised machine learning algorithms or generalized additive models? Those use numerical optimization to find the \sphinxstyleemphasis{best} answer!”

\sphinxAtStartPar
Well… yes and no. As you may recall, in the first chapter of the book I posited that all data science algorithms are just fancy tools for answering questions, and even the most sophisticated unsupervised machine learning algorithms are no exception. While it is true that the machinery that underlies these algorithms is much more sophisticated than the formula we use for calculating a variable’s average, it is important to not attribute too much intelligence to these tools.

\sphinxAtStartPar
Underlying any unsupervised machine learning algorithm is a simple formula that takes as input whatever parameters the algorithm gets to choose (be those factor loads in a PCA model, or the assignment of observations to clusters in a clustering algorithm) and returns as output a single number. Often this number is called “loss,” and the function is called a “loss function,” but occasionally different terminology will be used.

\sphinxAtStartPar
One way to think of the job of an unsupervised machine learning algorithm is to pick the parameter values that minimize this loss function. A clustering algorithm for example, may try and assign observations to clusters to maximize the similarity of observations within a cluster (say, by minimizing the sum of squared differences between the values of certain variables for all observations within a cluster) while also maximizing the differences between observations in different clusters (say, by maximizing the sum of squared differences between the values of certain variables for all observations \sphinxstyleemphasis{not} in the same cluster).

\sphinxAtStartPar
But another way to say that is that the job of an unsupervised machine learning algorithm (or any algorithm, really) is to find the parameter values (coefficients in a regression, observation assignments for a clustering algorithm) that answer the question “If my goal is to minimize {[}whatever the loss function your specific algorithm seeks to minimize{]}, how should I do it?” But while they are likely to find the best way to accomplish that goal given the parameters they control, they will do so \sphinxstyleemphasis{regardless of whether the “best” solution is actually a “good” solution!} Point a clustering algorithm at any data and ask it to split the data into 3 clusters, and it will pick the best way to split the data into three clusters, even if the three clusters are \sphinxstyleemphasis{almost} indistinguishable. In other words, clustering algorithms assign observations to clusters… even when there’s no real clustering of the data! Dimensionality reduction algorithms will always tell you a way to drop dimensions, and anomaly detection algorithms will always find (relative) outliers.

\sphinxAtStartPar
Moreover, just because your clustering algorithm finds what it thinks is the best solution doesn’t mean there isn’t a \sphinxstyleemphasis{substantively} very different solution that was \sphinxstyleemphasis{just} a little less good it hasn’t told you about.

\sphinxAtStartPar
It’s up to you, the data scientist, to evaluate whether the answers these algorithms provide to relatively myopic questions give a meaningful picture of the data.


\bigskip\hrule\bigskip


\sphinxstepscope


\section{(3) Relevant}
\label{\detokenize{30_questions/19_exploratory_internal_meaningful:relevant}}\label{\detokenize{30_questions/19_exploratory_internal_meaningful::doc}}\begin{quote}

\sphinxAtStartPar
Generating answers that are (1) understandable, (2) faithfully represent patterns in the data, and \sphinxstylestrong{(3) are relevant given the problem one is seeking to solve.}
\end{quote}

\sphinxAtStartPar
Inherent in creating any summarization is exercising discretion over what variation is relevant (signal) and what variation is not (noise). But just as one person’s trash may be another person’s treasure, so too may one person’s signal be another person’s noise, depending on their goals! Crucially, then, the data scientists’ guiding star when deciding what is important is whether certain variation in the data is \sphinxstyleemphasis{relevant to the stakeholder’s problem}.

\sphinxAtStartPar
As data scientists, we are blessed with an abundance of tools for characterizing different facets of our data. These range from the simple — means, standard deviations, and scatter plots — to the profoundly sophisticated, like clustering algorithms, principal component analyses, and semi\sphinxhyphen{}parametric generalized additive models.

\sphinxAtStartPar
Regardless of the specific methods being employed, however, none of these tools can really tell us whether the patterns they identify are meaningful or relevant, and that’s because what constitutes a meaningful pattern depends on the problem the stakeholder is seeking to address and the context in which they’re operating.

\sphinxAtStartPar
To illustrate the importance of context, suppose you are hired by a hospital to learn what can be done to reduce antibiotic\sphinxhyphen{}resistant infections. So you grab data on the various bacteria that had been infecting patients and write a web scraper and Natural Language Processing pipeline to systematically summarize all available research on the cause of these antibiotic\sphinxhyphen{}resistant bacteria. Your work is \sphinxstyleemphasis{amazing}, seriously top of the line, and after two months you conclude that in most cases, the cause of antibiotic resistance in the bacteria infecting patients is… the use of antibiotics in livestock.

\sphinxAtStartPar
Now, that analysis may not be \sphinxstyleemphasis{wrong} — you have properly characterized a pattern in the data — but it isn’t a pattern that’s meaningful to your stakeholder, who has no ability to regulate the livestock industry. That pattern might be meaningful to someone else — like a government regulator — but in this context, with this stakeholder, it just isn’t helpful. The features of the data that are important, in other words, depend on what we may be able to do in response to what we learn. And there’s no summary statistic, information criterion, or divergence metric that can evaluate whether a pattern of this type is \sphinxstyleemphasis{relevant}.


\subsection{The Concept of “Best” and Myopic Tools}
\label{\detokenize{30_questions/19_exploratory_internal_meaningful:the-concept-of-best-and-myopic-tools}}
\sphinxAtStartPar
In our discussion of ensuring that the data summaries one gets from different statistical or unsupervised machine learning tools faithfully represent patterns in the data, I made the point that these tools will \sphinxstyleemphasis{always} provide an answer, regardless of whether it actually tells you much about the data. Tools for answering Exploratory Questions tend to either be simple definitional tools (means, medians, standard deviations) or operate by trying to minimize a loss function (unsupervised machine learning). In the case of definitional tools, the result is the result, even if that means providing a mean that sits neatly in the middle of a bimodal distribution. In the case of algorithms that minimize a loss function, the algorithm will provide the “best” answer (the answer that minimizes the loss function) regardless of whether it’s a \sphinxstyleemphasis{good} answer.

\sphinxAtStartPar
There is, however, a second layer to this problem of myopia. Data science tools are incredibly powerful at finding answers to questions of the form “If my goal is to minimize X, how should I do it?” type questions — answers you may have never figured out in millions of years! — but their power lies in figuring out the best way to accomplish an articulated goal, \sphinxstyleemphasis{not} in figuring out what goal to pursue.

\sphinxAtStartPar
This is true at both the macro level (doesn’t make sense to look for clusters in my data?) and also at the micro level (when assigning observations to clusters, how do I measure success?). Hidden inside nearly all algorithms you use are a handful of baked\sphinxhyphen{}in choices you may not even realize are being made for you. Take clustering, for example. In general, when clustering observations, one has two objectives: maximize the similarity of observations within each cluster and maximize the \sphinxstyleemphasis{dis}similarity of observations in different clusters. But what you might not have thought about very much is that there’s an inherent tension between these two objectives — after all, the best way to maximize the similarity of observations within each cluster is to only assign observations to the same cluster if they are identical (a choice that creates lots and lots of very small clusters). And the best way to maximize \sphinxstyleemphasis{dis}similarity between clusters is to only put \sphinxstyleemphasis{really really} different observations in different clusters (resulting in a few really big clusters). So how is your clustering algorithm balancing these two considerations? Is the algorithm’s choice of how to balance them in any way a reflection of the balance that makes the most sense in the context of your stakeholder’s problem? (I’ll give you a hint — the algorithm sure can’t answer that question, so you’d better be able to!)

\sphinxAtStartPar
Discretion: it’s everywhere, and you’re exercising it, whether you realize it or not.


\subsection{Recap}
\label{\detokenize{30_questions/19_exploratory_internal_meaningful:recap}}
\sphinxAtStartPar
The internal validity of an analysis is the degree to which we feel the analysis has properly characterized the underlying patterns and structure in the data being studied. One aspect of internal validity — the aspect most of us have been exposed to our introductory statistics or machine learning courses — is specific to the tool being used, and can often be evaluated by mathematical or statistical means. “Is your regression properly specified?” is a question that can be answered, in part, by looking at your residuals and other diagnostic statistics.

\sphinxAtStartPar
When it comes to answering Exploratory Questions, however, there is also an aspect of internal validity that cannot be evaluated statistically — namely, are the answers you generated understandable, are they meaningful given the problem you are trying to solve, and do they faithfully represent the underlying patterns in the data? In many ways, these are harder questions to answer, but they are every bit as important.

\sphinxstepscope


\chapter{Using Passive Prediction Questions}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:using-passive-prediction-questions}}\label{\detokenize{30_questions/20_using_passive_prediction_questions::doc}}
\sphinxAtStartPar
In the past few chapters, we explored the role of Exploratory Questions in helping data scientists better understand the problems they seek to solve and to prioritize subsequent efforts. While useful, however, answering Exploratory Questions alone is rarely sufficient to solve a stakeholder’s problem. To really solve problems, data scientists usually need to answer Passive Prediction Questions — the focus of this chapter — and/or Causal Questions (a topic we will return to in future chapters).

\sphinxAtStartPar
Passive Prediction Questions are questions about the future or otherwise unknown outcomes of \sphinxstyleemphasis{individual entities} (customers, patients, stores, etc.). “How likely is Patient X to experience a heart attack in the next two years?,” for example, or “How likely is it Mortgage Holder Y will fail to make their mortgage payment next month?”

\sphinxAtStartPar
Passive Prediction Questions are usually deployed for one of two business purposes:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
\sphinxAtStartPar
identifying individual entities of particular interest (high\sphinxhyphen{}risk patients, high\sphinxhyphen{}value clients, factory machinery in need of preventative maintenance, etc.), and

\item {} 
\sphinxAtStartPar
automating classification or labeling tasks currently performed by people (reading mammograms, reviewing job applicant resumes, identifying internet posts that violate terms of use).

\end{enumerate}

\sphinxAtStartPar
Unlike Exploratory Questions, data scientists don’t generally come up with “\sphinxstyleemphasis{an} answer” to Passive Prediction Questions; rather, data scientists answer Passive Prediction Questions by developing statistical models that take the attributes of an entity as inputs and spit out a unique answer \sphinxstyleemphasis{for each entity}. A company interested in spam detection, for example, might hire a data scientist to develop a model that takes the content of an email as input and, for every email a person receives, answers the question, “If the recipient of this email looked at it, would they consider it spam?” The exact statistical machinery one uses will vary across applications, but answering these questions is the realm where terminology like “supervised machine learning” or “statistical prediction” is most likely to be used.

\sphinxAtStartPar
Since Passive Prediction Questions don’t usually have \sphinxstyleemphasis{an} answer, a data scientist faced with a Passive Prediction challenge will often start by considering the \sphinxstyleemphasis{feasibility} of developing a model to give individual\sphinxhyphen{}level answers to a Passive Prediction Question. “Given data on new customer behavior on my website,” for example, “\sphinxstylestrong{can I} predict how much a customer is likely to spend over the next year?” Assuming feasibility, however, at the end of the day, what a stakeholder cares about is not whether one \sphinxstyleemphasis{can} predict future spending; they care about the actual predictions a data scientist can give for each entity — “Given customer 389237’s behavior on my website, are they likely to spend a lot over the next year?”


\section{Types of Passive Prediction Questions}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:types-of-passive-prediction-questions}}
\sphinxAtStartPar
Passive Prediction Questions come in two types, corresponding to the two primary business purposes detailed above.

\sphinxAtStartPar
The first type of Passive Prediction Question pertains to what will likely happen in the future for a specific individual. Answering these questions is useful for identifying individuals for additional care or attention. For example, a hospital might want to know, “How likely is Patient A to experience complications after surgery?” so they can decide whether the patient should receive extra nursing attention during recovery, or a factory owner might ask, “How likely is this machine to break down in the next month?” to help them determine when to take the machine offline for maintenance. This is the more intuitive kind of Passive Prediction Question, as it accords nicely with the colloquial meaning of the term “prediction.”

\sphinxAtStartPar
The second type of Passive Prediction Question pertains to what \sphinxstyleemphasis{would} happen in different circumstances. Answering this type of question is the key to automation, as the “different circumstance” in question is often one in which a job is being done by an actual person. For example, a statistical model that can answer the question “\sphinxstyleemphasis{if} a radiologist \sphinxstyleemphasis{were} to look at this mammogram, would they conclude it showed evidence of cancer?” is a model that automates the review of mammograms. A model that can answer the question “\sphinxstyleemphasis{if} the intended recipient of this email \sphinxstyleemphasis{were} to see it, would they say it is spam?” is an algorithm that automates spam detection.


\section{Differentiating Between Exploratory and Passive Prediction Questions}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:differentiating-between-exploratory-and-passive-prediction-questions}}
\sphinxAtStartPar
If you have not felt a little confused about the distinction between Exploratory and Passive Prediction Questions previously, there’s a good chance you find yourself struggling with that issue here, and for understandable reasons.

\sphinxAtStartPar
The first thing to emphasize is that the distinction between Exploratory and Passive Prediction Questions is a distinction in one’s \sphinxstyleemphasis{goal}, not the statistical machinery one might use to achieve that goal.

\sphinxAtStartPar
With Exploratory Questions, our interest is in improving our understanding of patterns in the world to help us understand our problem space, not in making precise predictions for each entity in our data. If we were to use a regression to answer an Exploratory Question, for example, the “answer” to our Exploratory Question would be found in the variables on the right\sphinxhyphen{}hand side of our regression, their coefficients, and their standard errors. That’s because those coefficients are what help us understand the factors that contribute to the outcomes we care about. A good model, in other words, doesn’t actually have to explain a large share of variation at the level of individual entities, but it does have to help us understand how different factors contribute to the outcome we wish to understand.

\sphinxAtStartPar
For example, suppose you’re interested in the impact of a college education on earnings. We might try to understand the role of a college education using a regression model that regresses individuals’ salaries on age, education, and where individuals live. If we saw that the coefficient on having a college degree had a large and statistically significant coefficient, that would tell us a lot of important information about overall importance of a college degree on earnings. And this would be true even if the model only explained a small amount of overall variation in salaries (e.g., the R\sphinxhyphen{}Squared might only be 0.2). The model, in other words, is able to tell us a lot about average differences in earnings for college graduates and non\sphinxhyphen{}college graduates, even though it is not particularly good at tell you the likely salary of any \sphinxstyleemphasis{individual} person in the data.

\sphinxAtStartPar
With Passive Prediction Questions, this logic is reversed. With Passive Prediction Questions, we \sphinxstyleemphasis{don’t} care about how well the model helps us understand patterns in the world; we only care about whether it can make good predictions of some outcome we care about \sphinxstyleemphasis{for individual entities in the data}. That’s why we care about metrics like AIC, AUC, R\sphinxhyphen{}Squared, Accuracy, Precision, Recall, etc. when deciding whether a model does a good job answering a Passive Prediction Question, not the size of the standard errors on the coefficients.

\sphinxAtStartPar
This is also the reason that data scientists are often comfortable using “black box” models when answering Passive Prediction Questions. Black box models are statistical models — like neural networks or random forests — where the way variation in explanatory variables contribute to predictions is opaque to the user. That often doesn’t matter when answering Passive Prediction Questions — since all we care about are the predicted values the model generates — but precisely because the patterns these models rely on to make their predictions can’t be easily understood, they are of little value for answering Exploratory Questions.%
\begin{footnote}[1]\sphinxAtStartFootnote
Well… that’s why \sphinxstyleemphasis{many} data scientists are comfortable using Black Boxes, although as we will discuss in an upcoming reading, black box models have serious limitations, even when answering Passive Prediction Questions.
%
\end{footnote}


\section{The “Passive” in Passive Prediction}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:the-passive-in-passive-prediction}}
\sphinxAtStartPar
The term “passive” in “Passive Prediction Questions” is meant to emphasize the distinction between Passive Prediction Questions and Causal Questions. Both Passive Prediction Questions and Causal Questions can be thought of as trying to “predict” some future outcome, but they differ in the contexts in which their predictions are valid. A full accounting of the distinction between Passive Prediction Questions and Causal Questions will have to wait until we cover Causal Questions in detail, for the moment, we can get a sense of things by introducing a very casual definition of what it means for some cause X to effect some outcome Y.

\sphinxAtStartPar
In casual parlance, when we say that some factor X causes outcome Y (and that X is not merely correlated with Y), what we usually mean is that if we were to go out and actively change X, Y would change as a result. This isn’t a fully rigorous definition, but it drives home that causation is about what happens when we \sphinxstyleemphasis{actively manipulate} X.

\sphinxAtStartPar
To see how this distinction illustrated, let’s return to the example of a hospital interested in predicting which patients are likely to experience complications after surgery. Using past patient data, you are able to develop a model that very accurately answers the question “Given their pre\sphinxhyphen{}surgery vitals, how likely is a patient to experience complications after surgery?” Hooray! The hospital uses this model to determine which patients should get extra nursing visits and extra attention during recovery. You’ve done a great job answering a Passive Prediction Question by discovering a pattern in the world — a set of correlations between measurable variables — you can take advantage of.

\sphinxAtStartPar
Now in the course of developing this model, suppose you discover that one of the strongest predictors of complications after surgery is patient blood pressure — patients with high blood pressure are substantially more likely to experience complications than those with normal blood pressure. This leads you to wonder whether treating patients with high blood pressure with pressure\sphinxhyphen{}reducing medications prior to surgery might reduce complications. In other words, you now want to know the effect of going into the world and manipulating patient blood pressure — a Causal Question.

\sphinxAtStartPar
In the first case, you really don’t care if blood pressure is \sphinxstyleemphasis{causing} the surgical complications, by which we mean you don’t care if reducing blood pressure would reduce complications, or whether high blood pressure is just an easily observable symptom of an underlying condition that is the root cause of surgical complications (like leading a stressful life, or having relationship problems at home). In either case, the \sphinxstyleemphasis{correlation} is sufficient for your purposes of identifying patients you need to keep tabs on.

\sphinxAtStartPar
But if you want to know what would happen if you directly manipulated blood pressure, knowing that blood pressure and complications are correlated is not sufficient. After all, if living alone results in high blood pressure \sphinxstyleemphasis{and} difficulty recovering from surgery, then treating patient blood pressure may have no effect at all!

\sphinxAtStartPar
When answering Passive Prediction Questions, we are searching for correlations we can leverage to make accurate predictions, not causal relationships we can directly manipulate to shape outcomes. Indeed, those who specialize in answering Passive Prediction Questions (like computer scientists who specialized in supervised machine learning) don’t really care that “correlation does not (necessarily) imply causation.”

\sphinxAtStartPar
\sphinxincludegraphics{{correlation}.png}


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{Passive Prediction, Internal Validity, and Alignment}
\label{\detokenize{30_questions/23_passive_internal_alignment_and_bias:passive-prediction-internal-validity-and-alignment}}\label{\detokenize{30_questions/23_passive_internal_alignment_and_bias::doc}}
\sphinxAtStartPar
In this chapter, we will discuss a range of considerations that come into play when evaluating the internal validity of a model designed to answer a Passive Prediction Question. As with Exploratory Questions, I will assume you are already familiar with the usual methods of quantitatively measuring the quality of a model used for Passive Prediction. Instead, my focus will be on the kind of higher\sphinxhyphen{}level considerations that you should reflect on before and after you engage in the standard practices of model fitting and diagnostics you likely learned in your machine learning or statistics course.


\section{Alignment}
\label{\detokenize{30_questions/23_passive_internal_alignment_and_bias:alignment}}
\sphinxAtStartPar
After the last chapter, you’d be forgiven for wondering whether I’m crazy (or just a bad writer) given my insistence on referring to a model that reads mammograms as “a model that answers the question: if a radiologist were to look at this mammogram, would they conclude it showed evidence of cancer?” “A model that reads mammograms” is clearer and more succinct, after all.

\sphinxAtStartPar
So why have I insisted in continuing to discuss models in this way? Because while my formulation is less concise, it is also much more accurate.

\sphinxAtStartPar
The way that most statistical models are developed (“trained”) to answer Passive Prediction Questions is by using a large dataset of “training examples.” These training examples are observations in which the outcome we will eventually ask the model to predict is filled in.

\sphinxAtStartPar
For models designed to predict future events, our training data will be historical data in which the outcomes the model will eventually be designed to predict — like surgical complications or factory machine failures — have already occurred.

\sphinxAtStartPar
For models designed for automation, our training data will be records of a person completing the task the model will eventually be designed to automate. To train a mammogram reading algorithm, in other words, you first need a database of mammograms that radiologists have already labeled as indicating cancer or not. And what the model is actually trained to do is not “find cancer” but rather to “give the same answers given by human radiologists.”

\sphinxAtStartPar
This is where the distinction between “predicting what a radiologist would say” and “detecting cancer” becomes important: because this kind of statistical model was trained to emulate the behavior of radiologists in labelled mammograms, the model will recapitulate any systematic biases held by the human radiologists who reviewed the mammograms in the training data. Did the radiologists struggle to detect cancer in dense breast tissue? So too will the algorithm trained on their labels.

\sphinxAtStartPar
The difference between what you \sphinxstyleemphasis{want} a model to do (detect cancer) and what it \sphinxstyleemphasis{actually} does (guesses what a radiologist would do) is referred to as a model’s \sphinxstyleemphasis{alignment}, and the fact there is nearly always a gap between what we want a model to do and what it actually does is commonly referred to as an “alignment problem.”

\sphinxAtStartPar
And as we’ll discuss in detail below, often this difference between what we \sphinxstyleemphasis{want} a model to do (for example, find the best applicant for a job) and what the model is actually doing (like emulating biased human hiring managers) is how biased algorithms arise.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The terms “alignment” and “the alignment problem” I believe originated among computer scientists interested in artificial intelligence (and the possibility we might someday create a super\sphinxhyphen{}intelligent AI whose interests may not align with our own). With the rise of LLMs like chatGPT and Gemini, use of the term has spread, and data scientists from a range of backgrounds are becoming increasingly comfortable using the term.

\sphinxAtStartPar
Interested in AI safety? Then I cannot over\sphinxhyphen{}recommend the youtube videos of Rob Miles — an AI safety scholar and exceptional science communicator. He has a great \sphinxhref{https://www.youtube.com/watch?v=pYXy-A4siMw}{Intro to AI Safety talk here} that’s a good starting place, and a a HUGE library of videos on AI safety at all levels of granularity and nuance on \sphinxhref{https://www.youtube.com/@RobertMilesAI}{his channel here}.
\end{sphinxadmonition}


\subsection{How do Large Language Models (LLMs) Fit Into This?}
\label{\detokenize{30_questions/23_passive_internal_alignment_and_bias:how-do-large-language-models-llms-fit-into-this}}
\sphinxAtStartPar
Given their emergence as one of the most high profile examples of something people call “AI” these days, it’s worth directly addressing how LLMs like chatGPT, Llama, Bard, etc. fit into this framework.

\sphinxAtStartPar
One of the most powerful ways to understand LLMs is to think of them as tools for answering “If I came across this text {[}whatever text is in the LLMs prompt, pre\sphinxhyphen{}prompts, or other inputs in the model’s context window{]} while wandering around the internet,%
\begin{footnote}[1]\sphinxAtStartFootnote
A more accurate way to phrase this would be “If I came across this text \sphinxstyleemphasis{in my training data}…” For most LLMs, “my training data” is a corpus that consists of both text that is \sphinxstyleemphasis{not} on the internet — like every book every published — and fails to include many things that \sphinxstyleemphasis{are} on the internet but which are difficult to crawl in an automated way. You can learn more about the corpus of internet text used by many LLMs — the 9.5 petabyte \sphinxstyleemphasis{Common Crawl} dataset — \sphinxhref{https://foundation.mozilla.org/en/research/library/generative-ai-training-data/common-crawl/}{here.} But it does seem very clear the \sphinxstyleemphasis{vast} majority of training data comes from the internet. LLMs makers are becoming increasingly caggy about what data they use for training — in part because they don’t wish to make copyright lawsuits against them \sphinxstyleemphasis{too} easy — but books made up only \sphinxhref{https://arxiv.org/abs/2302.13971}{4.5\% of the training data for Meta’s first version of LLaMa}.
%
\end{footnote} what word am I most likely to encounter next?” LLMs then ask this question over and over, adding the newly selected word to the context window one at a time and then feeding the updated “conversation” back into itself as input to help it predict the next word. Indeed, this repetitiveness is why the technology behind most LLMs is called a \sphinxstyleemphasis{recurrent} neural network — because it keeps adding a word to the “conversation” you are having (its “context window”), then feeding the updated conversation back into the model as an updated input.

\sphinxAtStartPar
To be clear, this is a little reductionist. First, LLMs are able to \sphinxstyleemphasis{abstract} away from literal text to something like the meaning of words — they recognize that “pet” and “dog” have similar meanings in the sentences “I took my dog for a walk” and “I took my pet for a walk” — but even these abstractions are the result of looking for common patterns across existing text. And second, LLMs are also “fine\sphinxhyphen{}tuned” by having humans rate responses. But these nuances don’t change the fact that \sphinxstyleemphasis{fundamentally} LLMs are tools for recapitulating text and ideas that already exist in the world, with a strong bias towards what humans have \sphinxstyleemphasis{tended} to write most on the internet — a truth that both explains some of their power, but also helps to explain their fundamental limitations (e.g., their complete lack of fidelity to the truth, the fact they reflect societies’ gender and racial biases, etc.).


\subsection{Alignment and Coding LLMs: An Example}
\label{\detokenize{30_questions/23_passive_internal_alignment_and_bias:alignment-and-coding-llms-an-example}}
\sphinxAtStartPar
To illustrate what an alignment issue looks like with LLMs, let’s consider a simple example I’m stealing from \sphinxhref{https://www.youtube.com/@RobertMilesAI}{Rob Miles}, whose work I recommended above.

\sphinxAtStartPar
When we use a coding assistant LLM, what we \sphinxstyleemphasis{want} is for it to help us write good code. But LLMs aren’t designed to write \sphinxstyleemphasis{good} code, they’re designed to write the code that is most like what they would find on a website/github repo if what came before it looked like the code in the file you’re working on.

\sphinxAtStartPar
A consequence of this is that if you have a project where you’ve been writing \sphinxstyleemphasis{bad} code, what your coding assistant is likely to do is fill in \sphinxstyleemphasis{more bad code}. And crucially, it isn’t filling in bad code because it \sphinxstyleemphasis{can’t} write good code — it’s not a problem of capability — it fills in bad code because what the model does and what we want it to do are different things.

\sphinxAtStartPar
To illustrate, suppose I start writing a function to add two numbers, \sphinxcode{\sphinxupquote{a}} and \sphinxcode{\sphinxupquote{b}}. And suppose I was a really bad programmer who started off with something like:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{add\PYGZus{}two\PYGZus{}numbers}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{a} \PYG{o}{==} \PYG{l+m+mi}{0} \PYG{o+ow}{and} \PYG{n}{b} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}
    \PYG{k}{if} \PYG{n}{a} \PYG{o}{==} \PYG{l+m+mi}{1} \PYG{o+ow}{and} \PYG{n}{b} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}

\sphinxAtStartPar
What code is chatGPT going to suggest to build on what I’ve started? Will it suggest a re\sphinxhyphen{}write to \sphinxcode{\sphinxupquote{return a+b}}? Nope! As of early 2025, this is what I get:

\sphinxAtStartPar
\sphinxincludegraphics{{coding_alignment_suggestion}.png}

\sphinxAtStartPar
(\sphinxhref{https://en.wiktionary.org/wiki/your\_mileage\_may\_vary}{Your mileage may vary} — these models are \sphinxstyleemphasis{constantly} changing, LLM results are probabilistic not deterministic even for the same inputs with the same model, and model behavior will also differ depending on your input context, retained chat history, etc.)

\sphinxAtStartPar
This is obviously an extreme example, but it’s extreme for the purpose of illustration. Perhaps more importantly for \sphinxstyleemphasis{you}, the young data scientist, this happens at all level of coding. No one reading this book is likely to write crazy code like that, but there \sphinxstyleemphasis{is} a good chance you aren’t writing what you think is the best code you’ll ever write. And given that, it’s important to understand that chatGPT isn’t going to try to help you get better — it’s going to help you “do more of the same.” I think this is a \sphinxstyleemphasis{little} off\sphinxhyphen{}topic, but that’s also the risk with using these tools too much as a young programmer: they can get you stuck at a level of programming and understanding, limiting your growth by just reinforcing the practices you have and giving you the illusion you’re better at programming than you actually are.


\section{Bias}
\label{\detokenize{30_questions/23_passive_internal_alignment_and_bias:bias}}
\sphinxAtStartPar
I’ve started with the preceding example because it feels relevant for the average young data scientist, but providing less\sphinxhyphen{}than\sphinxhyphen{}amazing code suggestions is obviously not the reason to be most concerned about alignment issues. The more pervasive and important reason to recognize that the models we use to answer Passive Prediction Questions are not just “finding the best job candidate,” “detecting cancer,” or “identifying the people most likely to commit a crime,” but are instead trying to replicate the (usually human) behavior that gave rise to the training data is that it helps to explain why so many uses of machine learning recapitulate and reinforce human biases.

\sphinxAtStartPar
There was a time, not that long ago, that I heard the CEO of a company that marketed a tool for screening job applications say in a nationally broadcast interview “and the best part is our tool helps to reduce bias because an algorithm can’t be racist.”

\sphinxAtStartPar
Thankfully, I think most data scientists have long since stopped believing in this fallacy. News articles are now regularly published on cases of “racist (or misogynistic) algorithms.” In 2018, for example, Reuters reported that Amazon was forced to scrap an effort to use machine learning to automatically review resumes because it turned out the \sphinxhref{https://www.reuters.com/article/idUSKCN1MK0AG/}{algorithm was biased against women.} Medical algorithms prioritize White patients over Black patients \sphinxhref{https://www.wired.com/story/how-algorithm-blocked-kidney-transplants-black-patients/}{for kidney transplants} and \sphinxhref{https://www.washingtonpost.com/health/2019/10/24/racial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients/}{preventative care}. \sphinxhref{https://www.pewresearch.org/wp-content/uploads/sites/20/2018/12/JobsGender\_report\_FINAL1.pdf}{Online image search has strong gender biases}, favoring men for traditionally male jobs (doctors, lawyers, etc.) and women for traditional female jobs (nurses, teachers, etc.). And \sphinxhref{https://www.theguardian.com/education/2021/feb/18/the-student-and-the-algorithm-how-the-exam-results-fiasco-threatened-one-pupils-future}{racially biased models are being used in place of standardized testing.}

\sphinxAtStartPar
But not everyone understands \sphinxstyleemphasis{why} machine learning is prone to bias. Many think the issue is just one of poor implementation by the data scientists writing and deploying the algorithms. And in some cases, that’s probably the case — \sphinxhref{https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html?unlocked\_article\_code=1.qU4.7TqS.oKJzCzDb3bDG\&amp;smid=url-share}{facial recognition algorithms have been found to perform better on White men than women and People of Color due}, in significant part, to no one bothering to ensure the training data had enough training examples of non\sphinxhyphen{}White men.

\sphinxAtStartPar
But in many other cases, the problem is not negligence, but rather a failure of data scientists to understand exactly how their algorithms work. In short, when algorithms are trained on historic data, then \sphinxstyleemphasis{by design} they pick up not just the patterns in the data you want them to replicate, but also all the inequities and biases of society that were present when that historic data was being created.

\sphinxAtStartPar
To illustrate, consider the example of the Amazon algorithm that was discarded because it turned out to be biased against women. The exact details of the source of the bias is unclear — for obvious reasons Amazon is not eager to report on the failure — but my suspicion is that things went wrong in one of two ways.

\sphinxAtStartPar
The first is that the algorithm was given data on all past Amazon applicant resumes, along with data on which applicants had actually been hired. The algorithm was then effectively trained to answer the question “If an Amazon hiring manager looked at this resume, is it likely they would be hired?” In that case, the algorithm was recapitulating the gender bias of previous hiring managers.

\sphinxAtStartPar
My second guess is that the algorithm was given the resumes of \sphinxstyleemphasis{current} employees along with employee reviews. The algorithm was effectively being asked to answer the question “Given this resume, is it likely this is a person a current manager would rate highly once employed?” In that case, the algorithm was recapitulating gender biases in employee reviews.

\sphinxAtStartPar
In either case, these are examples of an \sphinxstyleemphasis{alignment problem}: the people developing these models \sphinxstyleemphasis{wanted} the algorithm to pick the applicants who would be the most productive employees, but the model they \sphinxstyleemphasis{actually} developed was trying to identify employees who looked like previously successful applicants. Had the previous hiring system been effective, this wouldn’t be a problem, but because the previous system included a gender bias, so too did the resulting algorithm. But because the engineers developing these tools did not think carefully enough about the question the model was actually being taught to answer, the problem was not identified until it was too late.

\sphinxAtStartPar
Bias in using machine learning to answer Passive Prediction Questions, in other words, isn’t usually the result of the algorithm “going awry”; bias in machine learning is usually the result of an algorithm operating exactly as designed: as a mirror that faithfully replicates precisely what it sees in the training data, racism, misogyny, classism, religious intolerance, and all.

\begin{sphinxShadowBox}

\sphinxAtStartPar
Bias in machine learning is usually the result of an algorithm operating exactly as designed: as a mirror that faithfully replicates precisely what it sees in the training data, racism, misogyny, classism, religious intolerance, and all.
\end{sphinxShadowBox}


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{The Right Way To Be Wrong}
\label{\detokenize{30_questions/24_passive_internal_errors:the-right-way-to-be-wrong}}\label{\detokenize{30_questions/24_passive_internal_errors::doc}}
\sphinxAtStartPar
With the rise of online machine learning competitions like Kaggle and an academic literature fixated on publishing papers showing marginal improvements in performance metrics on standard benchmarks, you could be forgiven for thinking that the hardest part of data science is finding the right model and features to max out standard metrics like Area Under the Curve (AUC), Accuracy (share of cases correctly classified) or F1 scores.

\sphinxAtStartPar
However, this is far from the case when it comes to solving real\sphinxhyphen{}world problems. Yes, advancement in academic computer science is often tied to one’s ability to write a new algorithm that performs marginally better on standard benchmarks, and most problem sets or online competitions you encounter will pre\sphinxhyphen{}specify how model performance will be evaluated.

\sphinxAtStartPar
But when it comes to solving real\sphinxhyphen{}world problems, determining what success looks like is actually a core part of your job as a data scientist. And what makes this task difficult is not how you measure your model’s successes — the number of true positives and true negatives the model generates — but in determining the types of mistakes it makes when it gets things wrong.

\sphinxAtStartPar
Understanding how a model fails is just as important as minimizing mistakes in the first place (i.e., maximizing accuracy). Depending on the context, there can be \sphinxstyleemphasis{huge} asymmetries regarding the consequences of false positives and false negatives. Tell someone with cancer they’re fine (false negative), and the result may be the death of the patient from an easily treatable condition; tell someone healthy there’s a chance they have cancer, and you may cause stress and additional tests, but the patient death is very unlikely.

\sphinxAtStartPar
Similarly (but in a much less scary context), classify a credit card applicant as a “good credit risk” who is not actually credit\sphinxhyphen{}worthy (a false positive), and your company may lose the credit limit on the card they issue; classify someone as high risk who is actually not, and your company may lose the transaction fees that customer would have generated, but they won’t lose tens of thousands of dollars in unpaid bills.

\sphinxAtStartPar
When answering Passive Prediction Questions, the choice of how to balance true positives, true negatives, false positives, and false negatives is \sphinxstyleemphasis{the} bridge between the math of statistics and machine learning and the specifics of real\sphinxhyphen{}world problems. And as a result, an ability to speak thoughtfully about how to balance these interests is one of the most important differentiators between data scientists who have only ever fit models for problem sets and data scientists business leaders trust to solve their problems.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In case you need more motivation to care about how best to distribute your true and false positives and negatives to best solve your stakeholder’s problem, allow me to offer the following: if you are comfortable just endorsing a single success metric (accuracy, F1 score, AUC), \sphinxstyleemphasis{most} Passive Prediction Questions can be answered relatively well by automated tools. And that means that if the only thing that differentiates you from the next data scientist (or generation of chatGPT) is that you can get a slightly higher AUC than the person sitting next to you, how much value do you think you are likely bringing to your stakeholder (and thus how well paid)?

\sphinxAtStartPar
Don’t believe me? Consider scikit\sphinxhyphen{}learn — by design, nearly all of the algorithms in that package have the exact same APIs — you run \sphinxcode{\sphinxupquote{test\_train\_split()}}, \sphinxcode{\sphinxupquote{.fit()}}, and \sphinxcode{\sphinxupquote{.predict()}}. That means it’s almost trivially easy to write a program that just loops over all the models in the library, fits them, and checks their performance against the simple metric you chose. Indeed, products exist that do just this, often under names like \sphinxcode{\sphinxupquote{AutoML}}.

\sphinxAtStartPar
Will these do as well as a well trained data scientist? Not yet — there’s skill in feature engineering and choosing which path to go down when computationally constrained. And if you’re someone working at the forefront of developing new machine learning algorithms or infrastructure, then this does not apply to you. But if you are someone who is primarily interested in applying the best tools of data science to solve real world problems, then you should also bear in mind that anything easily computable lends itself to automation.%
\begin{footnote}[1]\sphinxAtStartFootnote
An idea closely related to a point made by Steven Pinker in his 1994 book \sphinxstyleemphasis{The Language Instinct}: “The main lesson of thirty\sphinxhyphen{}five years of AI research is that the hard problems are easy and the easy problems are hard. The mental abilities of a four\sphinxhyphen{}year\sphinxhyphen{}old that we take for granted – recognizing a face, lifting a pencil, walking across a room, answering a question – in fact solve some of the hardest engineering problems ever conceived… As the new generation of intelligent devices appears, it will be the stock analysts and petrochemical engineers and parole board members who are in danger of being replaced by machines. The gardeners, receptionists, and cooks are secure in their jobs for decades to come.”
%
\end{footnote}

\sphinxAtStartPar
Thinking carefully about your stakeholder’s problem and being able to get them to articulate the relative value they place on true and false positives and negatives, then translating that domain expertise into an optimization problem — \sphinxstyleemphasis{that} is a task that requires substantially more critical thinking and interpersonal skills, and consequently is less likely to be made obsolete any time soon.
\end{sphinxadmonition}


\section{The Problem with Accuracy}
\label{\detokenize{30_questions/24_passive_internal_errors:the-problem-with-accuracy}}
\sphinxAtStartPar
Let’s begin our discussion about balancing true and false positives and negatives with a discussion of my least favorite metric for classification problems: Accuracy.

\sphinxAtStartPar
There is perhaps no better way for a data scientist to demonstrate they don’t know what they’re doing than for them to proudly proclaim that their model has an accuracy score of ninety\sphinxhyphen{}something percent without additional context. And yet it is a mistake that I see constantly. It is as though, being students, young data scientists implicitly assume that accuracy scores, like grades, exist on an absolute scale, where values in the 90s are “A”s and something to celebrate and values in the 70s are “C”s and something to feel bad about, when in reality neither is necessarily the case.

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Reporting Accuracy Without Context}

\sphinxAtStartPar
There is perhaps no better way for a data scientist to demonstrate they don’t know what they’re doing than for them to proudly proclaim that their model has an accuracy score of ninety\sphinxhyphen{}something percent without additional context.
\end{sphinxShadowBox}

\sphinxAtStartPar
How do I know this fallacy is common, you ask? As Director of Admissions for the \sphinxhref{https://datascience.duke.edu/}{Duke Masters of Interdisciplinary Data Science (MIDS)} program, I read hundreds of Statements of Purpose essays and resumes every year from aspiring data scientists from around the world. And despite having done this for years, I continue to be shocked by the number of applicants who proudly proclaim something like “I fit a model using XYZ method and was able to achieve a 95\% accuracy score,” or report an accuracy score in the 90s in their resume as though those numbers, absent context, were meaningful.

\sphinxAtStartPar
So why is reporting accuracy scores without context such a problem? There are at least three reasons.


\subsection{Reason 1: Performance is Relative}
\label{\detokenize{30_questions/24_passive_internal_errors:reason-1-performance-is-relative}}
\sphinxAtStartPar
The first problem with reporting accuracy scores absent context is that the value of a model can only ever be evaluated \sphinxstyleemphasis{relative to the best available alternative.} Over the years, I’ve developed the sense that students tend to view accuracy as an absolute scale, very much the way they view grades: 99\% is terrific (an A+!), between 90\% and 99\% is good (an A!), 80\sphinxhyphen{}90 is so\sphinxhyphen{}so (a B), and below 80 is bad.

\sphinxAtStartPar
The reality, however, is that the only way to evaluate model performance is with respect to the \sphinxstyleemphasis{best available alternative}. A model with a 93\% accuracy score is unlikely to be of particular value to a business if the model they were using before you arrived had an accuracy score of 98\%, and your model does not have any other benefits to offset its lower accuracy. Similarly, a model with an accuracy score of 70\% may constitute a considerable innovation to a business that could not make predictions more accurately than with 50\%\sphinxhyphen{}50\% odds. In life, decisions have to be made, so the value of a model is not based on whether it’s perfect, but whether it beats the status quo.

\sphinxAtStartPar
Treating accuracy as an absolute scale also ignores the fact that model performance will always be limited by the amount of \sphinxstyleemphasis{signal} in the data on which it is trained. A data scientist’s job is not to maximize a model’s apparent accuracy, but rather to harness the true predictive potential of the data. Any increases in metrics like accuracy beyond the true potential of the data is illusory, and can only come from overfitting.

\sphinxAtStartPar
Of course, we are not gods, and so we will never know the exact predictive potential of a given dataset, but the principle is one to bear in mind — the potential of a model is always bounded by the data on which is being trained, and the only way to get a model that exceeds that true performance frontier is by overfitting your data (creating an \sphinxstyleemphasis{illusion} of better performance that will not hold up when the model is actually deployed).


\subsection{Reason 2: Accuracy and Imbalanced Data}
\label{\detokenize{30_questions/24_passive_internal_errors:reason-2-accuracy-and-imbalanced-data}}
\sphinxAtStartPar
As detailed in the {\hyperref[\detokenize{10_introduction/23_mistakes::doc}]{\sphinxcrossref{\DUrole{std,std-doc}{introductory chapter to this book}}}}, most data you will encounter in your career will be \sphinxstyleemphasis{imbalanced}, meaning that one of the outcomes you are trying to predict with your model (assuming a classification task) will be much, much less prevalent in the data than the other. In these situations, because accuracy is just “the share of cases correctly classified,” getting high accuracy can be achieved trivially by always predicting the more prevalent outcome.

\sphinxAtStartPar
To illustrate, consider routine mammograms. Mammograms are x\sphinxhyphen{}rays of women’s breast tissue used to screen for early signs of breast cancer. In the United States, it is recommended that all women over 40 get a mammogram every two years. Unsurprisingly, therefore, \sphinxstyleemphasis{vast} majority of routine mammograms are medically unremarkable. According to the Susan G. Komen society, roughly 90\% of routine mammograms are perfectly normal and require no followup.%
\begin{footnote}[2]\sphinxAtStartFootnote
The vast majority of the roughly 10\% of scans that are abnormal are eventually determined to be false positives.
%
\end{footnote}

\sphinxAtStartPar
Consequently, it is trivially easy to write a model that achieves 90\% accuracy:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}cancer\PYGZus{}detection\PYGZus{}model}\PYG{p}{(}\PYG{n}{mammogram}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous} \PYG{o}{=} \PYG{k+kc}{False}
    \PYG{k}{return} \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous}
\end{sphinxVerbatim}

\sphinxAtStartPar
Obviously, of course, this model is worse than useless: it has a 100\% False Negative rate (all mammograms that are abnormal are classified as normal), meaning the algorithm will tell \sphinxstyleemphasis{all} patients they are cancer free, including those whose mammograms show indications of cancer.

\sphinxAtStartPar
Moreover, most data scientists wouldn’t even consider 90/10 data to be particularly imbalanced. In any given year in the United States, only about 3\% of single\sphinxhyphen{}family residential mortgages are in a state of delinquency%
\begin{footnote}[3]\sphinxAtStartFootnote
In other words, they have failed to make a mortgage payment for at least a certain period of days.
%
\end{footnote}, and fraudulent credit card purchases \sphinxhref{https://www.federalreserve.gov/newsevents/pressreleases/other20181016a.htm}{make up less than one\sphinxhyphen{}tenth of 1\% of all credit card transactions}. That means a “model” that reports all mortgages are in good standing or that all credit card transactions are valid will immediately have accuracy scores of 97\% and \(>\) 99.9\%, respectively.

\sphinxAtStartPar
Technically, this is just a special case of Reason 1 accuracy is meaningful absent context — \sphinxstyleemphasis{Performance is Relative} — because the most basic model will always be “assume all observations are from the most prevalent class,” but given how prevalent this trap is, it’s worth being it’s own category.


\subsection{Reason 3: Accuracy Doesn’t Characterize Mistakes}
\label{\detokenize{30_questions/24_passive_internal_errors:reason-3-accuracy-doesn-t-characterize-mistakes}}
\sphinxAtStartPar
The third reason reporting accuracy scores without context — and indeed the reason that accuracy is a problematic metric in general — is that it tells you nothing about the types of misclassifications your model is making. Are all its errors false positives? Are they all false negatives? In what ratio do they occur?

\sphinxAtStartPar
Accuracy says \sphinxstyleemphasis{nothing} about how different types of mistakes are being balanced, which is why accuracy is sometimes a fun statistic to use on problem sets but a \sphinxstyleemphasis{terrible} metric in the real world.


\subsection{ROC and Area Under the Curve (AUC)}
\label{\detokenize{30_questions/24_passive_internal_errors:roc-and-area-under-the-curve-auc}}
\sphinxAtStartPar
“OK, fine,” some of you may be saying, “but no one uses accuracy anymore — we all use ROC AUC scores!” (If you’re a Duke student concurrently taking IDS 705, it’s possible you aren’t there yet, but you will be soon.)

\sphinxAtStartPar
First, again, I can tell you from reading hundreds of essays and looking at hundreds of resumes, accuracy is still an \sphinxstyleemphasis{extremely} commonly used metric. But setting that aside…

\sphinxAtStartPar
Yes, AUC is certainly a more holistic metric than accuracy. Where accuracy evaluates the share of cases correctly classified, AUC averages the share of correct positive predictions across the full range of classification thresholds (one of the reasons it is commonly used in competitions). But it is not a substitute for thinking carefully about the correct metric \sphinxstyleemphasis{for the specific problem you are seeking to solve}.

\sphinxAtStartPar
First, most models are deployed at a specific classification threshold, so averaging across all classification thresholds may create a good \sphinxstyleemphasis{general purpose} indicator of a model’s performance, but it makes AUC poorly suited to evaluating how well a model will perform in any \sphinxstyleemphasis{specific} context (i.e., at a specific classification threshold).

\sphinxAtStartPar
Second, the ROC AUC metric is myopically focused on the proportion of correct positive predictions. But depending on our problem, we may also care about the ratio of false negatives to false positives or other properties of our negative predictions.


\section{Choosing the Best Way to be Wrong}
\label{\detokenize{30_questions/24_passive_internal_errors:choosing-the-best-way-to-be-wrong}}
\sphinxAtStartPar
How, then, should one approach being more thoughtful about model evaluation \sphinxstylestrong{given there is no single “correct” metric} that is universally correct?

\sphinxAtStartPar
The first step is always to evaluate the \sphinxstyleemphasis{relative value} of the four different types of classifications: true positives, true negatives, false positives, and false negatives. Writing a model that reviews the results of blood tests for signs of a terminal but treatable disease? You probably want to associate a strong negative value with false negatives (where you tell a sick patient they’re healthy) and a smaller negative value with false positives (being told you might have a lethal condition is stressful, even if later tests (which may have their own risks) may show it to be a false positive!). And you may then normalize your true positives and true negatives to zero.

\sphinxAtStartPar
You can then fit your classification model and, for each classification threshold, calculate the “cost” of the resulting distribution of true and false positives and negatives. Find the model and classification threshold that minimizes this problem\sphinxhyphen{}specific cost function, and you’ve identified the model and threshold that’s best \sphinxstyleemphasis{for your specific problem}.

\sphinxAtStartPar
Where do these values come from? Sometimes your stakeholder will be able to tell you the actual financial cost of different types of errors (e.g., when deciding whether to issue someone a credit card), but other times these values are more subjective. What’s the relative cost of falsely telling someone they may have a terminal disease condition? How might that vary depending on the risks associated with any followup procedures required to confirm a diagnosis or the amount of time it takes for the diagnosis to be confirmed? Those are hard, subjective questions you may not have the domain expertise to answer yourself. But because you \sphinxstyleemphasis{understand} the role of these values in how your eventual model will operate, you can raise these questions with your stakeholder (who should have better domain knowledge) and solicit values from them.

\sphinxAtStartPar
Similarly, I feel quite confident that anyone \sphinxstyleemphasis{using} a mine\sphinxhyphen{}detection algorithm would really, \sphinxstyleemphasis{really} appreciate a low false negative rate, and would be happy to tolerate a pretty high false positive rage in exchange.


\section{Errors with Non\sphinxhyphen{}Discrete Choice Models}
\label{\detokenize{30_questions/24_passive_internal_errors:errors-with-non-discrete-choice-models}}
\sphinxAtStartPar
Up until now, we’ve emphasized how we manage errors in the context of discrete, binary classification tasks, but it is worth emphasizing that this is only because binary classification is the easiest context in which to think about these problems. However, the issues raised her apply equally to classification tasks with more than two categories, and to efforts to answer Passive Prediction Questions about continuous outcomes. Latent in any model you use is a cost function, and implicit in that cost function is how mistakes are evaluated.

\sphinxAtStartPar
Linear regression, for example, minimizes the sum of squared errors across all observations, and (by default) it gives equal weight to the squared error associated with each observation. But if you don’t feel that’s an appropriate weighting scheme, you are not bound to it — weighted linear regression is a version of linear regression where the user provides a set of weights to associate with each observation. Have some customers you know are more valuable to your company? Perhaps you want to have the model give more weight to errors associated with those customers so the final model performs better for those customers. Or working with data from stores with different sales volumes? Maybe you want to give more weight to stores with larger sales volumes.

\sphinxAtStartPar
Don’t want to work with squared errors at all? Great! There’s a whole discipline called \sphinxhref{https://www.statsmodels.org/stable/examples/notebooks/generated/robust\_models\_1.html}{robust linear modeling} that uses different norms for evaluating errors (often with the goal of reducing the influence of outliers, as the name implies, but all they are doing is modifying how the errors the model seeks to minimize are handled).


\section{Recap}
\label{\detokenize{30_questions/24_passive_internal_errors:recap}}\begin{itemize}
\item {} 
\sphinxAtStartPar
No metric can meaningfully summarize the performance of a model absent information about the broader context.

\item {} 
\sphinxAtStartPar
Model performance only begins to be meaningful when compared with \sphinxstyleemphasis{the next best alternative}.
\begin{itemize}
\item {} 
\sphinxAtStartPar
A special case of this occurs with imbalanced data, where the most naive alternative will always be to “always report the dominant class.” When data is highly imbalanced (90/10, 99/1, 99.999/0.001), accuracy will always be trivially high, since always reporting the dominant class will have accuracy equal to the share of cases that are the dominant class.

\end{itemize}

\item {} 
\sphinxAtStartPar
Accuracy is the quintessential example a metric people think gives an absolute measure of model quality, but these issues apply to any metric, like AUC.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{There is no single “right” way to measure model quality.} A good model balances true positives, true negatives, false positives, and false negatives in a way that reflects the relative real\sphinxhyphen{}world consequences of different types of mistakes.

\end{itemize}


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Fairness and Passive Prediction}
\label{\detokenize{30_questions/25_passive_fairness:fairness-and-passive-prediction}}\label{\detokenize{30_questions/25_passive_fairness::doc}}
\sphinxAtStartPar
\sphinxstyleemphasis{How} our models make mistakes is often just as important as the number of mistakes they make, and not just for financial reasons. Statistical models increasingly inform a range of high\sphinxhyphen{}stakes decisions — like who should get a donated organ, or whether a criminal defendant is entitled to bail. In these situations, the question of who bears the costs of model errors can be an ethically fraught question.

\sphinxAtStartPar
To illustrate the ethical questions that arise when deciding what constitutes “fair” when it comes to models that answer Passive Prediction Questions, let’s consider the example of Risk Assessment models in the US criminal justice system. These Risk Assessment models are used to answer the question “if this criminal defendant (or incarcerated person) was released from custody, what is the likelihood they would re\sphinxhyphen{}offend within the next X months?” These models are used by judges and parole boards who must determine whether arrested individuals should be released while they await trial and whether incarcerated individuals should be paroled (released before the end of their prison sentence to a half\sphinxhyphen{}way house or monitored release).

\sphinxAtStartPar
The way Risk Assessment models are used in the US raises many questions, many outside the scope of this section. But one aspect of risk assessment models recently garnered a lot of attention: whether their misclassifications (cases where an individual the model identified as low risk re\sphinxhyphen{}offends, or where an individual the model identified as high risk does not re\sphinxhyphen{}offend) are racially biased.

\sphinxAtStartPar
The issue of racial bias in misclassifications rose to prominence in 2016 when the investigative news outlet \sphinxhref{https://propublica.org}{ProPublica} published an analysis of the COMPAS Risk Model — one of the most commonly used risk assessment models in the United States. The article — entitled \sphinxhref{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}{Machine Bias} — concluded, in part, \sphinxhref{https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}{that:}
\begin{quote}

\sphinxAtStartPar
Black defendants were often predicted to be at a higher risk of recidivism than they actually were. Our analysis found that black defendants who did not recidivate {[}are newly arrested for committing a crime in the future{]} over a two\sphinxhyphen{}year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts (45 percent vs. 23 percent).
\end{quote}

\sphinxAtStartPar
In other words, the false positive rate (the share of people not re\sphinxhyphen{}arrested predicted to be re\sphinxhyphen{}arrested) for Black defendants was higher than it was for White defendants.%
\begin{footnote}[1]\sphinxAtStartFootnote
I subscribe to the view — excellently articulated by \sphinxhref{https://www.theatlantic.com/ideas/archive/2020/06/time-to-capitalize-blackand-white/613159/?gift=UgvUqS8sUm995p-gPKiVO6whYuC5Y-UXWiJvvQg3jaU\&amp;utm\_source=copy-link\&amp;utm\_medium=social\&amp;utm\_campaign=share}{Kwame Anthony Appiah here} — that White should be capitalized in the same manner as Black.
%
\end{footnote}

\sphinxAtStartPar
While apparently rather damning, COMPAS’ response was that this was actually a consequence of the fact that their model generated equal Positive Predictive rates across Black and White defendants (the share predicted to be re\sphinxhyphen{}arrested that are re\sphinxhyphen{}arrested is the same for both Black and White defendents). And while I am generally loath to defend COMPAS, in this particular case they have a point.

\sphinxAtStartPar
As explored by \sphinxhref{https://www.law.upenn.edu/faculty/sgmayson}{Sandra Mayson} in a \sphinxhref{https://www.yalelawjournal.org/article/bias-in-bias-out}{2019 Yale Law Journal} article, the problem is that more Black defendants end up being re\sphinxhyphen{}arrested in the future than White defendants (in other words, the outcome of interest is imbalanced by race). This, in turn, imposes a simple, mechanical limit on the ability of the model to achieve racial balance in all error rates for reasons discussed below.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the discussion of imbalanced recitivism arrest rates, I said \sphinxstyleemphasis{arrested for} recitivism — a huge problem with all of these risk assessment models is that we know from many studies that even in situations where Black and White Americans \sphinxstyleemphasis{commit} crimes at similar rates (like drug use), Black Americans are substantially more likely to be \sphinxstyleemphasis{arrested} for those crimes. Thus, an imbalance in arrests for recitivism do not necessarily imply differences in \sphinxstyleemphasis{actual} recitivism.

\sphinxAtStartPar
Indeed, this form of inequity is also a problem on the input side — Risk Assessment models take into account whether a defendant has prior convictions, but given Black Americans are more likely to be arrested even in situations where were know criminal behavior occurs at similar rates for Black and White Americans, and Black Americans are likely to have lower\sphinxhyphen{}incomes and thus less likely to have lawyers who can get charges dismissed, these models tend to treat them as higher risk.
\end{sphinxadmonition}


\subsection{Error Rates and Risk Models}
\label{\detokenize{30_questions/25_passive_fairness:error-rates-and-risk-models}}
\sphinxAtStartPar
To illustrate the problem, Mayson’s paper asks the reader to imagine two imaginary groups of individuals, colored grey and black, represented by the two sets of silhouettes in the pictures below. These two groups are Mayson’s stand\sphinxhyphen{}ins for Black Defendants and White Defendants.

\sphinxAtStartPar
\sphinxincludegraphics{{mayson_fig1_hc}.png}

\sphinxAtStartPar
Within each group, some silhouettes have handcuffs super\sphinxhyphen{}imposed. These are the individuals who will eventually be re\sphinxhyphen{}arrested. In practice, of course, no one using a Risk Assessment model would know these future “true outcomes” of individuals, but this is the kind of data you’d have when training a model on retrospective data. In the grey group, 2/10 individuals are eventually re\sphinxhyphen{}arrested, while only 1/10 are re\sphinxhyphen{}arrested in the black group.

\sphinxAtStartPar
\sphinxstylestrong{Confusingly, this coloring implies the grey figures are more likely to be re\sphinxhyphen{}arrested, meaning they are in the position of Black Americans while the black figures are in the position of White Americans.} Mason made this choice to abstract from the specifics of the racial groups in question, but to be honest I think that just confuses the matter for readers and distracts from the empirical realities. I will use the terms “grey figures” and “black figures” to refer to the illustrative entities in these pictures (note I use the lower case and the term “figures”).

\sphinxAtStartPar
The goal of the risk model, therefore, is to predict which figures are most likely to be re\sphinxhyphen{}arrested. The vertical line in the figure represents the classification threshold used by the model (the probability cutoff used to convert continuous imputed probabilities into discrete classifications) — those to the left of the vertical line are those the model has predicted are likely to be re\sphinxhyphen{}arrested, while those to the right are those it predicts are unlikely to be re\sphinxhyphen{}arrested.

\sphinxAtStartPar
For many data scientists, it will be helpful to think about how this figure relates to a traditional confusion matrix, like the one below:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Predicted Positive
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Predicted Negative
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Actual Positive
&
\sphinxAtStartPar
True Positive (TP)
&
\sphinxAtStartPar
False Negative (FN)
\\
\sphinxhline
\sphinxAtStartPar
Actual Negative
&
\sphinxAtStartPar
False Positive (FP)
&
\sphinxAtStartPar
True Negative (TN)
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Where the entire population consists of \(P + N\), where \(P = TP + FN\) and \(N = FP + TN\), and the “positive” condition corresponds to being re\sphinxhyphen{}arrested.

\sphinxAtStartPar
In the image above, there are two True Positives (people predicted to be re\sphinxhyphen{}arrested who actually do end up re\sphinxhyphen{}arrested) and two False Positives (people predicted to be re\sphinxhyphen{}arrested who do \sphinxstyleemphasis{not} end up re\sphinxhyphen{}arrested) for the grey figures. For the black figures, there is one True Positive and one False Positive.

\sphinxAtStartPar
With the threshold in the location shown in the figure, the model classifies four grey figures and two black figures as “likely to be rearrested.” We also see that for both the grey and black figures, the Positive Predictive value of the model (\(\frac{TP}{TP + FP}\), or the share of entities predicted to be re\sphinxhyphen{}arrested who are re\sphinxhyphen{}arrested) is 50\%. Mason refers to this as a model that achieves “Predictive Parity.”

\sphinxAtStartPar
But to achieve this “Predictive Parity,” the False Positive Rate (\(\frac{FP}{N}\), or the share of people who are not eventually re\sphinxhyphen{}arrested who are \sphinxstyleemphasis{predicted} to be re\sphinxhyphen{}arrested) is higher for the grey figures (\(\frac{2 \text{ False Positives}}{8 \text{ Negatives}} = 25\%\)) than the black figures (\(\frac{1 \text{ False Positive}}{9 \text{ Negatives}} = 11\%\)).

\sphinxAtStartPar
Thus while the ProPublica finding is true — the False Positive Rate of COMPAS is higher for Black defendants — the only way to even this out would be to shift the classification threshold for the black outlines over, reducing the number of grey figures who are not actually re\sphinxhyphen{}arrested who are predicted to be re\sphinxhyphen{}arrested.

\sphinxAtStartPar
\sphinxincludegraphics{{mayson_fig2_hc}.png}

\sphinxAtStartPar
This balances the False Positive Rates for the two groups, but in doing so results in the Positive Predictive value of the model being lower for the black outlines.

\sphinxAtStartPar
Can we do better? Well, we could get equal Positive Predictive value and False Positive Rates for grey and black, but only by accepting differential False \sphinxstyleemphasis{Negative} rates (\(FN/P\), or the share of people who are eventually re\sphinxhyphen{}arrested the model predicts will not be re\sphinxhyphen{}arrested), as illustrated in Mayson’s Figure 3 (in which the False Negative rate for grey is 1/2 = 50\% black is 0/1 = 0\%):

\sphinxAtStartPar
\sphinxincludegraphics{{mayson_fig3_hc}.png}

\sphinxAtStartPar
As Mayson writes:
\begin{quote}

\sphinxAtStartPar
As this example illustrates, if the base rate of the predicted outcome differs across racial groups, it is impossible to achieve (1) predictive parity; (2) parity in false\sphinxhyphen{}positive rates; and (3) parity in false\sphinxhyphen{}negative rates at the same time (unless prediction is perfect, which it never is). Computer scientists have provided mathematical proofs of this fact.%
\begin{footnote}[2]\sphinxAtStartFootnote
See Alexandra Chouldechova, \sphinxstyleemphasis{Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments}, 5 BIG DATA 153 (2017) and Jon Kleinberg et al., \sphinxstyleemphasis{Inherent Trade\sphinxhyphen{}offs in the Fair Determination of Risk Scores}, LEIBNIZ INT’L PROC. INFORMATICS, Jan. 2017, at 43:1, 43:4.
%
\end{footnote} When base rates differ, we must prioritize one of these metrics at the expense of another. Race neutrality is not attainable.
\end{quote}

\sphinxAtStartPar
So what’s the “right” answer? What’s the “fair” way to distribute errors? Should the model strive to have the same Positive Predictive value for White and Black defendents? Or the same False Positive rate? Or some balance of the different rates?

\sphinxAtStartPar
The answer is that there is no \sphinxstyleemphasis{right} answer — each of these different schemes is defensible under different ethical frameworks. When you have to make choices about the relative desirability (“value”) of different outcomes, you enter the real of ethics and morality, as discussed in our reading on {\hyperref[\detokenize{30_questions/05_descriptive_v_prescriptive::doc}]{\sphinxcrossref{\DUrole{std,std-doc}{Prescriptive Questions}}}}.

\sphinxAtStartPar
But hopefully, this example makes clear the complexity \sphinxstyleemphasis{and the inescapability} of ethical questions when it comes to the use of data science in high stakes decision making.


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{Passive Prediction Questions and External Validity}
\label{\detokenize{30_questions/27_passive_external_general:passive-prediction-questions-and-external-validity}}\label{\detokenize{30_questions/27_passive_external_general::doc}}
\sphinxAtStartPar
In this chapter, we will discuss the concept of external validity as it pertains to answering Passive Prediction Questions. Where \sphinxstyleemphasis{internal validity} measures how well a model captures meaningful variation in the data we already have, \sphinxstyleemphasis{external validity} measures how well our model is likely to perform when faced with new data.

\sphinxAtStartPar
As we learned before, a model’s external validity is specific to the new context to which it is applied. A model will generally have very \sphinxstyleemphasis{high} external validity in a setting similar to the training context. In contrast, a model will generally have \sphinxstyleemphasis{low} external validity when applied in settings that differ from the training context geographically, temporally, or culturally. For example, a model trained to detect solar panels in satellite images in Arizona is likely to also perform well in the similarly hot arid climate of New Mexico, but performance would likely be lower in states that have different roof geometry as a result of winter snow like Michigan. That same model would also probably have extremely low external validity if used to detect solar panels in rural India or sub\sphinxhyphen{}Saharan Africa where housing structures and the types of panels commonly used are radically different than in the US.

\sphinxAtStartPar
Some of the factors that influence the external validity of Passive Prediction Questions are the same as those that shape the external validity of Exploratory Questions. Models trained on one population or during a specific time period may not generalize, whether used to answer Exploratory or Passive Prediction Questions. There are other concerns that are more specific to Passive Prediction Questions, however, as detailed in this chapter. As in our Exploratory Question readings, I will focus on more holistic considerations as I suspect you’ve already been exposed to more traditional statistical methods of model evaluation.


\section{Extrapolation and Training Parameter Ranges}
\label{\detokenize{30_questions/27_passive_external_general:extrapolation-and-training-parameter-ranges}}
\sphinxAtStartPar
Tools for evaluating internal validity help ensure that statistical and machine learning models will tend to fit the data on which they are trained relatively well. However, while most statistical models are capable of generating predicted values over a very broad range of input values, their reliability outside the range of values on which they were trained is often very limited. Asking a model to make predictions for inputs on which the model wasn’t trained is called \sphinxstyleemphasis{extrapolating}, and is a great way to get oneself into trouble.

\sphinxAtStartPar
To illustrate, consider the two models in the figure below (\sphinxhref{https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/06LeastSquares/extrapolation/complete.html}{source})—one a linear fit, and one a higher\sphinxhyphen{}order polynomial. Both model the data similarly \sphinxstyleemphasis{in the range for which data is available} — and so will perform similarly when one uses the metrics described above to evaluate the model’s internal validity — but make very different predictions when asked to extrapolate values of \sphinxcode{\sphinxupquote{x}} below 0 or above 2.

\sphinxAtStartPar
\sphinxincludegraphics{{extrapolation}.jpg}

\sphinxAtStartPar
Strategies like regularization%
\begin{footnote}[1]\sphinxAtStartFootnote
Regularization is the practice of modifying the loss function of a model to reward simpler specifications that are less prone to overfitting. Examples include adding a penalty term that penalizes coefficients with large absolute values, using weakly informative priors for Bayesian regression, or limiting the influence of outliers.
%
\end{footnote} are designed to constrain the “wonkiness” of models with the goal of making them less likely to go crazy outside the parameter range on which they were trained. Almost by definition, however, absent data in those extended ranges, there’s no way to know for certain whether the model will generalize.


\subsection{What Constitutes Extrapolation?}
\label{\detokenize{30_questions/27_passive_external_general:what-constitutes-extrapolation}}
\sphinxAtStartPar
In the example above, “extrapolation” refers to predicting values below 0 and above 2. However, what constitutes an extrapolation depends in part on the complexity of the model. With a nice, interpretable linear model, it’s not hard to have confidence that the model will make a reasonable prediction for \(x=0.5\), even though that specific value wasn’t in the training data.

\sphinxAtStartPar
But when working with highly non\sphinxhyphen{}linear models — neural networks, random forests, etc. — that aren’t interpretable and aren’t constrained to smooth functional forms — the only place one can feel sure of the behavior of the model is at the exact data points tested. The same flexibility that allows these models to accommodate unusual non\sphinxhyphen{}linear relationships can also lead to bizarre behavior either between points in the training data or where the model has over\sphinxhyphen{}accommodated a couple of idiosyncratic training examples (such as odd cases \sphinxstyleemphasis{or} observations with data entry errors).

\sphinxAtStartPar
For example, a credit risk model may make perfectly reasonable predictions for:
\begin{itemize}
\item {} 
\sphinxAtStartPar
a married 45\sphinxhyphen{}year\sphinxhyphen{}old woman of Hispanic descent who lives in Colorado, and

\item {} 
\sphinxAtStartPar
a married 47\sphinxhyphen{}year\sphinxhyphen{}old woman of Hispanic descent who lives in Colorado

\end{itemize}

\sphinxAtStartPar
but make a crazy prediction for:
\begin{itemize}
\item {} 
\sphinxAtStartPar
a married 46\sphinxhyphen{}year\sphinxhyphen{}old woman of Hispanic descent who lives in Colorado, or

\item {} 
\sphinxAtStartPar
a married 45\sphinxhyphen{}year\sphinxhyphen{}old woman of Hispanic descent who lives in Montana.

\end{itemize}

\sphinxAtStartPar
Indeed, it’s precisely for this reason that for many high\sphinxhyphen{}stakes decisions, regulators are increasingly requiring the use of interpretable models that include guarantees (like monotonicity) — a topic we will return to in a couple of readings.

\sphinxAtStartPar
The more flexible the model, the more data points are required to constrain the model’s behavior (the so\sphinxhyphen{}called “curse of dimensionality”), and the more cautious you should become. There’s a reason that LLMs hallucinate despite being fed unfathomably large amounts of data.


\subsection{Extrapolation with Non\sphinxhyphen{}Tabular Data}
\label{\detokenize{30_questions/27_passive_external_general:extrapolation-with-non-tabular-data}}
\sphinxAtStartPar
Thinking about “extrapolation” when dealing with tabular is sometimes a little easier than with non\sphinxhyphen{}tabular data, like image or video data. Nevertheless, models that take images or video as input are just as sensitive to external validity issues when extrapolating to types of behavior or inputs not seen in their training data.

\sphinxAtStartPar
Here’s a terrific example: for several years, car companies have been adding pedestrian detection algorithms to their cars to augment their automated crash prevention systems. Basically, these systems are designed to detect pedestrians in front of the car and apply the brakes if they determine a collision is imminent, just as they do when they sense an impending collision with another car.

\sphinxAtStartPar
Recently, however, the Insurance Institute for Highway Safety (IIHS) realized a problem with these systems: many companies now sell clothes that have reflective materials to help improve pedestrian visibility at night (they’re especially popular with runners and dog walkers). But it turns out that car companies didn’t train their models using pedestrians wearing reflective strips. As a result, these cars are actually \sphinxstyleemphasis{more} likely to hit pedestrian wearing reflective materials because their vision models fail to recognize what they see as pedestrians, \sphinxhref{https://www.youtube.com/watch?v=uyVk\_VVr2Y8}{as explained here}.

\sphinxAtStartPar
Oops!


\section{Train\sphinxhyphen{}Test\sphinxhyphen{}Splits and External Validity}
\label{\detokenize{30_questions/27_passive_external_general:train-test-splits-and-external-validity}}
\sphinxAtStartPar
A common misconception among young data scientists is that the train\sphinxhyphen{}test\sphinxhyphen{}split workflow used in machine learning inoculates against external validity concerns. After all, the idea of split\sphinxhyphen{}train\sphinxhyphen{}test is that models are trained on one set of observations and evaluated against an entirely different set of observations.

\sphinxAtStartPar
While train\sphinxhyphen{}test\sphinxhyphen{}split can \sphinxstyleemphasis{help} reduce external validity concerns by guarding against overfitting, a fundamental limitation of the workflow is that training and test observations both come from the \sphinxstyleemphasis{same context.} Indeed, because test and training datasets are created by randomly splitting the observations from a single dataset, they should always have the same properties (at least in expectation) — a guarantee one certainly won’t get when moving from the data used to build a model to a real\sphinxhyphen{}world deployment.


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Adversarial Users}
\label{\detokenize{30_questions/28_passive_external_adversarial_users:adversarial-users}}\label{\detokenize{30_questions/28_passive_external_adversarial_users::doc}}
\sphinxAtStartPar
Of all external validity concerns that data scientists tend to underappreciate, none is more likely to cause serious problems than the existence of \sphinxstyleemphasis{adversarial users}.

\sphinxAtStartPar
Adversarial users are users who attempt to subvert the intended function of a statistical or machine learning model. At first blush, adversarial users seem the stuff of spy novels and international espionage, and indeed the term encompasses people trying to deliberately cheat a system by nefarious means (e.g., \sphinxhref{https://gizmodo.com/idiots-who-tried-tiktoks-viral-free-money-glitch-at-atms-are-getting-reported-for-fraud-2000495838}{the 2024 TikTok “Infinite Money Glitch”} in which people deposited fake checks and then immediately withdrew money before the check was invalidated). But it also covers situations in which people use a system in a manner that is entirely within its rules, but do so in a way that the system designers did not foresee, resulting in outcomes the system designers do not desire. Indeed, adversarial users are likely to exist any time an algorithm or model is used to make decisions that are important to people.

\sphinxAtStartPar
The pervasive threat of adversarial users emerges because humans are strategic actors. Once people realize that the outcomes they care about — insurance approvals, promotions, hiring, etc. — are being influenced by a formula, they will change their behavior to respond to the incentives created by the system.


\subsection{Adversarial Users as External Validity Concern}
\label{\detokenize{30_questions/28_passive_external_adversarial_users:adversarial-users-as-external-validity-concern}}
\sphinxAtStartPar
Adversarial users emerge because as soon as a model is deployed to make decisions, that deployment itself constitutes a change in how the world operates. We train our models using historical data in which people are not attempting to adapt their behavior to accommodate the model we are training. But as soon as we deploy our model, people’s behavior will begin to change in response to the deployment, immediately threatening the validity of our model.

\sphinxAtStartPar
To the best of my knowledge, the term “adversarial users” is only used by computer scientists, but the concept that using any type of formula for evaluation or decision\sphinxhyphen{}making will immediately change how people behave (and thus the validity of the formula) has a long and storied history as immortalized by some famous “laws:”
\begin{quote}

\sphinxAtStartPar
“When a measure becomes a target, it ceases to be a good measure.”
\end{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Goodhart\%27s\_law}{Goodhart’s Law, named for Charles Goodhart.}

\end{itemize}
\begin{quote}

\sphinxAtStartPar
“The more any quantitative social indicator is used for social decision\sphinxhyphen{}making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor”
\end{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Campbell\%27s\_law}{Campbell’s Law, named for Donald Campbell}

\end{itemize}
\begin{quote}

\sphinxAtStartPar
“Given that the structure of any {[}statistical{]} model consists of optimal decision rules of economic agents, and that optimal decision rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in policy will systematically alter the structure of {[}statistical{]} models.”
\end{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Lucas\_critique}{Lucas Critique, named for Robert Lucas}

\end{itemize}

\sphinxAtStartPar
And since no idea is serious until it’s been immortalized in an XKCD comic:

\sphinxAtStartPar
\sphinxincludegraphics{{goodharts_law}.png}


\subsection{Robograders: A Close To Home Example}
\label{\detokenize{30_questions/28_passive_external_adversarial_users:robograders-a-close-to-home-example}}
\sphinxAtStartPar
To illustrate what “adversarial users” look like in what may feel like a familiar context, consider the Essay RoboGrader. Training an algorithm to answer the question “If a human English professor read this essay, what score would they give it?” is relatively straightforward — get a bunch of essays, give them to some English professors, then fit a supervised machine learning algorithm to that training data. What could go wrong?

\sphinxAtStartPar
The problem with this strategy is that the training data was generated by humans \sphinxstyleemphasis{who knew they were writing essays for humans to read}. As a result, they wrote good essays. The machine learning algorithm then looked for correlations between human essay rankings and features of the essays, and as a result, it could easily predict essay scores, at least on a coarse scale.

\sphinxAtStartPar
But what happens when humans realize they aren’t being graded by humans? Well, now, instead of writing for a human, they will write for the algorithm. They figure out what the algorithm rewards — big, polysyllabic words (don’t worry, doesn’t matter if they’re used correctly), long sentences, and connecting phrases like “however” — and stuff them into their essays.%
\begin{footnote}[1]\sphinxAtStartFootnote
I cannot for the life of me find the podcast where I first discovered this phenomenon being discussed, but it had lots of great colorful examples of school kids doing this. For now all I can find are these articles: \sphinxhref{https://www.wbur.org/hereandnow/2020/09/03/online-learning-algorithm}{here}, \sphinxhref{https://www.npr.org/2018/06/30/624373367/more-states-opting-to-robo-grade-student-essays-by-computer}{here}, \sphinxhref{https://www.theverge.com/2012/4/23/2969331/erater-robotic-essay-grader-effectiveness}{here}, \sphinxhref{https://www.nytimes.com/2012/04/23/education/robo-readers-used-to-grade-test-essays.html}{here} and \sphinxhref{https://www.vice.com/en/article/pa7dj9/flawed-algorithms-are-grading-millions-of-students-essays}{here}.
%
\end{footnote}

\sphinxAtStartPar
This works because the essay writers who used polysyllabic words and long sentences in the training data happened to also be the students who were writing good essays. These were reliable predictors of scores in essays people wrote for humans. But they \sphinxstyleemphasis{aren’t} a reliable predictor of essay quality in a world where students know the essays \sphinxstyleemphasis{aren’t} being written for humans, just machines.

\sphinxAtStartPar
Another way of thinking about this is that we’re back to the issue of alignment problems: they \sphinxstyleemphasis{want} the algorithm to reward good writing, but that’s not actually what they trained it to do. In this case, however, the alignment problem is rearing its head because people are actively trying to exploit this difference.


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Adversarial User Examples}
\label{\detokenize{30_questions/29_passive_external_adversarial_users_examples:adversarial-user-examples}}\label{\detokenize{30_questions/29_passive_external_adversarial_users_examples::doc}}\begin{quote}

\sphinxAtStartPar
“Economics can be summed up in four words: people respond to incentives. The rest is commentary.”
\end{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Stephen Landsburg

\end{itemize}

\sphinxAtStartPar
Whenever we deploy a model or algorithm to make decisions, that deployment changes what behavior is rewarded — that is, that deployment changes the \sphinxstyleemphasis{incentives} faced by actors. And people respond to incentives. Indeed, even when we try to design our model to reward \sphinxstyleemphasis{exactly} what we want, alignment issues are sure to arise, creating space between what we \sphinxstyleemphasis{want} our model to reward and what it \sphinxstyleemphasis{actually} rewards.

\sphinxAtStartPar
It is for this reason that an empirical regularity may be a good basis for answering Passive Predictive Questions in historical data, but often fails once the model is actually deployed — because deployment itself causes changes in behavior.

\sphinxAtStartPar
To illustrate just how pervasive this is, this reading details a set of examples of adversarial users in domains ranging from internet search to car insurance.


\subsection{Search Engine Optimization}
\label{\detokenize{30_questions/29_passive_external_adversarial_users_examples:search-engine-optimization}}
\sphinxAtStartPar
The history of Google and other search engines is, essentially, a history of adversarial users ruining things for everyone. Have you ever wondered why, when you search for a recipe online, you have to scroll through paragraphs of pointless narrative before you get to the actual recipe? Or why YouTube thumbnails are full of shocked faces and clickbait titles? In short: adversarial users!

\sphinxAtStartPar
Google’s perpetual challenge is to (a) find features that identify the websites that users want to see at the top of their search results, (b) update their search algorithm to up\sphinxhyphen{}rank sites with those features, then (c) find new features to use as everyone figures out what features Google is rewarding and adds them to their spammy sites.

\sphinxAtStartPar
In the beginning, for example, Google’s first ranking algorithm — \sphinxhref{https://en.wikipedia.org/wiki/PageRank}{PageRank} — essentially up\sphinxhyphen{}ranked sites that were linked to by other sites.%
\begin{footnote}[1]\sphinxAtStartFootnote
Google didn’t just count the number of links, of course, it weighted links by the ranking of the referring site. A link from a high\sphinxhyphen{}ranked source was more valuable than a link from a random blog.
%
\end{footnote} The more the web seemed to “like” a site, the higher it would rank in Google! Essentially, it outsourced the evaluation of website quality to web itself, generating results of a quality that quickly turned “Google” from a noun to a verb.

\sphinxAtStartPar
It wasn’t long, though, before people realized there was a way to game this system. If a site owner could increase the number of links to their site, they could increase their ranking and, thus, their site traffic. So people started creating websites just to create links to the page on which they made money. Entire ecosystems emerged of people and sites linking to one another to “artificially” boost rankings, a practice known as Search Engine Optimization (SEO).

\sphinxAtStartPar
Google, of course, noticed this and shifted metrics. Over the years, Google has been forced to turn to a nearly endless number of different heuristics for evaluating page quality, including things like “time users spend on a page” or “number of user clicks on a result.” Each time this occurred, adversarial users looked for ways to game the system, and even well\sphinxhyphen{}intentioned websites were forced to join the rat race as well, often making their sites worse to ensure they could compete with the “high scores” being generated by bad actors.

\sphinxAtStartPar
(If you’re interested in multitudinous ways in which SEO is responsible for how the internet looks and feels today, you can do no better than this recent feature from \sphinxhref{https://www.theverge.com/features/23931789/seo-search-engine-optimization-experts-google-results}{The Verge}.)


\subsection{Zillow}
\label{\detokenize{30_questions/29_passive_external_adversarial_users_examples:zillow}}
\sphinxAtStartPar
In 2021, the real estate information website Zillow \sphinxhref{https://www.wsj.com/articles/zillow-quits-home-flipping-business-cites-inability-to-forecast-prices-11635883500?mod=article\_inline}{announced} that it was shutting down an initiative to use its price models to buy and flip US residential houses, an initiative it later admitted had lost a \sphinxhref{https://www.wsj.com/articles/zillows-shuttered-home-flipping-business-lost-881-million-in-2021-11644529656?st=Bo8Ysa\&amp;reflink=desktopwebshare\_permalink}{staggering \$881 million dollars} in 2021 alone.

\sphinxAtStartPar
So what went wrong? Portions of Zillow’s loss appear to have been the result of over\sphinxhyphen{}confidence in their ability to predict overall housing market movements. But \sphinxhref{https://www.gsb.stanford.edu/insights/flip-flop-why-zillows-algorithmic-home-buying-venture-imploded}{as pointed out} by a group of finance professors at the Stanford Graduate School of Business, another major contributor was likely to have been a phenomenon known as “adverse selection,” a special flavor of the adversarial user problem.

\sphinxAtStartPar
To understand what happened, put yourself in Zillow’s shoes — you have a model that’s quite good at predicting the price at which houses will sell (the estimates from this model are referred to as “Zestimates”). So good, in fact, that you think you could make some money by offering to pay homeowners cash to buy their homes at a discount to your “Zestimate” of the home’s value, then flip the house for what it’s really worth.

\sphinxAtStartPar
(If you’ve never done it before, selling a home in the US is an \sphinxstyleemphasis{incredibly} complicated and time\sphinxhyphen{}consuming process, so it’s not unreasonable to expect many people would accept a slightly low price in exchange for a quick sale.)

\sphinxAtStartPar
So you pour \sphinxstyleemphasis{billions} into the house flipping business, using your Zestimate model to decide what homes are worth. And… well, you know how this ends: by the end of 2021, you’d lost 881 million dollars. But why?

\sphinxAtStartPar
Well, imagine there are two houses in similar neighborhoods with similar square footage, the same number of beds and bathrooms, the same school system, etc. Suppose your Zestimate for both homes — based on past sales and all publicly available data on the house and a few questions you ask the sellers — is \$350,000. So you offer them both \$300,000 for their homes, figuring you’ll make a profit of \$50,000 on each.

\sphinxAtStartPar
But as a data scientist, you know that all models are imperfect. Sure, on average, your Zestimates are dead on, but neither of these homes is probably worth exactly \$350,000. Suppose the first home — Home A — has a truly beautiful view over one of the best parks in the city, and while it’s not too far from the freeway, there is a set of tall apartment buildings between the home and the freeway that block all traffic noise. Let’s suppose that because of all of these factors — factors that aren’t available to the Zillow algorithm — the \sphinxstyleemphasis{true} value of Home A is actually \$450,000.

\sphinxAtStartPar
Now let’s consider Home B. It’s the same distance from the freeway, but where tall apartment buildings block traffic noise from Home A, the local geography channels the noise \sphinxstyleemphasis{right at} Home B. Moreover, while every other home on the street has a great view of the nearby park, \sphinxstyleemphasis{right} across the street from Home B is a city electrical utility station. The house also has fewer windows, and all are blocked by neighbor’s homes. Again, because of all these factors that aren’t available to the Zillow algorithm, the \sphinxstyleemphasis{true} value of Home B is actually only \$250,000.

\sphinxAtStartPar
So when you, Zillow, make offers of \$300,000 to the owners of both Home A and Home B, what do you think each owner will do? Well, obviously, the owner of Home A is gonna think, “I’m getting low\sphinxhyphen{}balled. No way I’m selling for \$300,000.” And the owner of Home B is gonna say, “holy cow, what are these idiots thinking? Yes! Please! I will absolutely sell you my house for \$300,000!” And Zillow will lose \sphinxstyleemphasis{at least} \$50,000 on Home B even before it has to pay fees and taxes. And \sphinxstyleemphasis{that’s} how companies lose hundreds of millions trying to buy and sell homes at scale based on models, a phenomenon that’s not only impacted Zillow but \sphinxhref{https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3616555}{also other companies that tried to do something similar during this period}.


\subsubsection{Adverse Selection and Asymmetric Information}
\label{\detokenize{30_questions/29_passive_external_adversarial_users_examples:adverse-selection-and-asymmetric-information}}
\sphinxAtStartPar
Economists use the term adverse selection to describe this phenomenon. Homeowners were \sphinxstyleemphasis{selecting} whether to accept Zillow’s offer and doing so in a way that was \sphinxstyleemphasis{adverse} (bad) for Zillow. Deals that were bad for Zillow were especially likely to happen because the homeowners in our example had private information about the value of their homes that was unavailable to Zillow’s algorithm. In economics, we refer to this as asymmetric information, and it happens everywhere.


\subsection{Insurance}
\label{\detokenize{30_questions/29_passive_external_adversarial_users_examples:insurance}}
\sphinxAtStartPar
Another context in which adversarial users are a problem (again due to adverse selection concerns) is insurance. For example, consider car insurance. In the United States, car insurance is provided by private companies, and these private companies have to be sure that the total amount of money they take in monthly premiums from all their clients is enough to cover what they pay out to clients who experience accidents. That means that the more clients a car insurance company has that are safe drivers, the less they will have to pay out for car repairs, and thus the lower they can set their monthly premiums and the more generous they can be with deductibles.

\sphinxAtStartPar
\sphinxstyleemphasis{BUT:} most people have a sense of whether they are good drivers or not, and the people who get the most out of car insurance are the people who get into a lot of accidents. So if you’re an insurance company, how do you ensure that you aren’t swamped by bad drivers, especially while also offering as low a price as possible to win business?


\subsubsection{Reduce the Information Asymmetry}
\label{\detokenize{30_questions/29_passive_external_adversarial_users_examples:reduce-the-information-asymmetry}}
\sphinxAtStartPar
With car insurance, the main strategy companies use is to try and reduce the information asymmetry that gives rise to adverse selection (i.e., learn as much as they can about whether a potential customer is a good driver). Car insurance companies collect as much information on clients as they can before setting a price, like customer accident and traffic ticket histories. Insurance companies also offer discounts to drivers willing to install a device on their car that monitors how they drive (e.g., do they speed, slam the brakes, etc.) if they show evidence someone is a good driver. One of my students even reported that when they applied for car insurance, the company even asked for the GPA!

\sphinxAtStartPar
Insurance companies will even go as far as to secretly buy data \sphinxhref{https://www.nytimes.com/2024/04/23/technology/general-motors-spying-driver-data-consent.html?unlocked\_article\_code=1.sE4.eFtk.dNaHXEZcJ5rc\&amp;smid=url-share}{that car manufacturers are quietly (and in some cases, illegally) collecting} on individual’s driving behavior.


\subsubsection{Limit Opportunities for Selection and Sorting}
\label{\detokenize{30_questions/29_passive_external_adversarial_users_examples:limit-opportunities-for-selection-and-sorting}}
\sphinxAtStartPar
Attempting to reduce information asymmetries is one strategy for avoiding adverse selection; another strategy is to limit opportunities for customers to shop around for the policy that is best for \sphinxstyleemphasis{them}. That’s the strategy commonly employed by US health insurance companies. If customers could change health insurance any time they wanted, a strategic customer would not enroll in health insurance (or enroll in a low\sphinxhyphen{}premium, high deductible, high co\sphinxhyphen{}pay policy) when they were healthy, and if they got sick, switch to a more expensive but more generous policy.

\sphinxAtStartPar
To prevent this, US health insurance policies only allow people to enroll or change their policies once a year (during an “open enrollment” period, which is usually October).%
\begin{footnote}[2]\sphinxAtStartFootnote
Legally, health insurance companies also have to let you change policies if you experience one of several “qualifying life events,” like changing jobs or having a baby.
%
\end{footnote} This limits customers’ ability to change policies in response to sudden changes in their health status.

\sphinxAtStartPar
Similarly, some health insurance policies are only open to qualified customers. For example, Duke has several health insurance policies only available to Duke faculty and employees. Most people who are especially prone to illness can’t just “become a Duke employee,” so adverse selection is less of a concern for this group of customers. That helps ensure Duke employees don’t end up with excessive medical bills, allowing the insurance provider to provide Duke employees with lower\sphinxhyphen{}cost insurance.


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{Interpretability and Passive Prediction}
\label{\detokenize{30_questions/32_passive_interpretable_models:interpretability-and-passive-prediction}}\label{\detokenize{30_questions/32_passive_interpretable_models::doc}}
\sphinxAtStartPar
It is my aim, at some point soon, to have my own chapter here. But for now, two external readings that are hard to beat!
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://arxiv.org/abs/1811.10154}{“Stop Using Black Boxes”} by Cynthia Rudin. Who is Cynthia Rudin? \sphinxhref{https://biostat.duke.edu/news/faculty-cynthia-rudin-wins-1-million-artificial-intelligence-prize-new-nobel}{A truly amazing scholar.}

\item {} 
\sphinxAtStartPar
Using Black Boxes requires \sphinxhref{https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html?unlocked\_article\_code=1.pk4.d6Ix.JKuN5\_dxK\_sS\&amp;smid=url-share}{trusting the data too}.

\end{itemize}



\sphinxstepscope


\chapter{Using Causal Questions}
\label{\detokenize{30_questions/35_using_causal_questions:using-causal-questions}}\label{\detokenize{30_questions/35_using_causal_questions::doc}}
\sphinxAtStartPar
In our past readings, we’ve learned about the value of both Exploratory and Passive Prediction Questions for solving problems.

\sphinxAtStartPar
Exploratory Questions help us better understand the contours of our problem — where our problem is most acute, whether there are groups who have figured out how to get around the problem on their own, etc. This, in turn, helps us identify where to prioritize our subsequent efforts.

\sphinxAtStartPar
Passive Prediction Questions have two main purposes. First, they help us to identify individual entities — patients, customers, products, etc. — to whom we may wish to pay extra attention or recommend certain products. Second, models built to address Passive Prediction Questions can also be used to automate tasks by predicting how a person \sphinxstyleemphasis{would} have classified an entity or behaved in a given setting.

\sphinxAtStartPar
In both cases, however, there is a drawback \sphinxhyphen{} answering these questions only helps us understand the world around us, not how our actions on the world will have an impact. But as data scientists, we will often want to act to address the problems that motivate us. Wouldn’t it be great if there was also a set of tools designed to help us predict the consequences of our own actions?

\sphinxAtStartPar
Enter \sphinxstyleemphasis{Causal Questions}. Causal Questions ask what effect we can expect from our actions. For example, “What effect will changing the interface of our website have on online sales?” or “What effect will prescribing a drug have on a patient?”

\sphinxAtStartPar
Because they help us understand the consequences of actions we might take, it should come as no surprise that the ability to answer Causal Questions is of \sphinxstyleemphasis{profound} interest to everyone from companies to doctors and policymakers. At the same time, however, it will also come as no surprise that answering Causal Questions is an inherently challenging undertaking.

\sphinxAtStartPar
In this reading, we will discuss both where Causal Questions arise in practice and a workflow for answering them, but we will gloss over the nuances of \sphinxstyleemphasis{how} exactly we answer Causal Questions. In our next reading, we will turn from workflows to theory and discuss in detail what it actually means to measure the effect of an action \(X\) (e.g., administering a new drug to a patient or showing an ad to a user) on an outcome \(Y\) (patient survival, customer spending, etc.). This section may feel a little abstract and woo\sphinxhyphen{}woo at times, but please hang in there. Answering Causal Questions is as much about critical thinking as it is about statistics, and understanding the concepts introduced here will be critical to your success in this domain.


\section{When Do Causal Questions Come Up?}
\label{\detokenize{30_questions/35_using_causal_questions:when-do-causal-questions-come-up}}
\sphinxAtStartPar
Causal Questions arise when stakeholders want to \sphinxstyleemphasis{do} something — buy a Superbowl ad, change how the recommendation engine in their app works, authorize a new prescription drug — but they fear the action they are considering may be costly and not actually work. In these situations, stakeholders will often turn to a data scientist in the hope that the scientist can provide greater certainty about the likely consequences of different courses of action before the stakeholder is forced to act at scale. This, in turn, helps to reduce the risk the stakeholder has to bear when making their decision — something all stakeholders appreciate.
\sphinxstyleemphasis{Usually}, the action the stakeholder is considering will not have been chosen at random. Rather, a stakeholder will generally pose a Causal Question because they have some reason to suspect a given course of action may be beneficial. Indeed, Causal Questions often arise in response to patterns discovered when answering Exploratory or Passive Prediction Questions.


\subsection{Where Causal Questions Come From — An Example}
\label{\detokenize{30_questions/35_using_causal_questions:where-causal-questions-come-from-an-example}}
\sphinxAtStartPar
Suppose the Chief of Surgery at a major hospital is interested in reducing surgical complications. The hospital begins by asking, “What factors predict surgical complications?” (a Passive Prediction Question) and developing a model that allows it to identify patients who are likely to experience complications during recovery.

\sphinxAtStartPar
While developing this model, the hospital discovered that blood pressure was one of the strongest predictors of surgical complications—patients with high blood pressure are substantially more likely to experience complications than those with normal blood pressure.

\sphinxAtStartPar
This leads the Chief to wonder whether they could reduce surgical complications if they treated patients who have high blood pressure with pressure\sphinxhyphen{}reducing medications before surgery. In other words, the Chief Surgeon wants to know, “What effect would treating patients with high blood pressure have on surgical complication rates?”

\sphinxAtStartPar
Rather than just giving all patients with high blood pressure new drugs (and delaying their surgeries while the drugs take effect), the Chief wants \sphinxstyleemphasis{you} to provide a rigorous answer to her question. After all, high blood pressure \sphinxstyleemphasis{may} cause surgical complications, in which case the blood pressure medication \sphinxstyleemphasis{may} reduce complications. But it might also be that high blood pressure is a \sphinxstyleemphasis{symptom} of a third factor that causes both high blood pressure \sphinxstyleemphasis{and} surgical complications. For example, some lower\sphinxhyphen{}income patients may be experiencing stressful lives and could have more difficulty taking time off after surgery to recover, both of which are factors that could contribute to higher blood pressure. In this case, high blood pressure is useful for identifying patients likely to experience surgical complications. However, treating high blood pressure wouldn’t reduce complications since those patients would still be unable to take time off after surgery.

\sphinxAtStartPar
And so, a Causal Question is born!




\section{The Causal Question Work Flow}
\label{\detokenize{30_questions/35_using_causal_questions:the-causal-question-work-flow}}
\sphinxAtStartPar
Before we dive into the technical details of answering Causal Questions, it’s worth providing a high\sphinxhyphen{}level overview of how data scientists approach answering them.


\subsection{Identify Relevant Previous Studies}
\label{\detokenize{30_questions/35_using_causal_questions:identify-relevant-previous-studies}}
\sphinxAtStartPar
Once a Causal Question has been posed, the next step is \sphinxstylestrong{to identify any research that has already been done} that may help answer your causal question. It’s hard to overstate how often data scientists overlook this step, but it’s \sphinxstyleemphasis{such} a no\sphinxhyphen{}brainer once you think of it! There’s no reason to spend days or weeks designing a study to answer a question if someone else has already put the time and money into doing it for you!

\sphinxAtStartPar
If your stakeholder works in public policy or medicine, then the first place to look for previous studies is in academic medical or policy journals. But don’t assume that if you aren’t working on a medical or public policy question, you won’t be able to find an answer to your question in academic or pseudo\sphinxhyphen{}academic publications — lots of data scientists present research done at private companies in “industry” conferences like the \sphinxhref{https://ide.mit.edu/events/2022-conference-on-digital-experimentation-mit-codemit/}{MIT Conference on Digital Experimentation (CODE@MIT)} or the \sphinxhref{https://netmob.org/}{NetMob Cellphone MetaData Analysis Conference}!

\sphinxAtStartPar
And if you are at a company, ask around! Someone at your own company may have investigated a similar question before, and talking to them could save you a lot of effort.


\subsection{Evaluate Previous Studies}
\label{\detokenize{30_questions/35_using_causal_questions:evaluate-previous-studies}}
\sphinxAtStartPar
If you do find studies, then for each study, you will have to ask yourself two questions:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Did the study authors do a good job of answering the Causal Question? \sphinxstyleemphasis{in the context they were studying}}?

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Do I believe that the \sphinxstyleemphasis{context} in which the study was conducted is similar enough to my context that their conclusions are relevant to me?}

\end{itemize}

\sphinxAtStartPar
This first question is about the \sphinxstyleemphasis{internal validity} of the study, and we’ll talk at length about how to evaluate that in the context of causal inference in the coming weeks. The second question is about the \sphinxstyleemphasis{external validity} (i.e., the generalizability) of the study to your context. There are lots of extremely well\sphinxhyphen{}conducted studies in the world that may be seeking to answer the same question as you. Still, if they investigated the effect of a new drug in \sphinxstyleemphasis{young} patients and your hospital only treats very old patients, you may not be comfortable assuming their results are good predictors for what might happen in your hospital.


\subsection{Plan A New Study}
\label{\detokenize{30_questions/35_using_causal_questions:plan-a-new-study}}
\sphinxAtStartPar
If you were unable to find any studies that answer your Causal Question satisfactorily (either on their own or in combination), then it may be time to do a study of your own!

\sphinxAtStartPar
When most people think about answering Causal Questions, their minds immediately jump to randomized experiments. Randomized experiments are \sphinxstyleemphasis{often} the best strategy for trying to answer Causal Questions, but they are not always the best choice.

\sphinxAtStartPar
Studies designed to answer Causal Questions can be divided into roughly two types: experimental studies and observational studies.

\sphinxAtStartPar
In an experimental study, a researcher has control over everything that happens, including who enrolls in the study and who gets assigned to the treatment group and who gets assigned to the control group. Examples of experimental studies include nearly all clinical trials, A/B tests where the version of a website or app users see is randomly determined, and field experiments where, say, voters are randomly assigned to receive different types of mailers from political campaigns to measure their effect on voter turnout.

\sphinxAtStartPar
In an observational study, by contrast, researchers use data from a context where the researchers did not control who was treated and who was not. This includes data from public opinion surveys, data on user behavior and demographics, or census data.

\sphinxAtStartPar
(We say studies can be divided into roughly two types because some studies fall into a category sometimes called “quasi\sphinxhyphen{}experimental.” In these studies, researchers were not in control over who was treated and who was not, but they have some reason for thinking that \sphinxstyleemphasis{something in the world} — like a chance storm or a draft lottery — caused who was treated and who was not to be determined randomly. But these types of studies tend to be more relevant for academics than applied data scientists, and evaluating them is incredibly difficult, so we will largely ignore them in this text.)

\sphinxAtStartPar
While it is sometimes believed that only experimental studies can generate valid answers to Causal Questions, this is \sphinxstyleemphasis{unequivocally untrue}, as is the slightly more generous version of this claim — that experimental studies always constitute the best evidence for answering Causal Questions. As we will explore in \sphinxstyleemphasis{great} detail in the coming readings, the validity of conclusions drawn from \sphinxstyleemphasis{both} experimental and observational studies rests on whether a number of fundamentally untestable assumptions hold. As a result, both types of studies are capable of providing meaningful answers to causal questions \sphinxstyleemphasis{and} of being deeply misleading.

\sphinxAtStartPar
Moreover, while experimental studies often (but not always) have greater \sphinxstyleemphasis{internal validity} (they are often better able to ensure that they have measured the true causal effect in the lab), this often comes at the expense of lower \sphinxstyleemphasis{external validity} because ensuring the researchers can control who is treated and who is not requires operating the study take place in a highly monitored, often artificial and unrealistic setting. Observational studies, by contrast, are often based on data collected in the real world and, as a result, may yield answers that tell us more about what is likely to happen in our own real\sphinxhyphen{}world application, even if they have somewhat lower internal validity.


\section{Wrapping Up and Next Steps}
\label{\detokenize{30_questions/35_using_causal_questions:wrapping-up-and-next-steps}}
\sphinxAtStartPar
Hopefully, this reading has given you a better sense of \sphinxstyleemphasis{how} Causal Questions are used to solve stakeholder problems and when and where they come up in the life of a practicing data scientist. In the readings that follow, we will turn first to the details of the \sphinxstyleemphasis{Potential Outcomes Model}, a rigorous, formal statistical framework for understanding the Counterfactual Model of Causality. This framework will not only provide a presentation of the Counterfactual Model of Causality that may be appealing to those who draw intuition from mathematical formalism but also machinery that we can use to evaluate how much confidence we can have in answers generated using different methods of answering Causal Questions — including both experimental and observational studies.

\sphinxstepscope


\section{Answering Causal Questions}
\label{\detokenize{30_questions/40_answering_causal_questions:answering-causal-questions}}\label{\detokenize{30_questions/40_answering_causal_questions::doc}}
\sphinxAtStartPar
In this reading, we turn the surprisingly slippery question “What do we mean when we say X causes Y, and how do we measure the effect of an action \(X\) (e.g., administering a new drug to a patient, or showing an ad to a user) on an outcome \(Y\) (patient survival, customer spending, etc.)?”

\sphinxAtStartPar
While this reading may come across as much more abstract than previous chapters, it must be emphasized that answering Causal Questions is as much about critical thinking as it is about statistics. The concepts introduced here will prove crucial to your effectiveness in this space.


\subsection{What Does It Mean for X to Cause Y?}
\label{\detokenize{30_questions/40_answering_causal_questions:what-does-it-mean-for-x-to-cause-y}}
\sphinxAtStartPar
To understand what it means to answer a Causal Question and why answering Causal Questions is intrinsically hard, we must start by taking a step back to answer the question: “What do we mean when we say some action X \sphinxstyleemphasis{causes} a change in some outcome Y?”

\sphinxAtStartPar
Seriously, what do we mean when we say “X causes Y?” Try and come up with a definition!

\sphinxAtStartPar
While this question may \sphinxstyleemphasis{seem} simple, this question has actually been the subject of serious academic debate for hundreds of years by philosophers no less famous than David Hume. Indeed, even today, there is still debate over how best to answer this question.

\sphinxAtStartPar
In this course, we will use the \sphinxstyleemphasis{Counterfactual Model of Causality} (sometimes called the Neyman\sphinxhyphen{}Rubin causal model). In plain English, it posits that for “doing X to cause Y,” it must be the case that if we do X, then Y will occur, and if we did not do X, Y would not occur. This is by far the most used definition of causality today, and yet remarkably, it only emerged in the 20th Century and was only really fleshed out in the 1970s. Yeah… that recently.

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Counterfactual Model of Causality}

\sphinxAtStartPar
For it to be the case that doing X causes Y”, it must be the case that if we do X, then Y will occur, and if we did not do X, then Y would not occur.
\end{sphinxShadowBox}


\subsection{Measuring the Effect of X on Y}
\label{\detokenize{30_questions/40_answering_causal_questions:measuring-the-effect-of-x-on-y}}
\sphinxAtStartPar
At first blush, this definition may seem simple. But its simplicity belies a profoundly difficult practical problem. See, this definition relies on \sphinxstyleemphasis{comparing} the value of our outcome Y in two states of the world: the world where we do X and the world where we don’t do X. As we only get to live in one universe, we can never perfectly know what the value of our outcome Y would be in \sphinxstyleemphasis{both} a world where we do X and one where we don’t do X for a given entity at a given moment in time. As such, we can \sphinxstylestrong{never} directly measure the causal effect of X on Y for a given entity at a given moment in time — a problem known as the \sphinxstylestrong{Fundamental Problem of Causal Inference} (\sphinxcode{\sphinxupquote{causal inference}} being what people call the practice of answering Causal Questions).

\sphinxAtStartPar
To illustrate, suppose we were interested in the effect of taking a new drug (our X) on cancer survival (our Y) for a given patient (a woman named Shikha who arrived at the hospital on June 18th, 2022). We can give her the drug and evaluate whether she is still alive a year later, but that alone can’t tell us whether the new drug \sphinxstyleemphasis{caused} her survival according to our counterfactual model of causality — after all, if she survives, maybe she would have survived even without the drug! To confirm the effect of the drug on Shikha \sphinxstyleemphasis{by direct measurement,} we would have to be able to measure her survival both in the world where we gave her the drug \sphinxstyleemphasis{and} the world where we did not and compare outcomes.

\sphinxAtStartPar
Since we can never see both states of the world — the world where we undertake the action whose effect we want to understand and the world where we don’t — almost everything we do when trying to answer Causal Questions amounts to trying to find something we \sphinxstyleemphasis{can} measure that we think is a \sphinxstyleemphasis{good approximation} of the state of the world we can’t see.

\sphinxAtStartPar
A quick note on vocabulary: by convention, we refer to the action whose effect we want to understand as a “treatment” and the state of the world where an entity receives the treatment as the “treated condition.” Similarly, we refer to the state of the world where an entity does \sphinxstyleemphasis{not} receive the treatment as the “control condition.” We use this language even when we aren’t talking about medical experiments or even experiments in general. We also refer to the state of the world we cannot observe as the “counterfactual” of the world we can observe — so the world where Shikha does not get the cancer drug is the \sphinxstyleemphasis{counterfactual condition} to the world where Shikha does get the drug.

\sphinxAtStartPar
It’s at this point most people start throwing out “but what about…“‘s, and that’s good! You should be — that’s exactly the kind of thinking you have to do when trying to answer Causal Questions. For example, “What about if we measured the size of Shikha’s tumor before she took the drug and compared it to the size of her tumor after? If the tumor got smaller as soon as she started the drug, then surely the drug caused the tumor to shrink!”

\sphinxAtStartPar
Maybe! Implicitly, what you have done is asserted that you think that the size of Shikha’s tumor before we administered the drug is a good approximation of what you think the size of Shikha’s tumor \sphinxstyleemphasis{would have been} had we not given her the drug.

\sphinxAtStartPar
But this type of comparison will always fall short of the Platonic ideal given by our definition of causality. Yes, Shikha’s tumor \sphinxstyleemphasis{may} have stayed the same size if we had not given her the drug (in which case the size of the tumor before she took the drug would be a good approximation), but it is also possible that regardless of whether we’d given her the drug, her cancer would have shrunk on its own.%
\begin{footnote}[1]\sphinxAtStartFootnote
The fact that diseases naturally change over time on their own is known as a disease’s “natural history.”
%
\end{footnote}

\sphinxAtStartPar
According to the Counterfactual Model of Causality, we could only ever \sphinxstyleemphasis{know} if taking the drug caused a decrease in tumor size if we could both administer the drug and observe the tumor \sphinxstyleemphasis{and also observe a parallel world in which the same person at the same moment in time was not given the drug for comparison}. Therefore, since we can never see this parallel world — the \sphinxstyleemphasis{counterfactual} to the world we observe — the best we can do is come up with different, imperfect tricks for \sphinxstyleemphasis{approximating} what might have happened in this parallel world, like comparing the tumor size before and after we administer the drug, imperfect though that may be.

\sphinxAtStartPar
So, does that mean we’re doomed? Yes and no. Yes, it \sphinxstyleemphasis{does} mean that we’re doomed to never be able to take the exact measurements that make it possible to directly answer a Causal Question. But no, that doesn’t mean we can’t do anything — in the coming weeks, we will learn about different strategies for approximating counterfactual conditions, and in each case, we will learn about what \sphinxstyleemphasis{assumptions} must be true for our strategy to provide a valid answer to our Causal Question. By making the assumptions that underlie each empirical strategy explicit, we will then be able to evaluate the plausibility of these assumptions.

\sphinxAtStartPar
In the example of Shikha, for example, we know that our comparison of tumor size before taking the drug to tumor size after taking the drug is only valid if her tumor \sphinxstyleemphasis{would not have gotten smaller without the drug}. This is something we can’t measure directly, but we can look at other patients with similar tumors or the history of her tumor size to evaluate how often we see tumors get smaller at the rate observed after she took the drug. If it’s very rare for these types of tumors to ever get smaller, then we can have more confidence that a decrease in tumor size was the result of the drug.

\sphinxAtStartPar
We are also sometimes in a position to be more proactive than our effort to answer Causal Questions. Rather than trying to make inferences from the world around us using what is termed “observational data” (data that was generated through a process we did not directly control, a process we only “observe”), we can sometimes generate our own data through randomized experiments.

\sphinxAtStartPar
Randomized experiments — perhaps the most familiar tool for answering Causal Questions — are also just another way of approximating the unobservable counterfactual condition. In a randomized experiment (referred to as Randomized Control Trials (RCTs) or “A/B Tests”), participants are assigned to either receive the treatment (the treatment group) or not (the control group) based on the flip of a coin a roll of a die, or \sphinxhyphen{} more commonly \sphinxhyphen{} a random number generator on a computer. Provided we have enough participants, the Law of Large Numbers then promises that \sphinxstyleemphasis{on average}, the people assigned to the control group will (probably) be “just like” the people assigned to the treatment group in every possible way (save being treated). Simply put, this means that the outcomes of the control group — being just like the treatment group \sphinxstyleemphasis{on average} — will be a good approximation of what \sphinxstyleemphasis{would} have happened to the treatment group in a world where they did not receive the treatment.

\sphinxAtStartPar
Randomized experiments are not a silver bullet, however. The validity of experimental comparisons still rests on a number of assumptions, many of which cannot be directly tested. For example, we can never be entirely sure that when we randomly assigned people to control and treatment groups, the process was truly random or that we ended up with people who were similar in both groups (the law of large numbers only promises that getting similar groups becomes \sphinxstyleemphasis{more likely} as the size of the groups increases, not that it will happen with certainty!). Moreover, conducting a randomized experiment requires working in a context where the researcher can control everything, and that can sometimes generate results that may not generalize to the big, messy world where you actually want to act.


\subsection{So where does that leave us?}
\label{\detokenize{30_questions/40_answering_causal_questions:so-where-does-that-leave-us}}
\sphinxAtStartPar
For many data scientists, this will feel \sphinxstyleemphasis{profoundly} dissatisfying. Many people come to data science because of the promise that it will provide direct answers to questions about the world using statistics. But because of the Fundamental Problem of Causal Inference, this will never be possible when answering Causal Questions. Rather, the job of a data scientist answering Causal Questions is a lot like that of a detective trying to solve a crime — your task is to determine what \sphinxstyleemphasis{probably} happened at a crime scene. You can gather clues, collect forensic evidence, and interview suspects, all to come up with the \sphinxstyleemphasis{most likely} explanation for a crime. But no matter how hard you try, you can’t go back in time to witness the crime itself, so you will never be able to be entirely sure if you are right or not.

\sphinxAtStartPar
But just as we investigate and prosecute crimes despite our inability to ever be 100\% certain an arrested suspect is guilty, so too must businesses and governments make decisions using the best available evidence, even when that evidence is imperfect. But it is our job, as data scientists, to help provide our stakeholders with the best available evidence and help them understand the strength of the evidence we are able to provide.


\subsection{Why Passive Prediction Is Not Enough}
\label{\detokenize{30_questions/40_answering_causal_questions:why-passive-prediction-is-not-enough}}
\sphinxAtStartPar
At this point, it is worth pausing to reflect on a question it may not have occurred to you to ask above — if answering Causal Questions is usually about \sphinxstyleemphasis{predicting} what would happen if we were to act on the world in a certain way, then how/why is it different from answering the kind of Passive\sphinxhyphen{}Prediction Questions we discussed previously?

\sphinxAtStartPar
There are a number of different ways one can frame the answer to this question. Still, the one I like most for Data Scientists is that when answering a Passive\sphinxhyphen{}Predictive Question, we can usually achieve our goals simply by identifying \sphinxstyleemphasis{correlations} that we think are likely to persist into the future. For example, suppose we run the maintenance department for a rental car company. The fact that a car whose \sphinxstyleemphasis{Check Engine} light is on is a car that is likely to break down if it isn’t taken to a mechanic is enough for us to identify cars in trouble! Obviously, the \sphinxstyleemphasis{Check Engine} light isn’t \sphinxstyleemphasis{causing} the cars to break down, but it doesn’t have be useful.

\sphinxAtStartPar
But when seeking to answer Causal Questions, we wish to go beyond just identifying cars in trouble and instead predict what might happen to cars if we \sphinxstyleemphasis{chose to act} in different ways. This requires going beyond simple Passive Prediction because, in choosing to act, we ask how things might turn out in a world where we are behaving differently than we are currently — in other words, we are no longer being passive.

\sphinxAtStartPar
Thus, in a sense, answering Causal Questions is \sphinxstyleemphasis{always} an example of “out\sphinxhyphen{}of\sphinxhyphen{}sample extrapolation” or “out\sphinxhyphen{}of\sphinxhyphen{}sample prediction” because, by definition, we are saying we want to know what happens in a world where at least one major agent — us! — changes their behavior. Indeed, there’s a very real sense that that’s what we \sphinxstyleemphasis{mean} by a causal relationship: a relationship between our actions and an outcome that would persist even if we change our behavior!

\sphinxAtStartPar
What’s an example of a situation where a correlation is sufficient for Passive Prediction but not answering a Causal Question? Well, let’s go back to our example of the rental car maintenance manager — suppose rather than using \sphinxstyleemphasis{Check Engine} lights to identify cars that needed more attention, the manager decided to just cut the cables that run to all the \sphinxstyleemphasis{Check Engine} lights! After all, the cars that are breaking down all have their \sphinxstyleemphasis{Check Engine} light on, and the cars that don’t have their \sphinxstyleemphasis{Check Engine} lights almost never break down! So why not just disable the \sphinxstyleemphasis{Check Engine} lights for all these cars so they stop breaking down?

\sphinxAtStartPar
Now that we’ve been clear about what we mean when we ask, “Does X cause Y?,” we can now understand why this is a perfect example of why correlation does not always imply causation.

\sphinxAtStartPar
Fundamentally, the manager is asking, “Would cutting the cables to the \sphinxstyleemphasis{Check Engine} lights prevent our cars from breaking down?” For that to be true, we know that in an ideal universe, we would want to compare a car on the verge of breaking down that has its \sphinxstyleemphasis{Check Engine} light intact to that same car in a world where we cut the \sphinxstyleemphasis{Check Engine} light — then we can see if there is a difference in whether these cars break down!

\sphinxAtStartPar
But this is \sphinxstyleemphasis{not} the data our manager has turned to draw their conclusion — rather, they are comparing cars with their \sphinxstyleemphasis{Check Engine} lights on and cars without their \sphinxstyleemphasis{Check Engine} lights on. It turns out that cars \sphinxstyleemphasis{without} their \sphinxstyleemphasis{Check Engine} lights on are not a good approximation for the cars \sphinxstyleemphasis{with} their \sphinxstyleemphasis{Check Engine} lights on because the cars without the lights on are different from the cars with the lights on in ways that matter for the likelihood of breaking down (they have engine problems!) \sphinxstyleemphasis{other} than the \sphinxstyleemphasis{Check Engine} light!

\sphinxAtStartPar
Depending on what classes you may have taken in the past, you may have heard these differences referred to as “confounders” or “omitted variables” — those are just different words or ways of talking about the same idea! Confounders or omitted variables are just different words for features that are different between the “treated” and “untreated” observations being examined that the untreated observations are bad approximations of the counter\sphinxhyphen{}factual condition for the treated observations!


\subsection{Next Steps}
\label{\detokenize{30_questions/40_answering_causal_questions:next-steps}}
\sphinxAtStartPar
In this reading, we learned — in an intuitive sense — why answering Causal Questions is inherently hard. But this explanation, while accurate, is a little informal to be rigorous. In the readings that follow, we will be introduced to the \sphinxstyleemphasis{Potential Outcomes Framework} — the formal statistical framework that underlies the Neyman\sphinxhyphen{}Rubin Counterfactual Model of Causality. This framework will help us reason more systematically about how and when methods like randomized experiments, linear regression, matching, and differences\sphinxhyphen{}in\sphinxhyphen{}differences can help us answer Causal Questions.

\sphinxAtStartPar
But first, in the interest of not losing perspective on the forest for the trees, a discussion of \sphinxstyleemphasis{how} Causal Questions are used in practice.


\bigskip\hrule\bigskip


\sphinxstepscope


\section{How to Match}
\label{\detokenize{35_causal/90_matching_how:how-to-match}}\label{\detokenize{35_causal/90_matching_how::doc}}
\sphinxAtStartPar
In this reading, I’ll give a high level summary of how matching works before referring to a youtube lesson a the nitty gritty of a few specific implementations.


\subsection{Pruning Your Data}
\label{\detokenize{35_causal/90_matching_how:pruning-your-data}}
\sphinxAtStartPar
As noted in our last reading (this reading is a follow\sphinxhyphen{}on to \DUrole{std,std-doc}{The Why of Matching}, so if you haven’t read that start there), matching could be more appropriately called “pruning”, as the goal is to winnow down your dataset until you have a set of observations for which your control and treatment variables look very similar in terms of observable characteristics. So how do we do that?

\sphinxAtStartPar
A simple matching algorithm would proceed like this:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Loop over all your treated observations.

\item {} 
\sphinxAtStartPar
For each treated observation, look for the most similar untreated observation (not already in a pair) in terms of your control variables.

\item {} 
\sphinxAtStartPar
If that untreated observation is too dissimilar to the treated observation, throw away the treated observation. (As the user you have to pick a threshold for how dissimilar is ok.)

\item {} 
\sphinxAtStartPar
If not, call them a pair and keep both.

\item {} 
\sphinxAtStartPar
When you’ve finished looping over your treated observations, throw away any unpaired untreated observations.

\end{enumerate}

\sphinxAtStartPar
When you’re done, you’ll have a collected of pairs of observations (one treated, one untreated), where both members of each pair are very similar in terms of their observable control variables. All other data has been thrown away.

\sphinxAtStartPar
To illustrate with the example from the last reading, if we started with this data:
\sphinxincludegraphics{{matching_king_1}.png}

\sphinxAtStartPar
A simple matching algorithm would probably prune it down to something like this:

\sphinxAtStartPar
\sphinxincludegraphics{{matching_king_4}.png}

\sphinxAtStartPar
Then, and here’s the cool part, you take this dataset and analyze just the way you would otherwise! Just run your regression on this dataset!


\subsubsection{Measuring Similarity}
\label{\detokenize{35_causal/90_matching_how:measuring-similarity}}
\sphinxAtStartPar
The biggest decision you have to make when doing matching is deciding how you want to measure whether two observations are “similar”. The most simple, commonly used strategy is to measure the dissimilarity of two observations in a pair by:
\begin{itemize}
\item {} 
\sphinxAtStartPar
For each control variable, calculate the difference between the two observations in the pair (so if one has Education of 20 and one of 14, you’d get 6). Note the example above just has Education, but in reality you likely have dozens of these variables, so you do this for each variable.

\item {} 
\sphinxAtStartPar
Normalize those differences by the standard deviation of the variable (so divide 6 by the standard deviation of Education)

\item {} 
\sphinxAtStartPar
Square all those differences, add them up, and take the square root. That’s your “similarity” score.

\end{itemize}

\sphinxAtStartPar
Basically, this is like taking the euclidean distance between the points in some really high dimensional space, except you also normalize distances by their standard deviation, a strategy called “Mahalanobis Distance Matching”.

\sphinxAtStartPar
Of course it’s not the only strategy – the video linked at the bottom of this reading will direct you to a talk on three very good strategies, as well as their strengths and weaknesses – but that’s generally the idea.


\subsection{When Can / Should I Use Matching?}
\label{\detokenize{35_causal/90_matching_how:when-can-should-i-use-matching}}
\sphinxAtStartPar
Matching is best used in a somewhat odd situation: a place where you have some overlap in what your treated and untreated observations look like (called having “common support”),  but where you also have some areas where they \sphinxstyleemphasis{don’t} overlap (imbalance).

\sphinxAtStartPar
The first is necessary because when you prune your data, the goal is to keep only observations that look similar, so you need \sphinxstyleemphasis{some} area of overlap, or you won’t have anything to match!

\sphinxAtStartPar
At the same time, however, if there’s \sphinxstyleemphasis{no} imbalance, then you don’t really need to do matching.

\sphinxAtStartPar
So when should you use it? When your distributions have \sphinxstylestrong{both} areas of overlap and areas of imbalance.


\subsection{Checking Balance}
\label{\detokenize{35_causal/90_matching_how:checking-balance}}
\sphinxAtStartPar
There’s a balance you have to strike when matching: the more strict you are about the maximium dissimilarity you’re willing to include before you throw out a pair of observations, the more balanced your final dataset will be, but the smaller your dataset will be do.

\sphinxAtStartPar
Right? If you reject any pairs that aren’t almost exactly identical, you’ll end up with less data, but what’s left will be more balanced.

\sphinxAtStartPar
So for your application, you have to decide on whether it’s better to have the statistical power of more observations, or the better balance from fewer.


\subsection{Analyze!}
\label{\detokenize{35_causal/90_matching_how:analyze}}
\sphinxAtStartPar
Now the best part of matching: now you just do do what you would have done normally.

\sphinxAtStartPar
In other words, you can think of this as a kind of “pre\sphinxhyphen{}processing step”, and now you can carry forward by feeding this into a regression just the way you would with the original data.


\subsection{Specific Models}
\label{\detokenize{35_causal/90_matching_how:specific-models}}
\sphinxAtStartPar
OK, for the details of a few common models, please go watch this great video by Gary King – you can probably start 15 minuntes in, and should watch till at least 45 minutes, though what follows is also really interesting!

\sphinxAtStartPar
\sphinxhref{https://youtu.be/tvMyjDi4dyg?t=910}{Gary King on Matching}

\sphinxstepscope


\part{Data Science in Practice}

\sphinxstepscope


\chapter{How to Read (Academic Edition)}
\label{\detokenize{40_in_practice/00_how_to_read_this_book:how-to-read-academic-edition}}\label{\detokenize{40_in_practice/00_how_to_read_this_book::doc}}
\sphinxAtStartPar
In this class, you will be asked to do significantly more reading than many of you — especially those of you from engineering backgrounds — are used to. Moreover, many of our reading may feel different from a lot of the reading that you’ve done in the past, and so I want to take a moment to discuss \sphinxstyleemphasis{how} you should approach reading in this — and indeed in \sphinxstyleemphasis{any} — reading intensive course.

\sphinxAtStartPar
Knowing that many students in this class speak English is a second language (and so may find reading more time\sphinxhyphen{}consuming), and many of you also don’t have experience with more reading intensive courses, I have worked very hard to ensure that the readings I assign communicate key concepts as efficiently as possible. But as we will see, learning to answer causal questions well requires wrestling with the complexities that arise when apparently simple math meets the real world, and that unavoidably requires thoughtful exposition.


\section{Read Actively}
\label{\detokenize{40_in_practice/00_how_to_read_this_book:read-actively}}
\sphinxAtStartPar
The first major piece of advice I can offer is that you should always be active when reading for this course. That may mean taking notes on a separate piece of paper as you go, or highlighting passages that stand out and adding comments in the margins. But reading of the type you will encounter in this course should never be a \sphinxstyleemphasis{passive} process.

\sphinxAtStartPar
This is especially true any time you encounter mathematical notation. A key skill in answering causal questions is the ability to map concepts represented in mathematical notation onto facets of real world examples. With that in mind, any time you are reading something written in mathematical notation, it is good practice to think of a specific example and see if you can relate each term you encounter to the real world example.


\section{Be Patient with Examples From Different Domains}
\label{\detokenize{40_in_practice/00_how_to_read_this_book:be-patient-with-examples-from-different-domains}}
\sphinxAtStartPar
Data science is an extremely diverse field. This course does its best to embrace that diversity though the use of examples from a wide range of substantive domains. This, at times, causes frustration among students as many examples will necessarily come from domains that don’t feel relevant to your particular interests. Try and resist this urge — while this may \sphinxstyleemphasis{seem} like a problem, it’s actually emblematic of a huge opportunity for intellectual arbitrage (the porting of insights that have been richly developed in one domain to a different domain where they are unfamiliar)!

\sphinxAtStartPar
This will be particularly relevant when we get to the study of causal inference (the study of how to answer Causal Questions), as there are lots of concepts that have been well\sphinxhyphen{}developed in the social sciences that people are only now starting to apply in industry, meaning many of the best texts and examples will be public policy or social science oriented.

\sphinxAtStartPar
And while the downside of that is that there aren’t as many great books written about causal inference in industry as you may wish, the upside is that there are lots of opportunities to for young data scientists to innovate by applying these concepts in new ways!

\sphinxAtStartPar
So please bear with these examples, and practice trying to apply the concepts you read to an industry example that matters to you.


\section{Do NOT Summarize with LLMs}
\label{\detokenize{40_in_practice/00_how_to_read_this_book:do-not-summarize-with-llms}}
\sphinxAtStartPar
I fully recognize that there is a strong temptation when faced with a long reading to stuff it into a Large Language Model and ask for a summary. \sphinxstylestrong{Don’t.}

\sphinxAtStartPar
There are a few reasons for this. The first is that while an LLM can provide you with a broad summary of what you’re reading, it will necessarily have to exclude all the nuance in the original reading. If you just want to figure out if a reading is generally relevant to your interests, that’s fine; but you’re here to learn a subject — one that can can be infuriatingly subtle — and cutting out all those nuances will limit what you can learn \sphinxstyleemphasis{and} prevent you from being able to test your understanding of concepts by wrestling to understand how each new sentence relates to what you’ve read previously. So just as the goal of the readings isn’t to allow you to answer our reading reflection questions (those are just there to draw your attention to especially salient points), nor is the goal of the readings to understanding it at the level of a summary.

\sphinxAtStartPar
The second big reason is that the process of summarizing material yourself is critical to consolidating your learning. This insight comes, in part, from research on whether students taking notes on computers learn more effectively than students taking notes on paper. This research has found that students taking notes on computers can write much more quickly than students taking notes by hand, but that counter\sphinxhyphen{}intuitively this seems to result in \sphinxstyleemphasis{worse} learning outcomes (based on subsequent learning assessments). Why? It appears that students taking notes on computers are effectively able to \sphinxstyleemphasis{transcribe} everything happening in class, while students taking notes on paper have to think about the material in real time in order to summarize it enough that they can keep up taking notes.

\sphinxAtStartPar
Letting LLMs summarize material for you seems likely to cause a similar issue — by allowing an algorithm to organize the information in a more concise manner, it deprives \sphinxstyleemphasis{you} of the opportunity to engage with the material to create your own summary, a process that forces you to \sphinxstyleemphasis{actively} think about the connections between concepts. The importance of this type of \sphinxstyleemphasis{active learning} is one of the biggest bedrock findings of research on learning in recent years — students who have material given to them (e.g., through a passive lecture) often think they understand material, but it is the students who learn material actively — through class or group exercises, problem sets, or other activities — who perform better on learning assessments.

\sphinxAtStartPar
Finally, learning to focus on a reading for a prolonged period of time is an important skill, and one we practice less and less in the modern age. Sometimes letting our attention flitting from thing to thing is fine, but being able to focus for prolonged periods when required is an important skill to cultivate! (If you’re interested, you can find a really interesting discussion of \sphinxhref{https://www.nytimes.com/2022/11/22/opinion/ezra-klein-podcast-maryanne-wolf.html}{this idea here.})

\sphinxstepscope


\chapter{Writing to Stakeholders}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:writing-to-stakeholders}}\label{\detokenize{40_in_practice/25_writing_to_stakeholders::doc}}
\sphinxAtStartPar
In the spirit of what is to follow, I will begin with the most important idea in this reading:
\begin{quote}

\sphinxAtStartPar
The key to effective writing to stakeholders is to continually ask yourself — where ever you are in your document, whatever your thinking of writing, and whatever you’re debating including — if my stakeholder stopped reading my report \sphinxstyleemphasis{right at this spot,} have I told them everything I want \sphinxstyleemphasis{them} to have learned.
\end{quote}


\section{Why Learning to Write to Stakeholders is Hard}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:why-learning-to-write-to-stakeholders-is-hard}}
\sphinxAtStartPar
As students (especially data science students), the way most of us were taught to write reports is to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
start with an introduction that helps explain the broader context in which the report is being positioned,

\item {} 
\sphinxAtStartPar
describes the data that we intend to use,

\item {} 
\sphinxAtStartPar
describe that how we have wrangled and cleaned that data,

\item {} 
\sphinxAtStartPar
describe how we plan to model that data,

\item {} 
\sphinxAtStartPar
report the results,

\item {} 
\sphinxAtStartPar
and discuss limitations and tack on a boilerplate conclusion.

\end{itemize}

\sphinxAtStartPar
This structure makes a lot of sense in the context of a class because the order of presentation mirrors the objectives of the assignment: demonstrate that you understand the substantive topics that are being taught, demonstrate that you are being thoughtful about the data that you are collecting and that you have internalized the emphasis your instructors have placed on the importance of data cleaning, and that you understand the principles of data modeling. Results come last because in the context of a class, your results don’t actually matter. No professor is going to make a major business decision on the basis of a student report, and no government is going set policy on the basis of what you say.

\sphinxAtStartPar
This structure — which almost entirely front loads material that is not particularly interesting to the instructor — is also viable because it is basically the \sphinxstyleemphasis{job} of the reader (your instructor or teaching assistant) to read everything you wrote.


\section{Putting Yourself in the Stakeholder Shoes}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:putting-yourself-in-the-stakeholder-shoes}}
\sphinxAtStartPar
\sphinxstylestrong{None of that} is true in the real world. In fact, I would argue that being taught to write reports in this manner is about the worst possible preparation one could give students for learning to communicate to real\sphinxhyphen{}world stakeholders for two key reasons:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
In the real world, the \sphinxstyleemphasis{only} thing that your stakeholder actually cares about are the conclusions you have reached about how to solve their problem (i.e. your results and how they relate to the stakeholder’s problem).

\item {} 
\sphinxAtStartPar
There is nothing more scarce in any organization than a decision makers time, and so \sphinxstyleemphasis{the moment} it stops being obvious to the decision maker why the material their reading is directly relevant to solving their problem, there is a very high probability that they will let their attention shift to one of the other hundred critical issues vying for their attention.

\end{enumerate}

\sphinxAtStartPar
Because of these two facts, when writing to a stakeholder you should always — \sphinxstyleemphasis{always} — be asking yourself two questions:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
If my stakeholder stopped reading right now, have I already told them the things that I think it is most important they walk away knowing? and

\item {} 
\sphinxAtStartPar
At every transition — between sections, between topics, and even between paragraphs — is it explicitly clear to the reader that what follows is relevant to solving the stakeholder’s problem so they have and affirmative reason to not get distracted?

\end{enumerate}

\sphinxAtStartPar
What does this look like in practice? It depends a little bit on how much of your stakeholder’s attention you think you can get in the best of circumstances (intelligence briefings were written very differently for Donald Trump than for Barack Obama), but the structure I am going to advocate for is roughly the following:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Executive Summary:} A full summary of absolutely everything you want your stakeholder to walk away knowing if they were to read nothing else in about two to four paragraphs. State the problem that you are setting out to address, state the question that you’re going to answer in order to help solve that problem, and at a very high level state how you are going to answer that question, state the answer you have found, reiterate how that result helps solve the problem.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Context:} With the Executive Summary out of the way, you have hopefully piqued your stakeholder’s interest just enough to double back and provide \sphinxstyleemphasis{a little} more context about the problem you are trying to address and the context and which it is situated. But don’t get complacent — even here, it is important to very clearly and explicitly motivate why all of the information you are providing is relevant to solving the stakeholder’s problem effectively.
\begin{itemize}
\item {} 
\sphinxAtStartPar
This is where the answers you generated to your Exploratory Questions fit — they provide information to your stakeholder about why you have chosen to prioritize certain facets of the problem, and why those choices are correct.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Your Strategy:} Now you get to explain how you are going to answer the question that will help solve the stakeholder’s problem in more detail. But don’t get lost in the weeds — the stakeholder only needs to understand enough about your strategy (including your data) to be able to evaluate how much confidence they should have in our results and conclusions.
\begin{itemize}
\item {} 
\sphinxAtStartPar
This is perhaps \sphinxstylestrong{the} hardest section for data science students to write, not because they can’t think of what to put here, but because they want to include \sphinxstyleemphasis{way, way too much}. There is a very natural and very human tendency when you finish a project that you have put a tremendous amount of work into to talk about all of the work you did. And most of that work will involve wrestling with data, dealing with merging issues, tuning models, etc. But almost nothing you have done in this domain is something that your stakeholder needs to know about. You can write it all up — there may very well be some people on the stakeholder’s staff who will want to read it — but it belongs at the back of the report in an appendix, \sphinxstyleemphasis{not} in the body of the report.

\item {} 
\sphinxAtStartPar
The \sphinxstylestrong{only} exceptions to this rule are places where you exercised substantial \sphinxstyleemphasis{discretion} in how you handled the data or constructed your sample. If you exercised discretion in a substantial way \sphinxstyleemphasis{that may impact how one interprets the conclusions of the report} — for example, you had to choose which states to include in an analysis as control states — you can/should include a \sphinxstyleemphasis{brief} explanation for your reasoning along with a link to an appendix were you discuss those choices in detail.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Your Results:} Now you get to present your results in greater detail. Note that nothing here should come as a surprise to the reader — don’t try and “hide the ball” to build suspense by not revealing your findings till now because while you may \sphinxstyleemphasis{think} what you’ve done is so interesting that the suspense will draw the reader in, in reality, it just means that when your stakeholder doesn’t make it past the Executive Summary, they will not have learned \sphinxstylestrong{the thing} you spent so much time trying to learn.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Also, note that while this structure still has a results section towards the end, because you have kept your Context and Strategy sections very short, it should actually be coming up much sooner than your results would come up in a normal class report (in terms of the page number on which it appears).

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Relating It Back To The Problem:} And now we bring things full circle — now that you presented your results, you have to make sure that your reader hasn’t lost the thread by relating your results back to the problem that motivated your analysis to begin with.

\end{itemize}


\subsection{Wow! What a Great Way of Thinking About This? Did You Invent This Yourself?}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:wow-what-a-great-way-of-thinking-about-this-did-you-invent-this-yourself}}
\sphinxAtStartPar
Obviously not. And now that you are primed to think about how writing can be organized in a manner that reflects the likelihood that most people who pick up a document won’t actually read it all the way to the end, you will start to see how people front load the things they think matter most everywhere.

\sphinxAtStartPar
Journalism is the quintessential example of this style of writing. \sphinxstyleemphasis{No one} reads entire news articles, so they are always nearly organized with the most critical information up front, after which they double back to fill in additional details for anyone still reading. There are a number of different ways this can be accomplished — for example the \sphinxhref{https://en.wikipedia.org/wiki/Inverted\_pyramid\_(journalism)}{inverted pyramid} — start with “who? what? when? where? how?”, then add important details, then add context — is one of the first article formats journalists are introduced to.

\sphinxAtStartPar
But the structure I think you’re likely to see most if you start looking for it is that in the first two or three paragraphs of a well\sphinxhyphen{}written news story, you will notice that there is a single paragraph that is designed to summarize everything that the journalist wants you to know about the story — the “\sphinxhref{https://en.wikipedia.org/wiki/Article\_structure\#Nut\_Graph}{nut graph}”.

\sphinxAtStartPar
Academic publications also usually follow this structure. They start with an abstract (essentially an Executive Summary of the Executive Summary), an “introduction” (which is, in effect, an Executive Summary), a literature review (to establish why what they’re doing is novel and provide some broader context), a not too long discussion of their methodology, their results, and a short conclusion that ties the results back to the motivating question. All the sensitivity analyses, detailed discussion of the data, etc.? Those go into an “Online Only Appendix,” a document that is often easily twice as long as the article itself.


\section{Other Points To Bear In Mind}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:other-points-to-bear-in-mind}}

\subsection{Putting The Pieces Together}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:putting-the-pieces-together}}
\sphinxAtStartPar
In this course, we’ve spent a lot of time establishing \sphinxstyleemphasis{taxonomies} — ways of organizing and recognizing the many distinct things that we do as data scientists. The assignments we have done for this class have often been structured in a manner to emphasize these distinctions. But when writing to a stakeholder, you don’t want to hold too rigidly to those distinctions. A good report to a stakeholder will not consist of stapling together all of the assignments we’ve done in this class one after the other. The goal of a good report is to construct a narrative that consistently and effectively communicates a key idea to the reader.

\sphinxAtStartPar
Many of you will be familiar with the idea of the “five\sphinxhyphen{}paragraph essay.” The five\sphinxhyphen{}paragraph essay is a structure that is often used to introduce students to essay writing — you start with an introductory paragraph, you write three paragraphs, each of which starts with a topic sentence and then includes evidence in support of that topic sentence, and then you write a conclusion paragraph. It is a useful framework for introducing students to essay writing, but it is also a framework that is best left behind as soon as it is understood.

\sphinxAtStartPar
The emphasis the assignments we have done in this class puts on different types of questions is similar — the goal is to make sure you can recognize the different types of questions and the purposes to which we put them. But your stakeholder report should not include a section called \sphinxstyleemphasis{Exploratory Questions} — rather the answers you generated in answering your Exploratory Questions should be reflected in how you’ve refined your problem statement, and the results of some of those analyses will likely make an appearance as tools for motivating the focus of your final analysis.


\subsection{Justify Your Methods With Concrete Examples}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:justify-your-methods-with-concrete-examples}}
\sphinxAtStartPar
In discussing the methods you choose to use — such as a difference\sphinxhyphen{}in\sphinxhyphen{}difference design or matching — be sure to explain \sphinxstyleemphasis{why} you’re using the method in concrete terms that are relevant to the context in question, and without using technical language your stakeholder may not understand.

\sphinxAtStartPar
If you’re doing a difference\sphinxhyphen{}in\sphinxhyphen{}difference design (and not just a pre\sphinxhyphen{}post comparison), give examples of specific events that would cause problems for a simple pre\sphinxhyphen{}post analysis but that aren’t a problem for the difference\sphinxhyphen{}in\sphinxhyphen{}difference design you are using. Consider the opioid project from IDS 720. In that project, we estimated the effect of state\sphinxhyphen{}level changes in opioid prescription regulations in states like Florida and Texas on opioid shipments and overdoses. For that project, it was important to give an example — like the US federal government creating a national limit on the amount of opioids manufacturers were allowed to ship to individual clinics around the same time — that would cause incorrect inferences to be drawn from a simple pre\sphinxhyphen{}post comparison, but not from our design of choice (a difference\sphinxhyphen{}in\sphinxhyphen{}difference).

\sphinxAtStartPar
If you don’t include an example like this, it won’t be clear to the stakeholder why you feel fancy techniques are necessary. And if you can’t think of an example like this, then maybe you don’t \sphinxstyleemphasis{need} the fancy technique at all!


\subsection{Data Cleaning}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:data-cleaning}}
\sphinxAtStartPar
As noted above, a discussion of data cleaning and data wrangling does not belong in the body of your report. Data cleaning is where you probably spent \sphinxstyleemphasis{most} of your actual time, so it’s natural to want to demonstrate how much effort you put into a project by detailing all of the painstaking merging, string cleaning, harmonizing, etc. work you put into your project. All of that can go into an appendix, but including it in a report only makes it more likely that all of that effort will go to waste because reading about it will bore your stakeholder into setting down your report before you want them to.

\sphinxAtStartPar
There’s a famous saying in writing \sphinxhref{https://slate.com/culture/2013/10/kill-your-darlings-writing-advice-what-writer-really-said-to-murder-your-babies.html}{that appears to have first appeared in a 1913 Cambridge Lecture by Arthur Quiller\sphinxhyphen{}Couch}:
\begin{quote}

\sphinxAtStartPar
If you here require a practical rule of me, I will present you with this: ‘Whenever you feel an impulse to perpetrate a piece of exceptionally fine writing, obey it — whole\sphinxhyphen{}heartedly — and delete it before sending your manuscript to press. \sphinxstylestrong{Murder your darlings.}’
\end{quote}

\sphinxAtStartPar
Well, when it comes to data science, you don’t have to murder those darling paragraphs about all the messy data you had to clean up, but you \sphinxstyleemphasis{should} shove them in an appendix.

\sphinxAtStartPar
Oh, and please \sphinxstyleemphasis{never} use verbatim variable names in reports — they’re fine in appendices, but in the body of a report you should explain what you’re measuring in substantive terms someone without the data documentation can understand.


\subsection{Discretion}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:discretion}}
\sphinxAtStartPar
The exception to the “don’t talk about your data manipulations” rule is that when you were forced to exercise substantial discretion in a way that may impact the internal or external validity of the analysis in a meaningful way (for example, selection of control states, or dropping certain time periods from the analysis you feel are too anomalous).

\sphinxAtStartPar
Even in these situations, however, it is usually best to explain the \sphinxstyleemphasis{logic} behind the decisions in the body of the report (like in choosing what entities belong in your control group). However, a full discussion of these kinds of choices belong in an appendix.

\sphinxAtStartPar
When you do exercise substantial discretion in an analysis (for example, deciding to include certain entities as controls while excluding others), \sphinxstylestrong{it’s best practice to include some sensitivity analyses in your report} by examining how your results do (or hopefully do not) change if you exercise discretion in slightly different ways. A sensitivity analysis, as a reminder, is where you make different discretionary choices (like choosing what entities to use as controls, or what variables you include in your regression), then re\sphinxhyphen{}run your analysis to see if the results change. The goal is to show that while, yes, you did have to exercise discretion in your analysis and data manipulations (we always do!), the choices you were forced to make aren’t driving your results and conclusions.

\sphinxAtStartPar
Ideally, you can just reference that “results are similar when {[}example of doing something a little differently{]}” in the body of the report and include the sensitivity analysis in an appendix.

\sphinxAtStartPar
If different discretionary choices \sphinxstyleemphasis{do} lead to substantively different results, then you need to (a) try to characterize how fragile your results are (how much do they change in response to how small of a tweak in your choices), and/or (b) defend why the choices you made were correct, and the choices that give different results are not.


\subsection{Figures}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:figures}}
\sphinxAtStartPar
Not everyone may agree with this approach, but my philosophy with plots is that they should be more or less freestanding — if a reader were to only look at the figures in a report, the labels on the axes, the titles, and any notes associated with the figure, they should be able to get a pretty good understanding what’s going on without having to go read the text in the report.

\sphinxAtStartPar
For example, this plot (which, yes, \sphinxhref{https://www.cambridge.org/core/journals/american-political-science-review/article/enfranchisement-and-incarceration-after-the-1965-voting-rights-act/C68FA7BB8CA313BDD8D9A39BA666A21D}{is from a paper on which I am a co\sphinxhyphen{}author}, but all credit for the figure quality goes to my coauthor):

\sphinxAtStartPar
\sphinxincludegraphics{{njc_diff_in_diff}.png}


\subsection{Limitations}
\label{\detokenize{40_in_practice/25_writing_to_stakeholders:limitations}}
\sphinxAtStartPar
There is a tendency for students to use their “Limitations” sections to, well… just try and cover their butts by throwing out anything they can think of about the paper that is imperfect. That’s ok in the classroom, but it’s not useful in the real world.

\sphinxAtStartPar
The point of a Limitations section isn’t to demonstrate your ability to identify any imperfections in the study; the point of a limitation section is to give your stakeholder a sense of how much confidence they should have in the results presented in the report \sphinxstyleemphasis{from your professional perspective.} Just because you had to make an assumption does not mean that the assumption constitutes a “limitation” of the study unless you have reason to think that the assumption is unlikely to be true (or is sufficiently untrue as to impact the results). Please only include things in your limitation section that you think really are substantive limitations!





\sphinxstepscope


\chapter{Giving Effective Feedback (Data Science Edition)}
\label{\detokenize{40_in_practice/30_giving_feedback:giving-effective-feedback-data-science-edition}}\label{\detokenize{40_in_practice/30_giving_feedback::doc}}
\sphinxAtStartPar
One of the best ways to thrive in the workplace is to help those around you be successful. This can, at times, feel counter\sphinxhyphen{}intuitive — we all have a natural tendency to measure ourselves against those around us, which can result in a feeling of implicit competitiveness with peers. But helping your colleagues be more effective isn’t just a good thing to do from an ethical perspective — it will also make your teams and units more effective, improve your professional reputation, make people eager to involve you in new projects, and make colleagues more likely to reciprocate with your generosity.

\sphinxAtStartPar
Providing detailed, constructive feedback is one of the best ways to help colleagues. Yet it’s a skill we rarely teach. In this reading, I provide a framework for giving feedback that’s specific to empirical data science. As with everything in this book, you may eventually decide this approach to feedback is not for you, and that you already have a style of feedback you enjoy. But give it a try and if nothing else, it will give you a foil to help you understand what you \sphinxstyleemphasis{don’t} like in feedback!


\section{Levels of Feedback}
\label{\detokenize{40_in_practice/30_giving_feedback:levels-of-feedback}}
\sphinxAtStartPar
The key to effective feedback on data science projects, in my view, is to recognize the multiple levels on which all data science projects operate (and where they can fall apart):
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Approach \sphinxhyphen{} Problem fit:} given the problem that motivates the project, will the question the author is seeking to answer actually help address the problem?

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Does the project have a good strategy for answering the question?}: Is the strategy being employed a good one for answering the question they have set out to answer (at least in theory)?

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Was the strategy executed successfully/Do you believe the answer generated?}: Basically, do you believe the answer generated by the analysis? Are there issues with the result, data, or diagnostics that give you pause?

\end{itemize}

\sphinxAtStartPar
A project can have issues at any (or all) of these three levels, and each step of this ladder from motivation to estimated answer is a place where feedback can be provided.

\sphinxAtStartPar
Richard McElreath, author of the amazing book \sphinxhref{https://xcelab.net/rm/}{Statistical Rethinking}, argues you can divide statistical analyses into a similar ladder of abstraction (it’s a touch different from mine, but similar). He refers to the levels of an analysis as:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{The Estimand}: What are you trying to measure in substantive — not statistical — terms. For example, “gender wage discrimination” or “the effect of opioids on neurologic pain.”

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{The Estimator}: The specific model and specification you’ve chosen to use to measure that quantity, like linear regression or logistic regression.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{The Estimate}: the actual number that you get from the model.

\end{itemize}

\sphinxAtStartPar
Delightfully, he also uses \sphinxhref{https://youtu.be/mBEA7PKDmiY?si=pykJpxpC9uBJVQKM\&amp;t=206}{this figure} to emphasize the inevitable… challenges that emerge at each transition from theory to empirical reality:

\sphinxAtStartPar
\sphinxincludegraphics{{estimand_estimator_estimate}.png}

\sphinxAtStartPar
Students tend to feel most comfortable at the lowest of these levels of abstraction — data science classes tend to emphasize factor engineering and model diagnostics, so students have the most practice raising concrete concerns about model fitting and implementation. Yet that is also the domain where most other data scientists are likely to feel most comfortable, and so while feedback at this level is valuable, learning to reflect more on project motivation and how the project fits with the motivating problem often gives one a comparative advantage in providing useful feedback.


\section{Ask What Kind of Feedback Your Colleague is Seeking}
\label{\detokenize{40_in_practice/30_giving_feedback:ask-what-kind-of-feedback-your-colleague-is-seeking}}
\sphinxAtStartPar
Before anything else, be sure to ask your colleague what type of feedback they are looking for. There are times that people are looking for any and all feedback on their project. More often than not, though, your colleagues will be particularly interested in your thoughts on certain aspects of their work, and will already know that some parts of a project need improvement (and so won’t want you spending time telling them about issues they are already aware of).

\sphinxAtStartPar
It’s also important to bear in mind what changes are feasible. If someone comes to you to practice a presentation they are giving to their boss in two days, there’s nothing to be gained by telling them they could have approached the whole project a different way and it would have turned out better. All that type of feedback would accomplish is to wreck their confidence on the eve of a big presentation.


\section{Providing Written Feedback}
\label{\detokenize{40_in_practice/30_giving_feedback:providing-written-feedback}}
\sphinxAtStartPar
What follows is how I approach providing written feedback. I generally prefer to provide my comments in written form, and I think writing is the best way to force yourself to be precise in your thinking, allows for iteration and revision, and it ensures your colleague can revisit and reflect on the comments you have invested time in providing.

\sphinxAtStartPar
If you are providing verbal feedback, you may need to modify some of the details of what follows, but I would still suggest following a model \sphinxstyleemphasis{like} this.


\subsection{1. Start with a Summarization}
\label{\detokenize{40_in_practice/30_giving_feedback:start-with-a-summarization}}
\sphinxAtStartPar
Before you offer suggestions, begin by laying out how you understand the project. Using this framework, lay out what you understand to be:
\begin{itemize}
\item {} 
\sphinxAtStartPar
the problem motivating the project (“This project aims to address {[}problem{]}…”),

\item {} 
\sphinxAtStartPar
how the project aspires to address this problem (“In order to address this problem, the seek to answer the question {[}question authors intend to answer{]}…”),

\item {} 
\sphinxAtStartPar
how they attempt to do so (“Using data from {[}such and such{]}, the authors set out to answer this question using {[}modelling strategy{]}),

\item {} 
\sphinxAtStartPar
what they find \sphinxstyleemphasis{expressed in concrete terms} (“They find that the partial correlation between X and Y is Z.”)

\item {} 
\sphinxAtStartPar
how they interpret what they find (“They interpret this as evidence that the answer to their question is that {[}something something{]}”),

\item {} 
\sphinxAtStartPar
how they relate it back to the problem (“Given that, they argue {[}something about their motivating problem{]}…”)

\end{itemize}

\sphinxAtStartPar
At each stage, try not to fill in any gaps in the logic of your colleague’s arguments — the goal here is to make sure, concretely, you see the project precisely for all its parts.

\sphinxAtStartPar
This exercise is useful for two reasons. First, it helps \sphinxstyleemphasis{you} think through the project in very concrete terms. Humans are exceptionally good at seeing patterns in the world even when they don’t exist, and when reading your brain will naturally try to gloss over inconsistencies in the logic of what you are reading. Decomposing a project into granular components in this manner helps counteract that tendency, bringing attention to logical fallacies or gaps you might otherwise overlook.

\sphinxAtStartPar
Second, summarizing how you understand a project helps the project authors understand what they have communicated to the reader and the basis for the feedback you provide. Moreover, it makes it clear to the recipient of feedback that you gave their project your full attention, which is likely to make them more receptive to critiques.


\subsection{2. Offer Higher\sphinxhyphen{}Level Thoughts On The Analysis}
\label{\detokenize{40_in_practice/30_giving_feedback:offer-higher-level-thoughts-on-the-analysis}}
\sphinxAtStartPar
It’s easy to get lost in the weeds — “fix that figure caption,” “did you try logging that variable?,” or “what about using {[}trendy new model{]}?”

\sphinxAtStartPar
But before you get into more detailed feedback, I find it helpful to provide feedback on the overall strategy of the analysis. In general, this amounts to thinking about whether the different levels of an analysis link up. For example, do you think answering the question the author sets out to answer will really help address the problem that motivated their analysis? Do you think their empirical strategy can actually answer the question they set out to answer?

\sphinxAtStartPar
When doing so, pay particular attention to the kind of “slippage” that occurs between levels of an analysis — many projects articulate a problem well and have a clear question they want to answer, but on reflect the question the authors seek to answer won’t actually help solve the problem. Similarly, sometimes an analysis will have a good question they want to answer, and a good empirical strategy, but the quantity the empirical strategy would estimate isn’t actually quite the answer to the question they authors want to know.


\subsubsection{Say What You Think The Analysis Did Well}
\label{\detokenize{40_in_practice/30_giving_feedback:say-what-you-think-the-analysis-did-well}}
\sphinxAtStartPar
As you work on this, remember to not just comment on problems — talk about the analysis’ strengths too! It’s easy to focus on the short\sphinxhyphen{}comings of an analysis, but emphasizing the positive not only prevents the recipient from feeling attacked, it also helps them understand the elements of their analysis that they should retain or emphasize.


\subsubsection{Be Constructive}
\label{\detokenize{40_in_practice/30_giving_feedback:be-constructive}}
\sphinxAtStartPar
Whenever you raise a concern, try to also provide some thoughts about how the concern could be addressed. It helps people to know about problems with their analyses before they show them to higher\sphinxhyphen{}ups, don’t get me wrong, but nothing makes people more grateful than identifying a problem \sphinxstyleemphasis{and} offering a solution. Your suggestion doesn’t have to be perfect. Even a so\sphinxhyphen{}so suggestion makes the point you see yourself as part of a collaborative enterprise (you’re working together towards getting it right!) and may inspire new lines of thinking.


\subsubsection{Motivate Your Suggestions}
\label{\detokenize{40_in_practice/30_giving_feedback:motivate-your-suggestions}}
\sphinxAtStartPar
If you provide suggestions, be sure to explicitly motivate those suggestions.

\sphinxAtStartPar
Suppose you’re reading a paper trying to explain the determinants of a salary in which the authors report the bivariate correlation between different variables and salary. You might be tempted to offer a suggestion like “Don’t just present several separate correlations of employee attributes and salary, put all those variables in a single regression.” But that suggestion doesn’t explain \sphinxstyleemphasis{why} they should use a regression.

\sphinxAtStartPar
A better approach is to explicitly explain the \sphinxstyleemphasis{problem} you see with what the authors are currently doing, your suggestion for addressing it, and why you think your suggestion would solve the problem. For example, you might say “It is difficult for the reader to understand the relative importance of different employee attributes to salary when you are only presenting a set of bivariate correlations. If you put all these attributes in a regression, the reader could more easily see the relative magnitudes of the coefficients as well as their statistical significance. Moreover, it would also help the reader understand the independent role of different factors where employee attributes are correlated.”

\sphinxAtStartPar
Motivating your suggestions not only makes it more likely your colleague will take your suggestion on board, but also helps them understand your thought process. This is particularly helpful when your suggestion is something that your colleague understands would be a bad thing to implement.

\sphinxAtStartPar
As writer, when you get a suggestion that doesn’t make sense, it usually means that you failed to communicate something about the goals of your project or the constraints under which you are operating. When this happens, it is your job to ensure future readers are not similarly confused by improving how the report is written.

\sphinxAtStartPar
So as someone giving feedback, the more information you provide about your thought process, the easier it will be for your colleague to understand the source of your confusion and address it.


\subsection{3. Then Get Granular, if Appropriate}
\label{\detokenize{40_in_practice/30_giving_feedback:then-get-granular-if-appropriate}}
\sphinxAtStartPar
Once you’re done with strategic, high\sphinxhyphen{}level reflections, \sphinxstyleemphasis{then} you can turn to the granular. But remember, the goal isn’t to score points — if you’ve just suggested that the analysis needs to go in a fundamentally different direction, nitpicking about feature engineering just comes across as petty and annoying.


\subsubsection{Provide Locations!}
\label{\detokenize{40_in_practice/30_giving_feedback:provide-locations}}
\sphinxAtStartPar
This may seem obvious, but when providing feedback — especially granular feedback that’s more specific to a figure, sentence, or passage, \sphinxstyleemphasis{be sure to state the exact location clearly} in your feedback. The location you have in mind may seem self\sphinxhyphen{}evident \sphinxstyleemphasis{to you}, but I can tell you that in practice it’s rarely the case that it’s as obvious your colleague. This is important to do when writing feedback, but also important when your feedback is verbal, as when giving feedback on a presentation — verbal feedback quickly becomes a little overwhelming for recipients, and being able to take notes on what they are hearing that includes things like slide numbers or slide headings is very helpful when making changes.


\subsection{4. Walk Away and Come Back}
\label{\detokenize{40_in_practice/30_giving_feedback:walk-away-and-come-back}}
\sphinxAtStartPar
Most of us tend to come across as a little… mean and ungenerous in the first draft of feedback we write. So I am personally a big believer in (a) walking away from my feedback after I’ve written it, (b) doing something else for a while, before (c) re\sphinxhyphen{}reading and editing the feedback before sending it. In a perfect world, I try to go to bed between when I finish the first draft of my feedback and when I make revisions.

\sphinxstepscope


\part{Advanced Topics}

\sphinxstepscope


\chapter{Interpretable Models}
\label{\detokenize{40_advanced_topics/30_interpretability:interpretable-models}}\label{\detokenize{40_advanced_topics/30_interpretability::doc}}
\sphinxAtStartPar
For most applications, perform just as well as fancy pants models. \sphinxurl{https://arxiv.org/abs/1811.10154}
Allow for non\sphinxhyphen{}specialists to see what goes into a model to debate ethics (after all, we data scientists have specialized knowledge when it comes to statistical methods, but not ethics)







\renewcommand{\indexname}{Index}
\printindex
\end{document}