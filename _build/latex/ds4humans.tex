%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Class Schedule}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Solving Problems With Data}
\date{Dec 25, 2024}
\release{}
\author{Nick Eubank}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{landing_page::doc}}


\sphinxAtStartPar
This website is both the course site for the Duke Interdisciplinary Data Science Course \sphinxstyleemphasis{Solving Problems with Data} (IDS 701), as well as the beginning of what I hope will eventually become a stand\sphinxhyphen{}alone textbook by \sphinxhref{https://www.nickeubank.com}{Nick Eubank}.

\sphinxAtStartPar
\sphinxstylestrong{If you \sphinxstyleemphasis{aren’t} a Duke IDS 701 student} and wish to explore the content of the course, I’d suggest starting with the {\hyperref[\detokenize{10_introduction/10_solving_problems_with_data::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Introduction chapter here}}}}. You’re also welcome to browse the class schedule of topics covered in the class in the {\hyperref[\detokenize{00_class_schedule/class_schedule::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{class schedule}}}} link.

\sphinxAtStartPar
If you \sphinxstyleemphasis{are} a MIDS student, here’s a little more about the course.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Solving Problems with Data (IDS 701)}
\end{DUlineblock}

\sphinxAtStartPar
Few fields have shown as much promise to address the world’s problems as data science. At the same time, however, recent years have also made clear that today’s global challenges will not be met by simply “throwing data science at the problem” and hoping things will work out. Even in business, where many assume that Artificial Intelligence is a sure ticket to profits, \sphinxhref{https://www.wired.com/story/companies-rushing-use-ai-few-see-payoff/}{a major recent study found only > 11\%} of businesses that had piloted or employed Artificial Intelligence had reaped a sizeable return on their AI investments.

\sphinxAtStartPar
How, then, should a burgeoning data scientist approach this field full of such promise but also so many pitfalls? And why have so many data science endeavors failed to deliver on their promise?

\sphinxAtStartPar
The answer lies — at least in significant part — in a failure to provide students with a systematic approach to bringing the techniques learned in statistical modeling and machine learning courses to bear on real\sphinxhyphen{}world problems. Data science curricula usually begin with coding, statistics, and model evaluation techniques. All too often, however, that’s where they stop. But while the hardest part of data science \sphinxstyleemphasis{classes} is often fitting a model well or getting a good AUC score, the hardest part of being an effective \sphinxstyleemphasis{professional} data scientist is ensuring that the models being fit and the results being interpreted actually solve the problem that motivated you (or your stakeholder) in the first place.

\sphinxAtStartPar
This class is designed to fill this gap. Through exercises, case studies, and projects, students will develop a \sphinxstyleemphasis{systematic} understanding of how to approach and manage data science projects from conception through delivery and adoption. It will provide a unified perspective on how the perspectives and tools learned in other courses complement one another, and \sphinxstyleemphasis{when} different approaches to data science are most appropriate.

\sphinxAtStartPar
In addition, this course will also provide an in\sphinxhyphen{}depth introduction into \sphinxstyleemphasis{causal inference} — the practice of answering causal questions. Given the interests of MIDS students, this introduction will focus heavily on experiments and A/B testing, but will also cover the use of observational data (data that did not come from an experiment that employed random assignment to treatment) for answering causal questions.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large How to Succeed in this Class}
\end{DUlineblock}

\sphinxAtStartPar
In Duke course reviews, students are asked, “What would you like to say about this course to a student who is considering taking this course in the future?”

\sphinxAtStartPar
\sphinxstyleemphasis{By far} the most consistent thing past students say they would like to tell a student considering taking it in the future is to \sphinxstylestrong{do the readings and take them seriously.}

\sphinxAtStartPar
There is a tendency among data science students — especially those from a STEM background — to assume that readings that don’t have a lot of math aren’t “serious,” and consequently don’t require substantial focus. That’s a mistake. This course is about the critical reasoning required to make the leap from the relatively clean math of statistics and machine learning to the messiness of real world problems. To help you learn how to do so, the readings are full of examples, ways to think about problems, and problem\sphinxhyphen{}solving frameworks to help you cross that wobbly bridge from the clean world of problem sets to the real, under\sphinxhyphen{} or mis\sphinxhyphen{}defined problems you will face when you enter the work force. But with this type of material, what you get out of it depends on what you put into it, and unlike with a theorem — which you either follow or you don’t — thinking critically happens on many levels. So while it’s easy to skim a reading and — because you weren’t confused by any greek notation — assume you internalized it, succeeding in this class requires actively wrestling with the material, not just letting your eyes glide over it.

\sphinxAtStartPar
In other words, \sphinxstylestrong{take the readings for this course just as seriously as the exercises.} There is as much learning to be done by thinking deeply about the readings as there is to be gained from doing the exercises, a fact that is also reflected in how the course is graded (individual or two\sphinxhyphen{}person exercises make up 20\% of your grade, while reading comprehension quizzes and the midterm count for 20\% each).

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Big Ideas}
\end{DUlineblock}

\sphinxAtStartPar
This course is organized around three big ideas:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data science is about solving problems.} All too often, data scientists get lost in the technical details of models and lose sight of the bigger picture. Data science is not about maximizing accuracy or AUC scores — it’s about using data and quantitative methods to solve problems, and at the end of the day the only “metric” that matters is whether your work has solved the problem you set out to address.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data scientists solve problems by answering questions, and the question you are asking determines what tool is appropriate.} At their core, all data science tools are tools for answering questions, whether you realize it or not. Learning to recognize how data scientists use questions to solve problems — and exactly what questions are being answered by the tools you use every day — is key to navigating the ambiguity of real\sphinxhyphen{}world problem solving.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reasoning rigorously about uncertainty and errors is what differentiates good data scientists from great data scientists.} Data science isn’t just about minimizing classification errors and uncertainty — it’s also about deciding how unavoidable errors should be distributed, and how to weigh the risks and trade\sphinxhyphen{}offs inherent in probabilistic decision\sphinxhyphen{}making rigorously and in a manner that takes into account the problem you are trying to solve.

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Pre\sphinxhyphen{}Requisites for Non\sphinxhyphen{}MIDS Students}
\end{DUlineblock}

\sphinxAtStartPar
This course is primarily designed for students in the Duke Masters in Interdisciplinary Data Science (MIDS) program, but students from other programs are more than welcome if they have the appropriate pre\sphinxhyphen{}requisite training. Data Science is a fundamentally interdisciplinary field, so the more perspectives we have represented in the classroom the better!

\sphinxAtStartPar
This course will assume that enrolled students have a good grasp of inferential statistics and statistical modeling (e.g. a course in linear models), though no prior experience with causal inference is expected. In addition, MIDS students will be taking a concurrent course in applied machine learning, so incoming students will also be expected to have some basic experience with machine learning or be concurrently enrolled in an applied machine learning course.

\sphinxAtStartPar
This course will also assume students are comfortable manipulating real\sphinxhyphen{}world data in Python. The substantive content of this course is language\sphinxhyphen{}independent, but because students will be required to work on their projects in teams, comfort with Python will be required to facilitate collaboration (MIDS students are, generally, “bilingual” in R and Python, but have a strong preference for Python, and it’s hard to write problem sets to accommodate multiple languages).

\sphinxAtStartPar
Finally, students will also be expected to be comfortable collaborating using git and github. If you meet the other requirements for this course but are not familiar with git and github, this is a skill you should be able to pick up on your own in advance of the course without too much difficulty. You can read more about \sphinxhref{https://www.practicaldatascience.org/ids720\_specific/exercises/Exercise\_git.html}{git and github here}. The \sphinxhref{https://library.duke.edu/data}{Duke Center for Data and Visualization Science} also hosts git and github workshops if you are a Duke student.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Course Schedule}
\end{DUlineblock}

\sphinxAtStartPar
You can find the {\hyperref[\detokenize{00_class_schedule/class_schedule::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{course schedule here}}}}. Note that our schedule is subject to change, but this should give you a good sense of the material we will cover.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Syllabus}
\end{DUlineblock}

\sphinxAtStartPar
To learn more about the course, please \sphinxhref{https://github.com/nickeubank/unifyingdatascience/raw/master/syllabus/Syllabus\_UnifyingDataScience.pdf}{read the course syllabus here.}.

\sphinxstepscope


\part{Class Schedule}

\sphinxstepscope


\chapter{Class Schedule}
\label{\detokenize{00_class_schedule/class_schedule:class-schedule}}\label{\detokenize{00_class_schedule/class_schedule::doc}}
\sphinxAtStartPar
Class: Tuesday / Thursday, 1:25\sphinxhyphen{}2:40pm

\sphinxAtStartPar
Office Hours:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Nick: Thursday, 9:30\sphinxhyphen{}10:30am, \sphinxurl{https://duke.zoom.us/my/nickeubank} or Gross Hall 231

\end{itemize}


\section{Texts Used}
\label{\detokenize{00_class_schedule/class_schedule:texts-used}}
\sphinxAtStartPar
This course will make use of readings from a handful of different sources. The main four, in order of the amount they will be used, are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{DS4H}: \sphinxhref{https://ds4humans.com}{Data Science for Humans} (This website, free.)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Cunningham}: \sphinxhref{https://www.amazon.com/Causal-Inference-Mixtape-Scott-Cunningham/dp/0300251688}{Causal Inference Mixtape} (\$30 new, \$20 used. \sphinxhref{https://ebookcentral.proquest.com/lib/duke/detail.action?docID=6425560}{Online Access through Duke Library})

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{KTX}: \sphinxhref{https://www.amazon.com/gp/product/1108724264/}{Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing} (\$40 new, \$30 used.)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{GHV}: \sphinxhref{https://www.amazon.com/Regression-Stories-Analytical-Methods-Research/dp/1107676517}{Regression and Other Stories} (\$40. Too new to have used versions.)

\end{itemize}


\begin{savenotes}
\sphinxatlongtablestart
\sphinxthistablewithglobalstyle
\makeatletter
  \LTleft \@totalleftmargin plus1fill
  \LTright\dimexpr\columnwidth-\@totalleftmargin-\linewidth\relax plus1fill
\makeatother
\begin{longtable}{\X{4}{51}\X{15}{51}\X{25}{51}\X{7}{51}}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Date
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Topic
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Do Before Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
In Class
\\
\sphinxmidrule
\endfirsthead

\multicolumn{4}{c}{\sphinxnorowcolor
    \makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} \textendash{} continued from previous page}}%
}\\
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Date
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Topic
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Do Before Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
In Class
\\
\sphinxmidrule
\endhead

\sphinxbottomrule
\multicolumn{4}{r}{\sphinxnorowcolor
    \makebox[0pt][r]{\sphinxtablecontinued{continues on next page}}%
}\\
\endfoot

\endlastfoot
\sphinxtableatstartofbodyhook

\sphinxAtStartPar
Th Jan 9
&
\sphinxAtStartPar
Course Overview
&&
\sphinxAtStartPar
acme
\\
\sphinxhline
\sphinxAtStartPar
Tu Jan 14
&
\sphinxAtStartPar
Solving Problems by Answering Questions
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/nickeubank/unifyingdatascience/raw/master/syllabus/Syllabus\_UnifyingDataScience.pdf}{Read, sign, submit syllabus on gradescope.}

\item {} 
\sphinxAtStartPar
\sphinxhref{how\_to\_read.ipynb}{How to Read}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/10\_introduction/10\_our\_approach.html}{Solving Problems}

\end{itemize}
&
\sphinxAtStartPar
generating
\\
\sphinxhline
\sphinxAtStartPar
Th Jan 16
&
\sphinxAtStartPar
Stakeholder Management
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/10\_introduction/30\_solving\_the\_right\_problem.html}{Solving the Right Problem}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/10\_introduction/40\_data\_science\_in\_historical\_context.html}{History of Data Science}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.youtube.com/watch?v=kYMfE9u-lMo}{Why It’s Important To Get Your Question Right (30 min video)}

\end{itemize}
&
\sphinxAtStartPar
stake
\\
\sphinxhline
\sphinxAtStartPar
Tu Jan 21
&
\sphinxAtStartPar
Proscriptive v. Descriptive Questions
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/30\_questions/05\_descriptive\_v\_prescriptive.html}{Descriptive versus Prescriptive Questions}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Th Jan 23
&
\sphinxAtStartPar
Exploratory Questions: Purpose, Internal and External Validity
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/30\_questions/07\_eda.html}{EDA}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/30\_questions/10\_using\_exploratory\_questions.html}{Using Exploratory Questions}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/30\_questions/15\_answering\_exploratory\_questions.html}{Answering Exploratory Questions}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/30\_questions/17\_exploratory\_questions\_internal\_external.html}{Internal v. External Validity}

\end{itemize}
&
\sphinxAtStartPar
exp
\\
\sphinxhline
\sphinxAtStartPar
Tu Jan 28
&
\sphinxAtStartPar
Capstone With Training Wheels Assignment
&&\begin{itemize}
\item {} 
\sphinxAtStartPar
Explor Assign

\item {} 
\sphinxAtStartPar
How to write to stakeholders

\end{itemize}
\\
\sphinxhline
\sphinxAtStartPar
Th Jan 30
&
\sphinxAtStartPar
Teams 1
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/nickeubank/unifyingdatascience/raw/master/team\_readings/project\_aristotle\_nytimes.pdf}{What Project Aristotle Learned}

\item {} 
\sphinxAtStartPar
Edmondson, The Fearless Organization, Chpt 1 (on Canvas)

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Tu Feb 4
&
\sphinxAtStartPar
Teams 2
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/nickeubank/unifyingdatascience/raw/master/team\_readings/MIDS\%20Team\%20Charter\%20Assignment.docx}{Review Team Charter Assignment}

\item {} 
\sphinxAtStartPar
Edmondson, Teaming, Chpt 2 from “Cognitive Barriers to Teaming” (p. 82) to end of Chapter (on Canvas)

\item {} 
\sphinxAtStartPar
Cross Cultural Intelligence

\item {} 
\sphinxAtStartPar
When and How to Escalate

\end{itemize}
\begin{quote}

\sphinxAtStartPar
Optional: \sphinxhref{https://docs.google.com/document/d/1PsnDMS2emcPLgMLFAQCXZjO7C4j2hJ7znOq\_g2Zkjgk/export?format=pdf}{Fostering Psychological Safety Tips}
\end{quote}
&\\
\sphinxhline
\sphinxAtStartPar
Th Feb 6
&
\sphinxAtStartPar
Passive Prediction Questions: Purpose and Alignment
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{20\_using\_passive\_prediction\_questions.html}{Using Passive Prediction}

\item {} 
\sphinxAtStartPar
\sphinxhref{30\_questions/25\_answering\_passive\_prediction.html}{Answering Passive Prediction}

\end{itemize}
&\begin{itemize}
\item {} 
\sphinxAtStartPar
Pass

\end{itemize}
\\
\sphinxhline
\sphinxAtStartPar
Tu Feb 11
&
\sphinxAtStartPar
Passive Prediction Questions: Internal and External Validity
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{30\_questions/26\_passive\_prediction\_errors.md}{Right Way To Be Wrong}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Th Feb 13
&
\sphinxAtStartPar
Giving Feedback
&
\sphinxAtStartPar
Feedback Exercise?
&
\sphinxAtStartPar
Feedback Exercise?
\\
\sphinxhline
\sphinxAtStartPar
Tu Feb 18
&
\sphinxAtStartPar
Passive Prediction Questions: Loss Functions
&&\begin{itemize}
\item {} 
\sphinxAtStartPar
admissions

\item {} 
\sphinxAtStartPar
interp

\end{itemize}
\\
\sphinxhline
\sphinxAtStartPar
Th Feb 20
&
\sphinxAtStartPar
Passive Prediction Questions: Interpretability
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://arxiv.org/abs/1811.10154}{Stop Using Black Boxes}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html}{Black box models require trusting data too}

\end{itemize}
\begin{quote}

\sphinxAtStartPar
\sphinxstylestrong{Optional:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://notamonadtutorial.com/scientific-machine-learning-with-julia-the-sciml-ecosystem-b22802951c8a}{Combining explicit models and black boxes in Scientific ML}

\end{itemize}
\end{quote}
&\\
\sphinxhline
\sphinxAtStartPar
Tu Feb 25
&
\sphinxAtStartPar
Causal Questions: Purpose
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/30\_questions/30\_using\_causal\_questions.html}{Using Causal Questions}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/30\_questions/40\_answering\_causal\_questions.html}{Answering Causal Questions}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Th Feb 27
&
\sphinxAtStartPar
Causal Questions: Potential Outcomes
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/35\_causal/10\_potential\_outcomes.html}{Potential Outcomes Framework}

\end{itemize}
&
\sphinxAtStartPar
po
\\
\sphinxhline
\sphinxAtStartPar
Tu Mar 4
&
\sphinxAtStartPar
Causal Questions: Potential Outcomes
&\begin{itemize}
\item {} 
\sphinxAtStartPar
Cunningham, Chpt 4, pp 135 (“Independence Assumption”) \sphinxhyphen{} 148 (Stop at “Randomization Inference”)

\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/35\_causal/30\_interpreting\_indicator\_vars.html}{Indicator Variables}

\end{itemize}

\sphinxAtStartPar
(Note \sphinxtitleref{SDO} (Simple Difference in Outcomes) in Cunningham same as “\sphinxtitleref{widehat\{ATE\}} from class)
\begin{quote}
\begin{description}
\sphinxlineitem{Optional:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Read Chpt 4 in Cunningham from start (different presentation of potential outcomes)

\end{itemize}

\end{description}
\end{quote}
&
\sphinxAtStartPar
resume
\\
\sphinxhline
\sphinxAtStartPar
Th Mar 6
&
\sphinxAtStartPar
Causal Questions: Experiments in Practice, Internal Validity
&
\sphinxAtStartPar
Experiments: Internal Validity (In Practice):
\begin{itemize}
\item {} 
\sphinxAtStartPar
KTX: Chpt 2 (End to End Example)

\item {} 
\sphinxAtStartPar
KTX: Chpt 3, “Threats to Internal Validity” (p. 42\sphinxhyphen{}47)

\item {} 
\sphinxAtStartPar
KTX: Chpt 19 (A/A Testing)

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Tu Mar 11
&
\sphinxAtStartPar
Causal Questions: Experiments in Practice, External Validity
&
\sphinxAtStartPar
Overall Evaluation Criteria: KTX Chpt 7.

\sphinxAtStartPar
Finishing Internal Validity:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Different Randomizations: KTX Chpt 22

\end{itemize}

\sphinxAtStartPar
Experiments: External Validity (In Practice):
\begin{itemize}
\item {} 
\sphinxAtStartPar
KTX, Chpt 3, “Threats to External Validity” to end (p. 47\sphinxhyphen{}54)

\item {} 
\sphinxAtStartPar
Kohvai, Tang and Xu, Chpt 23 (Primacy Effects / Long Term Decay)

\end{itemize}
&
\sphinxAtStartPar
abtest
\\
\sphinxhline
\sphinxAtStartPar
Th Mar 13
&
\sphinxAtStartPar
Causal Questions: Experiments in Practice, Design
&
\sphinxAtStartPar
Designing Experiments
\begin{itemize}
\item {} 
\sphinxAtStartPar
Statistical Power and Sample Sizes: GHV Ch 16

\item {} 
\sphinxAtStartPar
\sphinxhref{endogenous\_stopping.ipynb}{Don’t stop experiments early!}

\end{itemize}
&
\sphinxAtStartPar
power
p\sphinxhyphen{}value preview
\\
\sphinxhline
\sphinxAtStartPar
Tu Mar 18
&
\sphinxAtStartPar
\sphinxstylestrong{NO CLASS}
&&\\
\sphinxhline
\sphinxAtStartPar
Th Mar 20
&
\sphinxAtStartPar
\sphinxstylestrong{NO CLASS}
&&\\
\sphinxhline
\sphinxAtStartPar
Tu Mar 25
&
\sphinxAtStartPar
Causal Questions: Experiments, Bayesian Analysis
&
\sphinxAtStartPar
AB Testing Review
&\\
\sphinxhline
\sphinxAtStartPar
Th Mar 27
&
\sphinxAtStartPar
Causal Questions: Experiments, Bayesian Analysis \& Decision Theory
&\begin{itemize}
\item {} 
\sphinxAtStartPar
Statistical Decision Theory (on Canvas). 550\sphinxhyphen{}556

\end{itemize}

\sphinxAtStartPar
(This is same as IDS 705 Lecture 8 Reading)
&\\
\sphinxhline
\sphinxAtStartPar
Tu Apr 16
&
\sphinxAtStartPar
\sphinxstylestrong{MIDTERM}
&
\sphinxAtStartPar
\sphinxstylestrong{MIDTERM}
&\\
\sphinxhline
\sphinxAtStartPar
Th Apr 3
&
\sphinxAtStartPar
Causal Questions: Regression
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/35\_causal/40\_causal\_inference\_beyond\_ab\_testing.html}{Causal Beyond Experiments}

\end{itemize}
&
\sphinxAtStartPar
incomeineq
\\
\sphinxhline
\sphinxAtStartPar
Tu Apr 8
&
\sphinxAtStartPar
Causal Questions: Matching
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/35\_causal/80\_matching\_why.html}{The Why of Matching}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://ds4humans.com/35\_causal/90\_matching\_how.html}{The How of Matching Summary}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://youtu.be/tvMyjDi4dyg?t=910}{Methods for Matching}

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Watch the video above from about 15 minutes in (where link starts) till at least 45 minutes in, keep going if you want to learn about propensity score matching problems.}
&
\sphinxAtStartPar
matching
\\
\sphinxhline
\sphinxAtStartPar
Th Apr 9
&
\sphinxAtStartPar
Causal Questions: Differences and Differences / Panels
&\begin{itemize}
\item {} 
\sphinxAtStartPar
Cunningham, Chpt 9 pp 406 (Difference in Differences) \sphinxhyphen{} pp 433 (Stop at “Importance of Placebos in Diff\sphinxhyphen{}in\sphinxhyphen{}Diff”)

\end{itemize}
\begin{quote}

\sphinxAtStartPar
\sphinxhyphen{}(Book page numbers at bottom of PDF on canvas. Full chapter is in PDF, more than you need to read.)

\sphinxAtStartPar
Optional but encouraged:
(dont need to follow everything, but here’s a real diff\sphinxhyphen{}in\sphinxhyphen{}diff)
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.cambridge.org/core/journals/american-political-science-review/article/enfranchisement-and-incarceration-after-the-1965-voting-rights-act/C68FA7BB8CA313BDD8D9A39BA666A21D}{Enfranchisement and Incarceration}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://netflixtechblog.com/key-challenges-with-quasi-experiments-at-netflix-89b4f234b852}{Diff\sphinxhyphen{}in\sphinxhyphen{}Diffs at Netflix}

\end{itemize}
\end{quote}
&\\
\sphinxhline
\sphinxAtStartPar
Tu Apr 15
&
\sphinxAtStartPar
Interpretation of Results
&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Revised Exploratory Due along with Causal Proposal}

\end{itemize}
&
\sphinxAtStartPar
ev
\\
\sphinxhline
\sphinxAtStartPar
Wed, April 23
&&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Optional Due Date for Causal Rough Draft (11pm)}

\end{itemize}
&\\
\sphinxhline
\sphinxAtStartPar
Wed, Apr 30
&&\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Final Causal Report Due}

\end{itemize}
&\\
\sphinxbottomrule
\end{longtable}
\sphinxtableafterendhook
\sphinxatlongtableend
\end{savenotes}

\sphinxstepscope


\part{Introduction}

\sphinxstepscope


\chapter{Solving Problems with Data}
\label{\detokenize{10_introduction/10_solving_problems_with_data:solving-problems-with-data}}\label{\detokenize{10_introduction/10_solving_problems_with_data::doc}}
\sphinxAtStartPar
Few fields have shown as much promise to address the world’s problems as data science. Today, data science is improving our understanding of and adaptation to climate change. It is being used in medicine to speed drug discovery, improve the quality of X\sphinxhyphen{}rays and MRIs, and ensure that patients receive appropriate medical care. It is used in courtrooms to fight for fair elections and electoral maps and by data journalists to document and communicate the injustices prevalent in our criminal justice system and issues in policing.

\sphinxAtStartPar
Data science also enables new technologies that may improve our lives. Autonomous drones are delivering blood and medical supplies to rural health clinics from Rwanda to \sphinxhref{https://www.theverge.com/2020/5/27/21270351/zipline-drones-novant-health-medical-center-hospital-supplies-ppe/}{North Carolina}, and driver\sphinxhyphen{}aid features continue to make progress in reducing the over 30,000 traffic deaths and millions of injuries that occur in the US alone every year. And nearly every facet of business — from the way businesses source materials and manage inventory to the way product offerings respond to customer behavior — has been reshaped by data science.

\sphinxAtStartPar
At the same time, it is also increasingly clear that today’s challenges will not be met by “throwing data science at the problem” or “just adding AI.” According to a 2020 MIT/BCG survey, \sphinxhref{https://www.wired.com/story/companies-rushing-use-ai-few-see-payoff/}{only 11\% of businesses that had piloted or employed Artificial Intelligence (AI) had reaped a sizeable return on their AI investments.} Businesses and regulators are also coming to appreciate the potential of data science tools to reinforce racial and gender inequities. Algorithms at Amazon have been found to \sphinxhref{https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G}{discriminate against female job applicants}. Medical algorithms have been found to prioritize White patients over Black patients \sphinxhref{https://www.wired.com/story/how-algorithm-blocked-kidney-transplants-black-patients/}{for kidney transplants} and \sphinxhref{https://www.washingtonpost.com/health/2019/10/24/racial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients/}{preventative care}. In the criminal justice system, algorithms have been found to \sphinxhref{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}{incorrectly identify Black defendants than White defendants as being a “danger to society” when providing risk assessments to judges deciding on pre\sphinxhyphen{}trial release, bail and sentencing}. And even Meta’s own research has shown its algorithms drive political polarization and division among users, and push users into extremist groups.%
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxhref{https://www.wsj.com/articles/facebook-knows-it-encourages-division-top-executives-nixed-solutions-11590507499}{Recent reporting by the Wall Street Journal} has shown that Facebook’s own research has confirmed what many outside experts have long argued: the way its recommendation engines prioritize content that results in “user engagement” (clicks, shares, comments) ends up promoting partisan, polarizing, sensationalist, or extreme content. In addition, their own research has also shown that group recommendations are contributing to extremism. According to one internal presentation, “64\% of all extremist group joins are due to our recommendation tools” like \sphinxstyleemphasis{Groups You Should Join} and other discovery tools.
%
\end{footnote}

\sphinxAtStartPar
How, then, should a burgeoning data scientist approach this discipline, full of such promise and peril? And why have so many data science endeavors failed to deliver on their promise?


\section{The Three Big Ideas}
\label{\detokenize{10_introduction/10_solving_problems_with_data:the-three-big-ideas}}
\sphinxAtStartPar
This book is my answer to these questions, and it is organized around three big ideas:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data science is about solving problems.} All too often, data scientists get lost in the technical details of models and lose sight of the bigger picture. Data science is not about maximizing accuracy or AUC scores — it’s about using data and quantitative methods to solve problems, and at the end of the day the only “metric” that matters is whether your work has solved the problem you set out to address.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data scientists solve problems by answering questions, and the question you are asking determines what tool is appropriate.} At their core, all data science tools are tools for answering questions, whether you realize it or not. Learning to recognize how data scientists use questions to solve problems — and exactly what questions are being answered by the tools you use every day — is key to navigating the ambiguity of real\sphinxhyphen{}world problem solving.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reasoning rigorously about uncertainty and errors is what differentiates good data scientists from great data scientists.} Data science isn’t just about minimizing classification errors and uncertainty — it’s also about deciding how unavoidable errors should be distributed, how to weigh the risks and trade\sphinxhyphen{}offs inherent in probabilistic decision\sphinxhyphen{}making rigorously and in a manner that takes into account the problem you are trying to solve, and to take uncertainty into account when acting on data.

\end{enumerate}

\sphinxAtStartPar
From the summary of these three ideas, it should be immediately obvious that this book is different from many other data science books you may have read. Where most data science books are designed to teach specific data science techniques or methods, the aim of this book is to provide readers with a framework for thinking about their goals and how to achieve them using data science methods. It is, in a sense, about everything you need to know \sphinxstyleemphasis{beyond} the technicalities of model fitting. It is about what comes \sphinxstyleemphasis{before} and \sphinxstyleemphasis{after} you fit your model: it will help you decide what questions to answer, what data to collect, and what models to consider using \sphinxstyleemphasis{before} you actually fit your model, and it will help you learn to evaluate whether a result is likely to generalize, whether a model is safe to deploy, and how to communicate those facts to a stakeholder \sphinxstyleemphasis{after} model fitting.

\sphinxAtStartPar
The importance of these skills is often underestimated by data science students, and for understandable reasons. Data science curricula usually begin with coding, statistics, and model evaluation techniques. As a result, the hardest part of data science classes is often mastering the technical details of model implementation. Moreover, the limited time available to instructors and the need to support full classes of students means data science exercises almost always have to come with clear directions and problem scaffolding to ensure students meet their learning goals.

\sphinxAtStartPar
But real\sphinxhyphen{}world problems don’t come with directions. Indeed, a problem that is clearly defined and for which a solution is obvious isn’t a problem anyone will pay you very much to solve. No, classroom exercises are carefully structured to foster learning and to make it possible for instructors to grade and provide feedback at scale. But real problems — the kind you will encounter in industry, government, or research — are hard to even articulate clearly, never mind solve. And that is why, as we will see, what really sets exceptional professional data scientists apart is not their ability to get a high AUC — \sphinxstylestrong{it’s their ability to navigate and thrive in the face of ambiguous problems and goals.}


\section{This Is A Book About The Forest, Not The Trees}
\label{\detokenize{10_introduction/10_solving_problems_with_data:this-is-a-book-about-the-forest-not-the-trees}}
\sphinxAtStartPar
The goal of this book, to frame things a little differently, is to help young data scientists maintain perspective. Students spend so much time learning individual techniques that they are unable to see the forest for trees. But this is not a book about individual techniques — it’s about learning to think strategically about how to use those techniques to solve real problems.

\sphinxAtStartPar
To maintain its focus on the “forest,” this book \sphinxstyleemphasis{takes as given} that you have already been introduced to statistical inference and machine learning and know how to faithfully fit a model in a robust manner. That means that topics like hypothesis testing, cross\sphinxhyphen{}validation, how to use train\sphinxhyphen{}test splits, and how to evaluate a model’s AUC will be treated as assumed knowledge.%
\begin{footnote}[2]\sphinxAtStartFootnote
If you are a Duke student reading this, it’s ok if you are not yet familiar with all of these topics so long as you are taking a good machine learning course — like IDS 705 — concurrently.
%
\end{footnote} This is in no way meant to suggest these topics aren’t important — we will reference them constantly — just that I will not attempt to teach them here, both to maintain focus on the goals of this book, and also because there already exist many other resources that introduce these topics better than I could.


\section{Introduction Structure}
\label{\detokenize{10_introduction/10_solving_problems_with_data:introduction-structure}}
\sphinxAtStartPar
The remainder of this introductory chapter contains an overview of the Big 3 ideas of the book. All concepts discussed here will also be covered in greater detail in future readings, but before we dive into them in detail, it’s helpful to get a sense of the overall approach we will be taking!


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Idea 1: Solving Problems}
\label{\detokenize{10_introduction/20_solving_problems:idea-1-solving-problems}}\label{\detokenize{10_introduction/20_solving_problems::doc}}
\sphinxAtStartPar
Given the focus of data science coursework on the math, statistics, and programming that is required to use data science tools, one would be forgiven for thinking that understanding these tools is the ultimate goal of data science.

\sphinxAtStartPar
At the end of the day, however, your success as a data scientist will not be evaluated by whether you write good code or whether your classification model’s AUC score is high. No, your success as a data scientist will always be evaluated based on a much simpler criterion: \sphinxstylestrong{did you make your stakeholder’s life better by solving a problem they faced?}

\begin{sphinxShadowBox}

\sphinxAtStartPar
Your success as a data scientist will \sphinxstyleemphasis{always} be evaluated by one simple criterion: \sphinxstylestrong{did you make your stakeholder’s life better by solving a problem they faced?}
\end{sphinxShadowBox}

\sphinxAtStartPar
On the last page, I said that that one of the key goals of this book is to help young data scientist’s understand the broader context of their work. There is perhaps no better way to do that then to always keep this one fact front of mind. The things you practice in class exercises — writing good code and finding a classification model with a high AUC — are things that may \sphinxstyleemphasis{help} you achieve the goal of solving your stakeholder’s problem, but they are never sufficient in and of themselves. Indeed, it is hard to overstate how common it is to see talented young data scientists write brilliant, performant code and fit extremely accurate models that predict the wrong property, solve a problem that isn’t actually core to the stakeholder’s needs, require data not available in deployment, or work in training but won’t generalize to the stakeholder’s deployment context. The data scientists on these projects often deployed the skills they learned in technical classes magnificently, but this execution was not well\sphinxhyphen{}directed in service of the stakeholder — the classic “not seeing the forest for the trees” mistake.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Throughout this book, I will frequently use the term “stakeholder” to refer to the person whose problem that you, the data scientist, is seeking to address. I use this term because, as a young data scientist, you will often be in the position of having to use your data science skills to help someone else. Thus your stakeholder may be your manager, your CEO, or someone at another company you are advising.

\sphinxAtStartPar
However, if you’re lucky enough to \sphinxstyleemphasis{not} be directly answerable to someone else, either because you work for yourself or because you’re in a field that gives you substantial autonomy like academia, you can simply think of your “stakeholder” as yourself.

\sphinxAtStartPar
If you’re interested in developing a consumer\sphinxhyphen{}facing product (e.g., you’re an independing developer whose thinking of creating a new data\sphinxhyphen{}science\sphinxhyphen{}based web app), you may also find it useful to think of your customer of the stakeholder, since very few products are successful if they don’t solve a problem customers face.
\end{sphinxadmonition}


\subsection{Specifying The Problem}
\label{\detokenize{10_introduction/20_solving_problems:specifying-the-problem}}
\sphinxAtStartPar
How, then, can we be sure our hard work as data scientists serves the interests of our stakeholder?

\sphinxAtStartPar
The first step in solving any problem is \sphinxstyleemphasis{always} to carefully specify the problem. While this may seem obvious, properly articulating the core problem one seeks to address can be remarkably difficult. Moreover, because everything you will do \sphinxstyleemphasis{after} articulating your problem is premised on having correctly specified your objective, it is \sphinxstyleemphasis{the} greatest determinant of the success of your project. The most sophisticated, efficiently executed, high precision, high recall model in the world isn’t worth a lick of good if the results it generates don’t solve the problem you or your stakeholder need solved.

\sphinxAtStartPar
Specifying your problem not only ensures that your subsequent efforts are properly directed, but it can also radically simplify your task. Many times problems only \sphinxstyleemphasis{appear} difficult because of how they are presented. As Charles Kettering, Head of Research at General Motors from 1920 to 1947 once said, “A problem well stated is a problem half solved.”

\sphinxAtStartPar
How do you know if you’ve “clearly articulated the problem,” and how should you go about refining your problem statement with your stakeholder? Those are topics we will discuss in detail in the coming chapters, as well as strategies for using data to help inform this process through iterative refinement of your understanding of the contours of the problem space.


\subsection{Stakeholder Management and Domain Expertise}
\label{\detokenize{10_introduction/20_solving_problems:stakeholder-management-and-domain-expertise}}
\sphinxAtStartPar
Application is, in many ways, what sets data science apart from the disciplines of mathematics, computer science, and statistics. And to apply the tools of data science tools effectively, by definition, requires an understanding of the domain to which your tools are being deployed. This fact makes many data scientists uncomfortable — after all, many young data scientists do not even know the industry in which they will be employed when the graduate, never mind feel they can lay claim to being a “domain expert” in any specific substantive field.

\sphinxAtStartPar
A consequence of this discomfort is that “domain expertise” is that the term “domain expertise” is often invoked as a way of abdicating responsibility for part of an analysis as “somebody else’s problem” (a dangerously powerful construction, as explained by the \DUrole{xref,myst}{incomparable Douglas Adams}).

\sphinxAtStartPar
But it would be a mistake to view domain expertise as beyond the responsibility of the data scientist for two reasons. First, engaging with the details of a substantive domain to tailor the application of data science methods to solve a specific problem isn’t ancillary to the job of a data scientists — it’s a data scientist’s comparative advantage. Data scientists (generally) are not the most technically skilled mathematicians, statisticians, or programmers — we are professionals who specialize in taking the best insights from all three of these skills sets and adapting them to fit the specific needs of a specific problem. So embrace the role of “domain knowledge!”

\sphinxAtStartPar
The second reason is that while you are unlikely to be a “domain expert” yourself, learning to solicit important information about a domain from true domain experts is a skill unto itself. As we will detail in our readings on “stakeholder management,” stakeholders may be experts in their particular substantive domain, but because they (usually) won’t understand data science, they are unlikely to ever provide you with the domain knowledge you need to do your job successfully (and you can’t get away with just saying “yes, this project failed, but it was because you didn’t tell me X!” See the discussion above of the single criterion used to evaluate data scientists). So learning to \sphinxstyleemphasis{partner} with your stakeholder and domain experts to understand a problem is a key part of understanding it properly, and thus eventually solving it.

\sphinxstepscope


\section{Idea 2: Solving Problems by Answering Questions}
\label{\detokenize{10_introduction/22_question_types:idea-2-solving-problems-by-answering-questions}}\label{\detokenize{10_introduction/22_question_types::doc}}
\sphinxAtStartPar
Once you’ve clearly articulated the problem you wish to solve, the next step is… to solve it! But how best to do so?

\sphinxAtStartPar
As data scientists, we are somewhat restricted in the types of solutions to which we have access. Nobody expects a data scientist to discover a new semiconductor manufacturing technique or solicit donors for cancer research funds.

\sphinxAtStartPar
But what data scientists can do is \sphinxstylestrong{answer questions about the world.} And while that may not seem inspiring at first, it’s a remarkably powerful ability. That’s because whether it’s trying to answer the question “does this patient have disease X?”, “what would happen to profits if we changed our pricing strategy?”, or “do universal income programs reduce long\sphinxhyphen{}term poverty?”, it turns out that a great many problems faced by both businesses and society as a whole become much easier to solve if we can better understand the consequences of our actions or reduce our uncertainty about the world.

\sphinxAtStartPar
In light of that fact, we can reframe the challenge of a data scientist from the more amorphous task of “figuring out how to solve the problem” to the more concrete “what question, if answered, would make it easier to solve this problem?” Once we’ve articulated a question to answer we can turn to choosing the best tool for generating an answer. But it is worth emphasizing this point — it is only at this stage of our project—not at the beginning!—that we start thinking about what statistical method, algorithm, or model is appropriate for the task.

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Answering Questions}

\sphinxAtStartPar
The challenge of a data scientist is to figure out “what question, if answered, would make it easier to solve this problem?”
\end{sphinxShadowBox}

\sphinxAtStartPar
But what if my stakeholder wants me to do something other than answer a question about the world? What if they want me to write a model to automatically read x\sphinxhyphen{}rays, or detect fraudulent transactions?

\sphinxAtStartPar
While it may not be immediately obvious how these fit in the “data scientists solve problems by answering questions about the world” framework, I can assure you they do. Indeed, anything you can think of that a data scientist might do ends up fitting this model because — as we’ll discuss in detail below and later in the book — \sphinxstyleemphasis{all} data science models and algorithms can be understood as instruments designed to answer very specific, very concrete questions about data. And once you come to appreciate that fact, not only will this frame make more sense, but you will probably also find you understand many of the models you work with regularly more intuitively than you did before.




\subsection{Types of Questions}
\label{\detokenize{10_introduction/22_question_types:types-of-questions}}
\sphinxAtStartPar
There are three types of questions we will explore in this book, each of which helps to solve problems in a different way:%
\begin{footnote}[1]\sphinxAtStartFootnote
Careful readers may notice that these categories do not include \sphinxstyleemphasis{should questions}, which are sometimes referred to as “prescriptive” or “normative” questions. As we will discuss in detail in an upcoming reading, that is because while data science is an amazing tool for characterizing the world around us, it cannot, on its own, answer questions about how the world \sphinxstyleemphasis{should} be. Answering “should questions” requires evaluating the desirability of different possible states of the world, and that can only be done with reference to a system of values, making them inherently subjective. Data science can help us predict the \sphinxstyleemphasis{consequences} of different courses of action, but it cannot tell us whether those consequences make a given course of action \sphinxstyleemphasis{preferable}.
%
\end{footnote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Exploratory Questions: Questions about patterns and structure in the world.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Help us to understand the problem space better and prioritizing subsequent efforts.

\end{itemize}

\item {} 
\sphinxAtStartPar
Passive Prediction Questions: Questions about likely outcomes for individual observations or entities.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Useful for targeting individuals for additional attention or automating certain tasks.

\end{itemize}

\item {} 
\sphinxAtStartPar
Causal Questions: Questions about the consequences of actions or interventions being considered.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Useful for deciding on appropriate courses of action.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
Each of these can play a different but important role in solving problems, and any effort to answer a question of each type will raise similar issues that need to be considered. As summarized next, by recognizing the \sphinxstyleemphasis{class} of questions we are seeking to answer, we can significantly narrow both the set of data science tools that are appropriate to consider and provide a short list of common considerations to think through.


\subsection{Exploratory Questions}
\label{\detokenize{10_introduction/22_question_types:exploratory-questions}}
\sphinxAtStartPar
Once you have settled on a problem you wish to address, the next step is often to use data science to better understand the contours of the problem in order to better prioritize and strategize your efforts. As data scientists, our best strategy for this type of investigation is to ask questions about general patterns related to your problem — what I call \sphinxstyleemphasis{Exploratory Questions.}

\sphinxAtStartPar
Why is this necessary? Well, as we’ll discuss in a future reading on “stakeholder management,” you would be \sphinxstyleemphasis{shocked} at how often stakeholders have only a vague sense of the patterns surrounding their problem. This makes refinement of your problem statement (and thus prioritization of your subsequent efforts) impossible. So before you get too far into any data science project, it’s important to ask Exploratory Questions to improve your understanding of how best to get at your problem.

\sphinxAtStartPar
To illustrate, suppose a company hired you because they were having trouble recruiting enough high\sphinxhyphen{}quality employees. You \sphinxstyleemphasis{could} ask for their HR data and immediately try to train a neural network to… well, I’m not even sure what you’d want to train it to do right off that bat! And that’s a big part of the problem. Getting more high\sphinxhyphen{}quality employees is a very general problem, and you could imagine addressing it in any number of ways — you could try and get more people to apply for the job in the first place, you could try and get a \sphinxstyleemphasis{different type} of candidate to apply then is currently applying, you could try and get more high\sphinxhyphen{}quality people who are given job offers to accept those offers, or you could help try to increase the number of people who are hired who turn out to be successful hires! But which should you do first?

\sphinxAtStartPar
To help answer this question, we can start by asking a series of Exploratory Questions that, when answered, will aid in your efforts to solve your stakeholder’s problem:
\begin{itemize}
\item {} 
\sphinxAtStartPar
How many job applications are you receiving when you post a job?

\item {} 
\sphinxAtStartPar
What share of your current job applicants are of high quality?

\item {} 
\sphinxAtStartPar
If your current applicants come from different sources (online ads, services like Indeed, outreach to colleagues for recommendations, etc.), what share of job applicants from each of these sources are of high quality?

\item {} 
\sphinxAtStartPar
What share of employees you try to hire accept your offer?

\item {} 
\sphinxAtStartPar
What share of employees you do hire turn out to be successful employees?

\end{itemize}

\sphinxAtStartPar
Suppose, for example, only 10\% of applicants who receive job offers accept. Then clearly that would seem a place where intervention would be likely to substantially increase the number of high\sphinxhyphen{}quality employees being hired. If, by contrast, 95\% of applicants accept offers, then that is clearly not a place where you would want to focus.

\sphinxAtStartPar
Similarly, if most applicants are high quality and there just aren’t enough of them, then you would probably want to focus your efforts on increasing the number of people who apply in the first place. But if only 2\% of applicants seem appropriate to the company, then maybe focus should be put on changing \sphinxstyleemphasis{who} is applying for positions with an eye towards increasing the average quality of applicants.

\sphinxAtStartPar
Answering these questions will likely not, on its own, make it clear exactly where to focus your efforts. Your stakeholder may look at the fact that only 2\% of applicants are appropriate and say “That’s fine — we have so many applications that the \sphinxstyleemphasis{absolute number} of quality applicants is actually high enough, and it’s easy to filter out the bad applicants.” But these are numbers you can bring back to your stakeholder to discuss and use to zero in on the specific facet of their problem that is most amenable to an impactful solution.

\sphinxAtStartPar
Generating answers to these types of Exploratory Questions doesn’t have the same “coolness factor” as using expensive GPUs to train deep learning models. But it is precisely this type of analysis that will help ensure that when if you \sphinxstyleemphasis{do} later run up a giant bill renting GPUs, at least that money will have been spent addressing a part of your stakeholder’s problem that matters.

\sphinxAtStartPar
So how does one go about answering Exploratory Questions? As we’ll discuss in later chapters, at times Exploratory Questions can be answered with simple tools, like scatter plots, histograms, and the calculation of summary statistics like means and medians. Other times, however, it may require more sophisticated methods, like clustering or other unsupervised machine learning algorithms that can, say, identify “customer\sphinxhyphen{}types” in a large dataset of customer behavior.

\sphinxAtStartPar
Regardless of the tool used, however, the goal is always to identify patterns in the world that are salient to understanding your stakeholder’s problem.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Answering Exploratory Questions is \sphinxstyleemphasis{not} synonymous with “Exploratory Data Analysis” (EDA). As it is commonly understood and practiced by students, EDA refers to the process of poking around in a new data set before fitting a more complicated statistical model. It entails learning what variables are present, how they are coded, and \sphinxstyleemphasis{sometimes} looking at general patterns in the data prior to model fitting. Crucially, it is generally defined by what it is \sphinxstyleemphasis{not} — it is what you do \sphinxstyleemphasis{before} you fit a complicated model — and by the tools used — EDA consists of plotting distributions or cross\sphinxhyphen{}tabulations, not fitting models or doing more sophisticated analyses.

\sphinxAtStartPar
Answering Exploratory Questions, by contrast, is about achieving a substantive goal — learning about patterns \sphinxstyleemphasis{in the world} — not the tools used to do it, or what it comes before. Answering an important Exploratory Question may require you to actively seek out new data, merge data from different sources together, and potentially do novel data collection. It may also entail model fitting or use of unsupervised machine learning algorithms to uncover latent patterns.

\sphinxAtStartPar
{\hyperref[\detokenize{30_questions/07_eda::doc}]{\sphinxcrossref{\DUrole{std,std-doc}{We will discuss the conceptual issues surrounding EDA in more detail in a later reading}}}}.
\end{sphinxadmonition}


\subsection{Passive Prediction Questions}
\label{\detokenize{10_introduction/22_question_types:passive-prediction-questions}}
\sphinxAtStartPar
Answering Exploratory Questions helps you to prioritize your efforts and improve your understanding of your stakeholder’s problem. Often you will bring the answers you generate to Exploratory Questions back to your stakeholder and use them to refine your problem statement in an iterative loop. “Yes, your broader problem is your profit margins are too thin, but after looking at your cost structure, it seems that the biggest driver of costs is the labor that goes into product packaging. So let’s focus on minimizing that.” But what then? Simple! We return to asking “What question, if answered, would help solve my problem?”

\sphinxAtStartPar
The second class of questions which, if answered, often help solve stakeholder problems are \sphinxstyleemphasis{Passive Prediction Questions}. Passive Prediction Questions are questions about the future outcomes or behaviors of \sphinxstyleemphasis{individual entities} (people, stocks, stores, etc.). This type of question may take the form “Given this new customer’s behavior on my website, are they likely to spend a lot over the next year?” or “Given the symptoms of this patient and their test results, how likely are they to develop complications after surgery?”  (Don’t worry about the “passive” part of the name — we’ll circle back to why that’s there below. For the moment, you may find it helpful to just think of these as “Prediction Questions.”).

\sphinxAtStartPar
Because Passive Prediction Questions are questions about individual entities, they don’t necessarily have one “big” answer. Rather, Passive Prediction Questions are answered by fitting or training a model that can take the characteristics of an individual entity as inputs (e.g., this patient is age 67, has blood pressure of 160/90, and no history of heart disease) and spitting out an answer \sphinxstyleemphasis{for that individual} (given that, her probability of surgical complications is 82\%). This differentiates Passive Prediction Questions from Exploratory Questions, which are about global patterns, not individual level predictions.

\sphinxAtStartPar
With Passive Prediction Questions, the first question to ask is often one of feasibility: “Given data on new customer behavior on my website, \sphinxstylestrong{can I} predict how much they are likely to spend a lot over the next year?” But you then answer that question by training a model that can answer the question you really care about for any given customer: “Given this new customer’s behavior on my website, are they likely to spend a lot over the next year?”

\sphinxAtStartPar
This ability to make predictions about future outcomes is obviously of tremendous use to stakeholders as it allows them to tailor their approach at the individual level. A hospital that can predict which patients are most likely to experience complications after surgery can allocate their follow\sphinxhyphen{}up care resources accordingly. A business that knows which customers are more likely to be big spenders can be sure that those customers are given priority by customer care specialists.

\sphinxAtStartPar
But the meaning of the term “Prediction” in Passive Prediction Questions extends beyond “predicting the future”. Passive Prediction Questions also encompass efforts to predict how a third party \sphinxstyleemphasis{would} behave or interpret something about an individual if given the chance.

\sphinxAtStartPar
For example, suppose our hospital stakeholder wanted to automate the reading of mammograms so that rural hospitals without full\sphinxhyphen{}time radiologists could give patients diagnoses more quickly (or, more cynically, pay fewer radiologists).%
\begin{footnote}[2]\sphinxAtStartFootnote
Mammograms are x\sphinxhyphen{}rays of breast tissue used for the detection of breast cancer.
%
\end{footnote} How does a model that reads mammograms “answer a question”? Well, in a very meaningful way, if you train your model by feeding it a dataset of mammograms that have been labelled as “normal” or “abnormal” by radiologists, then what that model is learning to do is answer the question: “if a radiologist looked at this particular scan, would they conclude it is abnormal?”

\sphinxAtStartPar
The value of this type of prediction to stakeholders is likely also self\sphinxhyphen{}evident, as it opens the door for automation and scaling of tasks that would otherwise be too costly or difficult for humans. Indeed, answering this question is the type of task for which machine learning has become most famous. Spam filtering amounts to answering the question “If the user saw this email, would they tag it as spam?” Automated content moderation amounts to answering “Would a Meta contractor conclude the content of this photo violates Facebook’s Community Guidelines?” Indeed, even Large Language Models (LLMs) like chatGPT, Bard, and LLaMA can be understood in this way, as we will discuss later.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Thinking of training an algorithm to read a mammogram as a model that answers the question “if a radiologist looked at this particular scan, would they conclude it is abnormal?” may seem a little strange at first. But this framing is both more accurate and more conceptually useful than the more common frame of “this is a a model that detects abnormal mammograms.” That’s because when a data science problem is solved by training a model on data on past human behavior, it isn’t actually learning to “detect cancer” — it’s learning to emulate the behavior of the humans in the training data.

\sphinxAtStartPar
This distinction is subtle, but it is important because it helps us to understand why any model we train in this way will inherit all of the biases and limitations of the radiologists who created the data used to train the algorithm. If, for example, our radiologists were less likely to see cancer in denser breast tissue, that bias would also be inherited by the algorithm.

\sphinxAtStartPar
(We call the inevitable existance of some difference between between what we \sphinxstyleemphasis{want} the algorithm to do — in this case, detect cancer — and \sphinxstyleemphasis{what it is actually being trained to do} — predict how a radiologist would interpret the scan — an “alignment problem.”)
\end{sphinxadmonition}

\sphinxAtStartPar
It is worth emphasizing that the distinction between Exploratory Questions and Passive Prediction Questions is a distinction \sphinxstyleemphasis{in purpose}, not necessarily a distinction in the statistical tools that are most appropriate to the task. A linear regression, for example, may be used for answering either type of question, but in different ways. To answer an Exploratory Question, we might look at the coefficients in a linear regression to understand the partial correlations between variables in the data. To answer a Passive Prediction Question, we might only look at the predicted values from the regression model.

\sphinxAtStartPar
But even if the same \sphinxstyleemphasis{type} of model can be used for both purposes, how one \sphinxstyleemphasis{evaluates} the model depends entirely on the purpose to which it is being put. When answering an Exploratory Question through the interpretation of regression coefficients, the size of the standard errors on the coefficients is critical. When making predictions, by contrast, one may not care about the coefficients of a model at all! So long as the R\sphinxhyphen{}squared is high enough (and other diagnostics seem good), one can simply use the predicted values the regression generates without ever looking “inside the box.”

\sphinxAtStartPar
As such, there’s no simple mapping between statistical or machine learning methods and the type of questions you aim to answer. With that said, the study of how to answer Passive Prediction Questions is often referred to as “supervised machine learning.”


\subsection{Causal Questions}
\label{\detokenize{10_introduction/22_question_types:causal-questions}}
\sphinxAtStartPar
The last category of question that data scientists are commonly called upon to answer are \sphinxstyleemphasis{Causal Questions}: questions about what the \sphinxstyleemphasis{effect} might be if a certain action is taken. For example, “What would be effect be of patients with disease X taking medication Y?” or “what would the effect of changing the interface of my app be on user retention?”

\sphinxAtStartPar
Causal Questions most often arise when a stakeholder wants to do something — buy a Superbowl ad, change how a recommendation engine works, authorize a new medical device — but they fear the action they are considering might be costly and not actually work. In these situations, stakeholders will often turn to a data scientist in the hope that the scientist can “de\sphinxhyphen{}risk” the stakeholder’s decision by providing guidance on the likely effect of the action \sphinxstyleemphasis{before} the action is undertaken at full scale.

\sphinxAtStartPar
Causal Questions, therefore, take the form of “What is the effect of an action X on an outcome Y?”—or more usefully, “If I do X, how will Y change?”. Nearly anything can take the place of X and Y in this formulation: X could be something small, like changing the design of a website, or something big, like giving a patient a new drug or changing a government regulation. Y, similarly, could be anything from “how long users stay on my website” or “how likely are users to buy something at my store” to “what is the probability that the patient survives”.

\sphinxAtStartPar
In my view, Causal Questions are perhaps the hardest to answer for two reasons. The first is that when we ask a Causal Question, we are fundamentally interested in \sphinxstyleemphasis{comparing} what our outcome Y would be in two states of the world: the world where we do X, and the world where we don’t do X. But as we only get to live in one universe, we can never perfectly know what the value of our outcome Y would be in \sphinxstyleemphasis{both} a world where we do X and one where we don’t do X—a problem known as the \sphinxstylestrong{Fundamental Problem of Causal Inference} (causal inference is just what people call the study of how to answer Causal Questions).

\sphinxAtStartPar
But the second reason is Causal Questions land on the desk of data scientists when a stakeholder wants to know the likely consequences of an action \sphinxstyleemphasis{before they actually undertake the action at full scale.} This may seem obvious, but it bears repeating — not only is answering Causal Questions hard because we never get to measure outcomes in both a universe where our treatment occurs and also a universe where it does not (the Fundamental Problem of Causal Inference), but answering Causal Questions is \sphinxstyleemphasis{also} hard because stakeholders want to know about the likely consequences of an action they aren’t ready to actually undertake!

\sphinxAtStartPar
As a result, the job of a data scientist who wants to answer a Causal Question is to design a study that not only measures the effect of a treatment but also does so in a setting that is enough like the context in which the stakeholder wants to act that any measured effect will generalize to the stakeholder’s context.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Now that we’ve discussed Causal Questions, we can discussion the term “passive” in “Passive Prediction Questions.” The term “Passive” in “Passive Prediction Questions” is meant to differentiate situations where a stakeholder wants to predict things about individuals they do not yet know \sphinxstyleemphasis{in a world where the status quo prevails and our behavior doesn’t change.}

\sphinxAtStartPar
For example, when answering the Passive Prediction Question “Given their case history, how likely is this patient to experience post\sphinxhyphen{}surgical complications?” we don’t actually want to know how likely they are to experience complications — we want to know how likely they would be to experience complications \sphinxstyleemphasis{absent any intervention.} Our hope, after all, is that by learning that a certain patient is likely to experience complications we can act to prevent that outcome!

\sphinxAtStartPar
Obviously, we might then want to build on the insights provided by that model and ask a followup Causal Question: “What would the effect be of assigning an extra nurse to patients predicted to be more likely to have post\sphinxhyphen{}surgical complications?”

\sphinxAtStartPar
But because these questions are different in their goals, we would also go about answering them in very different ways. For example, we might answer our Passive Prediction Question using historical patient data and a logistic regression or decision tree. And to answer our Causal Question, we might run a randomized experiment in which half the patients we predict as likely to experience complications are assigned an extra nurse and half are not, and we later compare patient outcomes.
\end{sphinxadmonition}


\subsection{An Example}
\label{\detokenize{10_introduction/22_question_types:an-example}}
\sphinxAtStartPar
In this introductory chapter alone, we’ve already covered a substantial amount of material. We’ve discussed the importance of problem articulation, the idea that the way data scientists solve problems is by answering questions, and the three types of questions data scientists are likely to encounter.

\sphinxAtStartPar
It’s easy to see how this framework might result in a sequential development of a project. First, a hospital comes to you concerned about the cost of surgical complications. So you:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Work with them to more clearly define the problem (“Surgical complications are extremely costly to the hospital and harm patients. We want to reduce these complications in the most cost\sphinxhyphen{}effective manner possible.”)

\item {} 
\sphinxAtStartPar
You answer some Exploratory Questions (“Are all surgical complications equally costly, or are there some we should be most concerned about?”).

\item {} 
\sphinxAtStartPar
You develop a model to answer a Passive Prediction Question (“Given data in patient charts, can we predict which patients are most likely to experience complications?”) so the hospital can marshal its limited nursing resources more effectively.

\item {} 
\sphinxAtStartPar
The hospital then comes back to you to ask the Causal Question “Would a new program of post\sphinxhyphen{}discharge nurse home visits for patients identified as being at high risk of complications reduce complications?”

\end{enumerate}

\sphinxAtStartPar
In reality, however, while it is important that some steps come before others (if you don’t start by defining your problem, where do you even start?), real projects are never so linear. The reality is that you will constantly find yourself moving back and forth between different types of questions, using new insights gained from answering one question to refine your problem statement and articulate new questions.

\sphinxAtStartPar
Nevertheless, by using this framework as a starting point, and using this taxonomy to help you recognize (a) the type of question you are asking, and (b) the reason you are seeking to answer a given question even when iterating through a project, you will see tremendous gains in your ability to please your stakeholders by staying focused on the problems they need addressed.


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Idea 3: Being Thoughtful About Being Wrong}
\label{\detokenize{10_introduction/23_mistakes:idea-3-being-thoughtful-about-being-wrong}}\label{\detokenize{10_introduction/23_mistakes::doc}}
\sphinxAtStartPar
The third big idea of this book is that one of the biggest differentiators of good data scientists and great scientists is not their ability to maximize the accuracy of their models, but to think rigorously about their models limitations and the errors they will inevitably commit.

\sphinxAtStartPar
To illustrate the type of fallacy that young data scientists fall prey to in their quest to maximize model performance, let us consider that most intuitive of metrics, \sphinxstyleemphasis{accuracy} — the share of classifications a model makes that are correct. In my role as the Admissions Chair for the Duke Masters of Interdisciplinary Data Science (MIDS) program, I read hundreds of essays a year from aspiring data scientists, and I long ago lost count of the number of times I have seen some version of the following passage with little to no additional context:
\begin{quote}

\sphinxAtStartPar
While working in {[}person{]}’s lab, I fit a {[}logit/XGBoost/Random Forest/Deep Learning{]} model to the data and achieved an accuracy of 96\%.
\end{quote}

\sphinxAtStartPar
Or, in the applicant’s resume, they report attaining an accuracy score in the 90s with some model.

\sphinxAtStartPar
I assume that these authors believe that this type of declaration reflects well on them — after all, 96\% accuracy means only 4\% of observations were mis\sphinxhyphen{}classified! But the truth is that reporting a high accuracy absent addition information about model performance actually tells me more about the author’s failure to understand this third Big Idea of the book than it tells me about their modelling prowess.

\sphinxAtStartPar
To illustrate why, suppose I told you I had developed a model that could predict breast cancer in mammograms with an accuracy of over 90\%. That’d be amazing, right? Mammograms cost roughly \$10 \sphinxstyleemphasis{billion} dollars a year in the US alone,%
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxhref{https://pmc.ncbi.nlm.nih.gov/articles/PMC4142190/}{“Aggregate Cost of Mammography Screening in the United States: Comparison of Current Practice and Advocated Guidelines”}, \sphinxstyleemphasis{Annals of Internal Medicine}, February 2014.
%
\end{footnote} so any reduction in need for skilled radiologists to review mammograms would be a huge win for women’s health and healthcare costs, right?

\sphinxAtStartPar
Now suppose I told you that the model I wrote was this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}cancer\PYGZus{}detection\PYGZus{}model}\PYG{p}{(}\PYG{n}{mammogram}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous} \PYG{o}{=} \PYG{k+kc}{False}
    \PYG{k}{return} \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous}
\end{sphinxVerbatim}

\sphinxAtStartPar
How does that have an accuracy of 90\%? Simple — according to the Susan G. Komen Society, roughly 90\% of routine mammograms are normal and require no followup. Thus a “model” that reports all routine mammograms are normal will immediately achieve an accuracy score of 90\%. In fact, the true accuracy of the model is actually higher than 90\%, since most mammograms flagged as “abnormal” are determined to not be indicative of cancer after followup (e.g., after biopsies).

\sphinxAtStartPar
So, are you still impressed by my model’s 90\% accuracy?

\sphinxAtStartPar
Of course not. And to be clear, the problem with this model is \sphinxstyleemphasis{not} that its accuracy is only 90\% and not, say 95\%. The problem with this model is that \sphinxstyleemphasis{it has a False Negative Rate (the share of cases that are positive — in this case, mammograms from women with cancer — that are classified as cancer free) of 100\%.} And since the thing we care about most in cancer screenings is not telling a patient with cancer they’re fine (a False Negative), that’s a huge problem!

\sphinxAtStartPar
OK, if what we care about is the False Negative Rate, shouldn’t we just minimize the False Negative Rate? Not so fast! Consider this model:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}no\PYGZus{}false\PYGZus{}negative\PYGZus{}model}\PYG{p}{(}\PYG{n}{mammogram}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous} \PYG{o}{=} \PYG{k+kc}{True}
    \PYG{k}{return} \PYG{n}{is\PYGZus{}scan\PYGZus{}abnormal\PYGZus{}maybe\PYGZus{}cancerous}
\end{sphinxVerbatim}

\sphinxAtStartPar
Oh dear. Yes, that model has a 0\% False Negative Rate, but it also has a 100\% False \sphinxstyleemphasis{Positive} Rate, meaning anyone subject to this screening would be told they might have cancer and needs followup diagnostic procedures! That’s no good either.

\sphinxAtStartPar
No, a \sphinxstyleemphasis{good} model for reading mammograms needs to maximize accuracy while also balancing the desire to not subject healthy women to unnecessary procedures (minimize False Positives) \sphinxstyleemphasis{and} the desire to not fail to flag potentially cancerous scans. And that’s what I mean when I say that a \sphinxstyleemphasis{great} data scientist is one who is thoughtful about how their models make mistakes. Determining what model is “best” requires more than optimizing a simple objective function — it requires thinking critically about the problem one is trying to solve (Big Idea 1), the substantive impact of different types of model errors in the context of that problem, and using that to inform model selection and evaluation.




\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{What \sphinxstyleemphasis{is} Data Science: An Historical Perspective}
\label{\detokenize{10_introduction/40_data_science_in_historical_context:what-is-data-science-an-historical-perspective}}\label{\detokenize{10_introduction/40_data_science_in_historical_context::doc}}
\sphinxAtStartPar
Given how often the term “data science” gets thrown around, you would be excused for thinking that the meaning of the term was clearly understood. The reality, however, is that if you were to ask ten people working in the field you will almost certainly get ten different descriptions of what it is and what they do.

\sphinxAtStartPar
Part of that is deliberate obfuscation—data science is \sphinxstyleemphasis{so} trendy that everyone wants to claim that what they’re doing is data science in order to woo venture capitalists or to win research grants. Indeed, it has been said (half\sphinxhyphen{}joking, half\sphinxhyphen{}seriously): “Data science is \sphinxstyleemphasis{Artificial Intelligence} when you’re raising money, \sphinxstyleemphasis{Machine Learning} when you’re hiring, and it’s \sphinxstyleemphasis{Logistic Regression} when you actually have to get the job done.

\sphinxAtStartPar
But the ambiguity that surrounds the term “data science” is also the result of the fact that data science is not a mature discipline in the way that computer science, economics, or mechanical engineering are mature disciplines. And, as a young data scientist, that immaturity is important for you to understand, as it is both the source of some of the most exciting opportunities and also some of the biggest challenges you will face.


\section{The Organization of Academia, Data Science, and You}
\label{\detokenize{10_introduction/40_data_science_in_historical_context:the-organization-of-academia-data-science-and-you}}
\sphinxAtStartPar
To explain what the term data science means in practice, we have to start by discussing a bit of the inside\sphinxhyphen{}baseball%
\begin{footnote}[1]\sphinxAtStartFootnote
“\sphinxhref{https://en.wikipedia.org/wiki/Inside\_baseball\_(metaphor)}{Inside baseball}” refers to the discussion of the idiosyncracies and details of how an institution or system operates internally, something that is often not of interest to people who aren’t part of the system.
%
\end{footnote} of how academia operates. This may feel esoteric, but it’s important to understand because the way academia is organized has shaped the professional training — and thus the language and thought patterns — of most people you will encounter in the data science space. Understanding academia better, as a result, will not only help you understand the material you are exposed to in data science classes better, but also help you relate to your future peers and colleagues.

\sphinxAtStartPar
The idea that academia is deeply fragmented often surprises students, and understandably so. Universities \sphinxstyleemphasis{love} to pay lip service to the importance of interdisciplinarity and are quick to highlight successful interdisciplinary collaborations. But successful interdisciplinary collaborations are so notable precisely because they are the exception, not the rule. The reality is that academic research is starkly divided into disciplinary silos (e.g., computer science, statistics, political science, economics, and engineering). This isn’t because researchers aren’t \sphinxstyleemphasis{interested} in interdisciplinary collaborations, but rather that their professional imperatives push them to focus their attention on the priorities and language of their own departments and disciplines.%
\begin{footnote}[2]\sphinxAtStartFootnote
Nearly all university faculty are hired by established departments like statistics or economics, faculty submit their research to journals specific to their discipline, those journals in turn ask fellow members of the discipline to evaluate their work for publication, and promotions and tenure reviews are managed by the faculty in a faculty member’s own department.
%
\end{footnote}

\sphinxAtStartPar
Thus, while the past several decades have seen an unprecedented emergence of new methods across all of academia, the lack of intellectual cross\sphinxhyphen{}pollination across academic silos has resulted in disciplines failing to take full advantage of discoveries from other disciplines. Over time, each discipline has developed a perspective on computational methods that emphasizes its own intellectual priorities.

\sphinxAtStartPar
To illustrate, suppose we were interested in using patient data to reduce heart attacks. A computer scientist looking at this problem might use their discipline’s methods to \sphinxstyleemphasis{predict} which patients are most likely to experience a heart attack in the future using current patient data; a social scientist might focus on trying to understand the \sphinxstyleemphasis{effect} of giving patients a new drug on heart attack risk; and a statistician might focus on understanding \sphinxstyleemphasis{how confident} we should be in the conclusions reached by the computer scientist and social scientist.

\sphinxAtStartPar
This fragmentation has also resulted in a fragmentation of \sphinxstyleemphasis{language} around data science methodologies. Disciplines often come up with different terminology for the same phenomena, adding another layer of difficulty to efforts to work across departmental silos.

\sphinxAtStartPar
The result is a situation analogous to the Buddhist parable of the blind men and the elephant, wherein a group of blind people come upon an elephant, and upon laying hands on different parts of the elephant, they come to different conclusions about what lies before them. The person touching the tail declares “we have found a rope!”, while the person touching the leg declares “we have found a tree!”

\sphinxAtStartPar
\sphinxincludegraphics{{blindmenelephant}.jpg}

\sphinxAtStartPar
(\sphinxstyleemphasis{Note}: Not sure of original source of this image. \sphinxhref{https://pursuitofresearch.org/2011/01/19/the-blind-men-and-the-elephant/}{Found it here}, but need to figure out rights prior to anything about this becoming commercial! Lots of pics in public domain if needed, but not blindfolded scientists.)

\sphinxAtStartPar
And yet, as the poet John Godfrey Saxe wrote in his poem \sphinxhref{https://en.wikipedia.org/wiki/Blind\_men\_and\_an\_elephant\#John\_Godfrey\_Saxe}{\sphinxstyleemphasis{The Blind Men and the Elephant}} about this parable many centuries later:
\begin{quote}

\sphinxAtStartPar
And so these men of Indostan,
Disputed loud and long,
Each in his own opinion
Exceeding stiff and strong,
Though each was partly in the right,
And all were in the wrong!
\end{quote}

\sphinxAtStartPar
In recent years, however, there has been a growing appreciation of what can be gained from pulling together the insights that have been developed in different fields, despite the challenges of language and professional imperatives to such collaborations. And, at least amongst those who are serious about the development of data science as a discipline and not just a buzzword to use when raising money, is the promise of data science: to unify the different perspectives and methods for analyzing data. Or, to put it more succinctly: to finally see the whole elephant.

\sphinxAtStartPar
While the field is making progress towards “seeing the elephant as a whole,” however, as a result of this fragmented origin story, \sphinxstyleemphasis{most} people you will encounter in the world doing data science were trained in one of these academic silos. That means that depending on who you are working with and how they were trained, you may find your future colleagues using terms you’ve never heard before. And when that happens, it’s important to remember that while that \sphinxstyleemphasis{may} be because they’re talking about a concept you’ve yet to encounter, it may also simply be because they’re using different language for something you know. Similarly, you may also find senior colleagues unfamiliar with concepts that seem basic to you simply because you were exposed to perspectives that were alien to your colleague’s academic silo at the time they were trained. Indeed, given that data science education \sphinxstyleemphasis{is} finally becoming more unified, you should probably expect to learn a lot of ideas that even your more senior colleagues (or rather, especially your more senior colleagues!) were never exposed to.

\sphinxAtStartPar
And therein also lies some of the greatest opportunities. Precisely because of this intellectual fragmentation, there are \sphinxstyleemphasis{lots} of opportunities for taking insights from one intellectual silo and using them to solve problems in another — a kind of “intellectual arbitrage,” if you will.


\section{The Data Analyst / Software Engineering Distinction}
\label{\detokenize{10_introduction/40_data_science_in_historical_context:the-data-analyst-software-engineering-distinction}}
\sphinxAtStartPar
In addition to this broader intellectual fragmentation, the world of data science also often feels oddly fragmented around the way people use the tools of data science.

\sphinxAtStartPar
One model of data science is what we will call the “data analyst” approach. Data scientists doing this type of work often collect data to answer specific questions—what is the effect of expanded government health insurance subsidies on mortality? what type of customer should we target with our new advertising campaign?. As a result, when they write code, they write it to be run against a specific set of data to answer a specific question.

\sphinxAtStartPar
The other model is what we will call the “software engineering” approach. Data scientists doing this type of work write software they plan to \sphinxstyleemphasis{deploy} to thousands or millions of users. This is the type of work that gets embedded in the apps on your phone, or that generates your movie recommendations at Netflix. As a result, when these data scientists write code, they are writing more sophisticated and generalizable programs.

\sphinxAtStartPar
To be clear, most data scientists do at least a bit of both types of work—data analysts may often write small programs or packages to aid in types of analysis they do a lot, and software engineers have to prototype and test new programs before they write a version that can be deployed broadly. But most people will eventually choose to specialize in one direction or another, and when you see data science resources in the world—especially ones about programming for data science—bear in mind that depending on \sphinxstyleemphasis{your} proclivities towards on approach or another, not all resources will be well suited to your interests.

\sphinxAtStartPar
I also want to draw attention to this distinction because it’s remarkable how dismissive most data scientists will be of the “other” type of data science, and I want to encourage you to both (a)not be so tribal yourself (both flavors of data science have their place, and help solve real world problems!), and (b) not be too surprised when you encounter people with irrationally strong opinions about which approach is the “right” approach to doing data science.


\bigskip\hrule\bigskip


\sphinxstepscope


\part{Problems to Questions}

\sphinxstepscope


\chapter{Solving the Right Problem}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:solving-the-right-problem}}\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem::doc}}
\sphinxAtStartPar
If data science is the study of how to solve problems using quantitative methods, then the first — and arguably most important — stage in any data science project is to define the problem to be solved.

\sphinxAtStartPar
While this may seem simple, it is often far from it. Indeed, as noted in the introduction, problems often only appear complicated because they are poorly understood. Reframing or rearticulating a difficult problem is often the key to figuring out how to solve it, which is why the adage “A problem well stated is a problem half solved” has remained popular for so long.

\begin{sphinxShadowBox}

\sphinxAtStartPar
A problem well stated is a problem half\sphinxhyphen{}solved.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Charles Kettering, Head of Research at General Motors

\end{itemize}
\end{sphinxShadowBox}

\sphinxAtStartPar
In this chapter, we will discuss what happens when data scientists fail to appreciate the importance of this stage of a project. With that established, we will then turn to advice on how to approach problem articulation.

\sphinxAtStartPar
This chapter will be written as if you are the sole actor on a data science project, responsible for everything from problem articulation to execution. But of course, this will rarely be the case in your professional life as a data scientist, especially early in your career. With that in mind, in the next chapter we will turn to the topic of “Stakeholder Management” — the practice of refining your understanding of the problem to be solved with a stakeholder.


\section{If You Don’t Articulate The Problem}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:if-you-don-t-articulate-the-problem}}
\sphinxAtStartPar
There are many examples in the world of data science projects going awry because the team behind them failed to properly articulate the problem they wished to solve. Rather than start with one of those true stories, however, I’d like to begin with one of my favorite fictional (and humorous) examples of the phenomenon.

\sphinxAtStartPar
In Douglas Adams’ comedic sci\sphinxhyphen{}fi classic \sphinxstyleemphasis{Hitchhiker’s Guide to the Galaxy}, a race of hyperintelligent pandimensional beings set out to build a massive supercomputer the size of a city to solve the mysteries of the cosmos once and for all. When they turned on the computer, named Deep Thought, they announced that:
\begin{quote}

\sphinxAtStartPar
“The task we have designed you to perform is this. We want you to tell us… the Answer!”

\sphinxAtStartPar
“The Answer?” said Deep Thought.

\sphinxAtStartPar
“The Answer to what?”

\sphinxAtStartPar
“Life!” urged one designer.

\sphinxAtStartPar
“The Universe!” said another.

\sphinxAtStartPar
“Everything!” they said in chorus.

\sphinxAtStartPar
Deep Thought paused, then answered, “Life, the Universe, and Everything. There is an answer. But,” Deep Thought added, “I’ll have to think about it.”
\end{quote}

\sphinxAtStartPar
Seven and a half million years later, when Deep Thought had \sphinxstyleemphasis{finally} finished its calculations, the descendants of those designers assembled to learn the result of their ancestors’ work.
\begin{quote}

\sphinxAtStartPar
“Er …good morning, O Deep Thought,” said Loonquawl {[}one descendant{]} nervously, “do you have … er, that is …”

\sphinxAtStartPar
“An answer for you?” interrupted Deep Thought majestically. “Yes. I have.”

\sphinxAtStartPar
The two {[}descendants{]} shivered with expectancy. Their waiting had not been in vain.

\sphinxAtStartPar
“There really is one?” breathed Phouchg {[}the other descendant{]}.

\sphinxAtStartPar
“There really is one,” confirmed Deep Thought.

\sphinxAtStartPar
“To Everything? To the great Question of Life, the Universe and Everything?”

\sphinxAtStartPar
“Yes. {[}…{]} Though I don’t think,” added Deep Thought, “that you’re going to like it.”

\sphinxAtStartPar
{[}…{]}

\sphinxAtStartPar
“All right,” said the computer, and settled into silence again.

\sphinxAtStartPar
The two fidgeted.

\sphinxAtStartPar
The tension was unbearable.

\sphinxAtStartPar
“Forty\sphinxhyphen{}two,” said Deep Thought, with infinite majesty and calm.

\sphinxAtStartPar
“Forty\sphinxhyphen{}two!” yelled Loonquawl. “Is that all you’ve got to show for seven and a half million years’ work?”

\sphinxAtStartPar
“I checked it very thoroughly,” said the computer, “and that quite definitely is the answer. I think the problem, to be quite honest with you, is that you’ve never actually known what the question is.”

\sphinxAtStartPar
“But it was the Great Question! The Ultimate Question of Life, the Universe and Everything,” howled Loonquawl.

\sphinxAtStartPar
“Yes,” said Deep Thought with the air of one who suffers fools gladly, “but what actually is it?”

\sphinxAtStartPar
A slow stupefied silence crept over the men as they stared at the computer and then at each other.

\sphinxAtStartPar
“Well, you know, it’s just Everything … everything …” offered Phouchg weakly.

\sphinxAtStartPar
“Exactly!” said Deep Thought. “So once you do know what the question actually is, you’ll know what the answer means.”%
\begin{footnote}[1]\sphinxAtStartFootnote
Yes, I recognize that it is wildly indulgent to open a chapter with such a long epigraph. But it’s my book, and if there’s anything to be indulgent about its quotes from \sphinxstyleemphasis{Hitchhiker’s Guide to the Galaxy}, damn it!
%
\end{footnote}
\end{quote}

\sphinxAtStartPar
In addition to establishing the premise for one of the greatest comedic science fiction novels in human history,%
\begin{footnote}[2]\sphinxAtStartFootnote
Not least for being the only 5\sphinxhyphen{}book trilogy of which I am aware!
%
\end{footnote} I feel this passage perfectly exemplifies the three reasons most data science projects fail.

\sphinxAtStartPar
\sphinxstylestrong{The first is our absolute faith that technology will save us.} All we have to do to solve any problem is feed it into the newest, shiniest AI/LLM/ML model available. Sure, there may be technical challenges associated with configuring the environment, formatting the data, etc., but fundamentally, all that lies between us and success is putting the problem into technology’s hands.

\sphinxAtStartPar
The second is that when a data science project fails, it’s rarely because the technology itself failed. Rather, \sphinxstylestrong{projects usually fail because people failed to ensure that the task they asked their model to accomplish would actually solve their problem.} Technology doesn’t care if the task it’s been given is useful to the user. It will do what it has been asked to do — no more and no less. Garbage in, garbage out.

\sphinxAtStartPar
Finally, this passage also gives a nod to the third — and perhaps most important reason — that data science projects fail: \sphinxstylestrong{data scientists are far too deferential to their stakeholders.} In this passage, we can see that Deep Thought recognizes the idiocy of the request it has been given by its stakeholders — Loonqual and Phouchg — but does nothing about it. And to the degree to which we can think of the Deep Thought personality as a stand\sphinxhyphen{}in for the data scientist, this is a troubling realistic illustration of how many data science interactions go. The stakeholder says they want something and the (usually younger) data scientist assumes their only responsibility is to ensure the stakeholder’s request is implemented.

\sphinxAtStartPar
But as discussed in the introduction of this book, your success as a data scientist will \sphinxstyleemphasis{always} be evaluated in terms of whether you’ve made your stakeholder’s life better. And if doing precisely what they ask does not improve their lives, that’s the only thing that will matter. No one is blaming Deep Thought in this story, but they sure aren’t excited about what it did, either.


\section{Articulating and Reframing Your Problem}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:articulating-and-reframing-your-problem}}
\sphinxAtStartPar
The preceding story gives (an admittedly extreme) example of how things can go wrong when a data science project is not well motivated — in short, large amounts of energy and sweat can go into creating technically impressive results that, in the end, in no way solve a problem or improve the lives of anyone involved.

\sphinxAtStartPar
How, then, can you avoid that fate? Every problem is different, but here are some strategies to employ when faced with a problem to aid in refinement, reframing, and articulation. As we work through these, we will also work a few examples based on real cases designed to help you get a feel for what this process entails and why it’s so important.


\subsection{1. How do you know if you’ve been successful?}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:how-do-you-know-if-you-ve-been-successful}}
\sphinxAtStartPar
One of the most helpful questions to ask when trying to understand a problem is “how would we know if we’ve successfully solved this problem?” This question is powerful because it abstracts away the question of \sphinxstyleemphasis{how} you might solve a problem, and instead focuses attention on the \sphinxstyleemphasis{goal} one is trying to achieve. In other words, the objective of this question is to figure out how you might \sphinxstyleemphasis{measure} success.

\sphinxAtStartPar
Crucially, the answer to this question shouldn’t be something like “we have a good model for doing X” — that answer has largely just shifted the question to what constitutes “good.” Rather, try to come up with an answer in terms of the \sphinxstyleemphasis{metric} or \sphinxstyleemphasis{behavior} that, if observed, would tell you that you’ve been successful.

\sphinxAtStartPar
To illustrate the potential power of this question, let’s consider an example I’ve encountered in the real world (with some details changed to protect the anonymity of everyone involved).

\sphinxAtStartPar
The stakeholder is an online auction site for used construction equipment (front loaders, backhoes, etc.). They want to improve the algorithm that suggests prices to people selling equipment, but they’ve realized that while they have good data on how machinery type impacts prices, their algorithm can’t take into account the condition of equipment being listed. So they’ve hired a data science team to train an image classification model on listings to estimate the condition of each listed piece of machinery.

\sphinxAtStartPar
So: how would you know if you’ve been successful in solving this stakeholder’s problem?

\sphinxAtStartPar
At first blush, you might say I would know I was successful if I have a model that has high classification accuracy for equipment condition when fed photos of equipment listed in the past.

\sphinxAtStartPar
But look more carefully, is that really what the stakeholder cares about? No — because “image classification model” is something young data scientists are familiar with, they tend to latch on to it immediately. “Oh, great, image classification! I can do that. Let’s go!”

\sphinxAtStartPar
No, the problem motivating the stakeholder is that their price suggestion algorithm doesn’t take into account the condition of equipment going up for sale. Given that, you would know your model was successful if the predicted prices from the model were more in line with final sale prices for equipment of all conditions. \sphinxstyleemphasis{That’s} the actual goal.

\sphinxAtStartPar
Yes, an image classification model might allow the stakeholder to estimate equipment condition which could then be used as training data to revise the pricing model. But is that the best approach?

\sphinxAtStartPar
To train a model that can differentiate construction machinery in good or bad condition, the first thing you’d need to do is have a person go through and label a lot of old listings as being in good or bad condition. And you’d probably need a lot of them — after all, your model would have to be able to differentiate between indicators of wear that are superficial (dirt, superficial rust, and peeling paint) and indicators of wear that are substantial (cracks, patch welds, etc.), and do so on a range of different types of construction equipment. Then you’d use that to train an image classifier, then you’d have to tune and validate that image classifier, then you could use its classifications to improve the pricing model.

\sphinxAtStartPar
Alternatively, you could just (a) have someone label some old listings and use those labels \sphinxstyleemphasis{directly} to refine the pricing model, and (b) add a drop\sphinxhyphen{}down menu that asks people listing equipment to report the condition of their equipment (so that data is available to the pricing model when someone goes to list a product). Congratulations! You’ve just \sphinxstyleemphasis{entirely} removed image classification from this task.


\subsection{2. Abstract the Problem}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:abstract-the-problem}}
\sphinxAtStartPar
A second strategy for better understanding a problem is to try to reframe it at a higher level of generality/abstraction. When you do, you may find your thorny problem is actually just a specific example of a more general type of problem, one that people smarter than either of us have already figured out how to solve.

\sphinxAtStartPar
Alternatively, you may realize that you or your stakeholder has (often unknowingly) introduced constraints to the problem that aren’t actually constraints.

\sphinxAtStartPar
Perhaps my favorite example of this phenomenon comes from a talk given by \sphinxhref{https://youtu.be/kYMfE9u-lMo?t=1281\&amp;si=haO8mlmO5tB4OC9k}{Vincent Warmerdam at PyData 2019.}

\sphinxAtStartPar
The World Food Program (WFP) is a global leader in food aid provision. As Vincent tells the story — which he reports having heard at an Operations Research Conference — the WFP was struggling with an extremely difficult data science problem: how best to get food from the places it was being grown/stored to the people who needed it most. Essentially, the WFP would receive reports of needs from communities facing food insecurity. One community might report a need for bread and beef, while another might request lentils and meat. The WFP would compile these requests and then set about trying to determine the most efficient way to meet these needs.

\sphinxAtStartPar
This type of logistics problem is an example of a notoriously difficult problem (essentially a version of the Traveling Salesman Problem, which is NP\sphinxhyphen{}Complete, if that means anything to you) that companies like FedEx and UPS buy supercomputers to address. But this particular problem was made extra challenging by all the different types of food the WFP was trying to provide communities.

\sphinxAtStartPar
What the WFP realized was that they didn’t actually need to provide bread to the village asking for bread. See, humans don’t need \sphinxstyleemphasis{bread} to avoid starvation — they need a certain number of calories, a certain amount of protein, and a handful of other nutrients.%
\begin{footnote}[3]\sphinxAtStartFootnote
As I understand it, calcium, iron, vitamins A, B1, B2, C, and niacin.
%
\end{footnote} So when a village asks for bread, rice, or wheat, you can instead think of them asking for carbohydrates. And a village asking for beef or beans is actually asking for protein and iron. So by simply abstracting the task from “How best can we meet all these food requests?” to “How best can we meet the nutritional needs indicated by these requests?” the WFP was able to \sphinxstyleemphasis{dramatically} reduce the number of constraints being imposed on the logistical optimization problem WFP needed to solve, making its task \sphinxstyleemphasis{far} simpler.

\sphinxAtStartPar
How far should you abstract things? To the level that feels most useful. Yes, in the private sector you \sphinxstyleemphasis{can} abstract almost any problem to “how do we maximize the lifetime discounted profits of the company,” but I don’t think it would be controversial to say that that formulation is not particularly useful if you’re trying to pick the best box sizes for your company to stock for online orders. But there’s also no real cost to thinking about your problem at that level of generality for a little while, so my advice would be: if you haven’t gotten to a level of abstraction that feels silly, you probably haven’t abstracted enough.


\subsection{3. Let Your Articulation Change}
\label{\detokenize{20_problems_to_questions/10_solving_the_right_problem:let-your-articulation-change}}
\sphinxAtStartPar
My last suggestion is to allow your understanding of the problem you are trying to solve evolve. As your project develops, you will learn new things about your problem context, and you should allow those to inform how you are thinking of your problem. This is true not only for you — the data scientist — but also your stakeholder. We’ll discuss stakeholder management more in our next chapter, but you should regularly be asking them: “does the way we’ve articulated the problem we’re trying to solve still feel right to you given everything we’ve learned?”


\bigskip\hrule\bigskip


\sphinxstepscope


\section{What Solving the \sphinxstyleemphasis{Wrong} Problem Looks Like}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:what-solving-the-wrong-problem-looks-like}}\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem::doc}}

\subsection{Recruitment}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:recruitment}}

\subsection{Outliers}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:outliers}}

\subsection{Pizza Ads}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:pizza-ads}}
\sphinxAtStartPar
Our discussion up to this point has been a little abstract, so to illustrate what it means to “mis\sphinxhyphen{}specifying a problem.” The details of this example are fictitious, but the underlying logic of this example is not; indeed, the insight illustrated by this example is central to one of the biggest pivots in how people think about online advertising.

\sphinxAtStartPar
You have been hired by the advertising division of a fictitious national pizza chain—let’s call it Little Papa Dominos (LPD). LPD spends a \sphinxstyleemphasis{lot} on online advertising, but their resources aren’t being deployed as effectively as they could be. They spend more than most of their competitors, and yet their online sales are lagging.

\sphinxAtStartPar
After consulting industry groups and online advertising experts, they discovered that the rate at which people click their ads (their ads’ \sphinxstyleemphasis{click\sphinxhyphen{}through rate}, or CTR) is well below the industry average.

\sphinxAtStartPar
To address the problem, they’ve hired you—a newly minted Data Scientist—to improve the CTR of their ads. They give you a large budget, access to all the cloud computing resources you need, and even a small staff.

\sphinxAtStartPar
“Well,” you reason, “maybe the problem is that our ads aren’t being shown to the right people. After all, it seems unlikely that any ad for pizza—no matter how appealing—is likely to draw a click if it’s shown to a 75\sphinxhyphen{}year\sphinxhyphen{}old at 7 am.” So you set out to build a statistical model to answer the question, “given a user’s demographics and online behavior, how likely are they to click on one of LPDs ads?” If you can answer that, you figure, LPD can prioritize buying the ad spots for the types of users most likely to click on their ads.

\sphinxAtStartPar
To develop that model, you use your budget to run your ads on different sites and at different times. You then use that data (and those glorious cloud computing resources) to train a machine learning model that predicts whether someone will click on one of your ad based on the user’s demographics and ad placement. You try out a few different models, tune the model parameters, and eventually settle on a neural network model with extremely high precision \sphinxstyleemphasis{and} recall. Hooray!

\sphinxAtStartPar
LPD uses the model to target users likely to click their ads, and almost immediately the CTR of their ads increases 5\sphinxhyphen{}fold! Not only that, but the share of people who click on ads that go on to buy a pizza has also increased. Everyone congratulates you, and you move on to the next project feeling very smug.

\sphinxAtStartPar
A few months later, though, you are called into a meeting with the LPD advertising team and the company’s Chief Financial Officer. They’ve been looking over the numbers, and despite the huge rise in CTR, they seem to be getting fewer online orders than before you arrived. CTR rates are up, but somehow it isn’t generating greater profits.

\sphinxAtStartPar
Can you figure out what went wrong?

\sphinxAtStartPar
OK, this is the place in most books where the authors ask you that question, and you look up at the ceiling for a minute, shrug, and then read on.

\sphinxAtStartPar
But I’m really, \sphinxstyleemphasis{really} serious about this: close your laptop, stand up, set a 5\sphinxhyphen{}minute timer on your phone, and go for a walk. Ponder this example. See if you can figure out what’s going on. This is \sphinxstyleemphasis{precisely} the kind of problem you will soon face as a professional data scientist, so why not practice trying to think through the problem?


\subsection{Solving The Wrong Problem}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:solving-the-wrong-problem}}
\sphinxAtStartPar
So what happened?

\sphinxAtStartPar
The reason an increased click rate wasn’t making LPD richer is that LPD’s problem was never the fact they had a low CTR; LPD’s \sphinxstyleemphasis{real} problem was that they weren’t getting a lot of orders online. And because Little Papa Domino’s problem wasn’t a low CTR, being able to answer the question “How likely is a given user to click on an ad” \sphinxstyleemphasis{didn’t actually solve their real problem}.

\sphinxAtStartPar
What question, if answered, would have helped solve their problem? “Given a user’s demographics and online behavior, \sphinxstyleemphasis{how much more likely are they to buy a pizza} from LPD if we show them an ad?”

\sphinxAtStartPar
Or, expressed more succinctly, LPD \sphinxstyleemphasis{thought} their problem was that their ads weren’t \sphinxstyleemphasis{getting clicks,} but really their problem was that their ads weren’t \sphinxstyleemphasis{driving increased sales}.

\sphinxAtStartPar
The difference is subtle, but critically important: someone clicking an ad doesn’t make Little Papa Dominos any money. Someone clicking an ad \sphinxstyleemphasis{and ordering a pizza} doesn’t necessarily make LPD any money. Why? Because they may be someone who would have bought a pizza from LPD anyway, whether you showed them an ad or not. The person who was already thinking of ordering a pizza from LPD is \sphinxstyleemphasis{precisely} the type of person your algorithm may have targeted, and who may have clicked the ad to save themselves a Google search!

\sphinxAtStartPar
But the person LPD \sphinxstyleemphasis{wants} to show an ad to isn’t the person who was already thinking of ordering a pizza from LPD, it’s the person who was thinking of a pizza but wasn’t sure who to order it from, or the person who wanted dinner but didn’t know what to get. They may be less likely to click the ad than the person who was about to Google “Little Papa Dominos,” but their precisely the type of user who is more likely to buy a pizza from LPD as a result of seeing an ad than they would have been otherwise.


\subsubsection{Counter\sphinxhyphen{}Factual Advertising}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:counter-factual-advertising}}
\sphinxAtStartPar
Lest you think this example is contrived, it’s not. The realization that the goal of ads isn’t to maximize clicks but rather to induce the largest possible change in purchasing behavior is one of the most important ideas in online advertising. It has had a huge impact on how online advertising works, and how people evaluate the success of ad campaigns.

\sphinxAtStartPar
Indeed, this is why companies like Meta and Google are so eager to track user behavior across apps and websites. When Meta and Google can “follow” users after they’ve clicked an ad, they can evaluate ad performance based not on clicks but on customer behavior. When paired with their ability to show ads to some users and not to others and track both groups as they move around the web, Meta and Google can see whether users who see the ads are more likely to make purchases than those who don’t. This allows them to estimate the true effect of ads on sales, data they use to improve ad targeting \sphinxstyleemphasis{and} justify higher prices to advertisers.


\subsection{Next Up: Types of Questions}
\label{\detokenize{20_problems_to_questions/15_solving_the_wrong_problem:next-up-types-of-questions}}
\sphinxAtStartPar
Having established the importance of first articulating the problem one seeks to solve, we will shortly turn to developing our understanding of the three types of questions introduced in the first chapter of this book.

\sphinxAtStartPar
First, though, a quick digression into understanding the historical context of data science. This may feel like an odd topic to talk through in a technical data science text, but as we’ll see understanding how we got to where we are today is key to successfully navigating modern data science.

\sphinxstepscope


\chapter{Stakeholder Management}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:stakeholder-management}}\label{\detokenize{20_problems_to_questions/20_stakeholder_management::doc}}
\sphinxAtStartPar
stuff


\section{Step 0: Recognize Your Role}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:step-0-recognize-your-role}}
\sphinxAtStartPar
If you remember nothing else from this chapter, please remember this: helping your stakeholder better understand their problem is a core part of the job.

\sphinxAtStartPar
Because most stakeholders are older and domain experts in their field, young data scientists tend to err on the side of deference. It is important to be respectful of your stakeholder’s experience and to use their domain expertise, but it is important to also recognize that data science is about \sphinxstyleemphasis{pairing} domain expertise with computational methods and quantitative insights, and neither you nor your stakeholder is likely to have expertise in \sphinxstyleemphasis{both} the substantive domain in question \sphinxstyleemphasis{and} cutting edge quantitative methods. Indeed, if they did, they probably wouldn’t be hiring you!%
\begin{footnote}[1]\sphinxAtStartFootnote
Obviously there are exceptions to this — if you work for a mature tech company like Google or Meta, you may very well end up working under a manager who knows sides of a problem significantly better than you. In my experience, however, is circumstance is the exception, not the rule.
%
\end{footnote} So don’t hesitate to speak up! Ask questions, raise concerns, and while you should do so with \sphinxstyleemphasis{some} humility, have confidence in your own expertise.


\section{Step 1: Don’t Assume Your Stakeholder Knows What They Need}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:step-1-don-t-assume-your-stakeholder-knows-what-they-need}}
\sphinxAtStartPar
A corollary to Step 0 is to not assume your stakeholder understands what they need. So when I say “helping your stakeholder understand their problem is a core part of the job,” I don’t only mean that it’s part of your job \sphinxstyleemphasis{if the stakeholder admits to deep uncertainty about their problem}.” Odds are your stakeholder will come to you with a strong statement of what they think they want, but you should take that as a starting point for discussion, not your mandate.

\sphinxAtStartPar
This is particularly true if your stakeholder comes to you with really specific technical suggestions. Often you will be approached by a stakeholder who, rather than laying out a problem, announces they would like you to do X using some data science tool Y. Occasionally the stakeholder doing this knows exactly what they’re talking about, and you should use Y to do X.

\sphinxAtStartPar
More often, however, you’re dealing with a stakeholder with just enough knowledge to be dangerous (and to drop buzzwords), but not enough to know how best to solve their problem.

\sphinxAtStartPar
Most people ask data scientists for help because they don’t know much about data science (or, worse, they \sphinxstyleemphasis{think} they know about data science but don’t). Again, different rules apply if you’re at Google or Apple, but in most contexts, it’s a good idea to treat implementation details provided by the client as a red herring. Focus on the stakeholder’s \sphinxstyleemphasis{needs}. Only get into implementation details once you feel you understand the problem well.

\begin{sphinxShadowBox}

\sphinxAtStartPar
Focus on the stakeholder’s \sphinxstyleemphasis{needs}. Only get into implementation details once you feel you understand the problem well.
\end{sphinxShadowBox}


\section{Step 3: Ask Questions (Especially Quantitative Ones!)}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:step-3-ask-questions-especially-quantitative-ones}}
\sphinxAtStartPar
Be sure to ask a lot of questions of your stakeholder. In particular, I would suggest two types: questions about what success would look like, and questions about the problem itself.


\subsection{Questions About Success}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:questions-about-success}}
\sphinxAtStartPar
Getting a sense of where the goalposts are for your stakeholder will both help you know what to target and also help you better understand your stakeholder’s understanding of the problem. Make sure to ask questions like:
\begin{itemize}
\item {} 
\sphinxAtStartPar
How are you measuring the problem? What would you measure to help you know if you were successful in solving the problem?

\item {} 
\sphinxAtStartPar
How big, in quantitative terms, is this problem?

\item {} 
\sphinxAtStartPar
How much would you need the current situation to change to call this a success?

\end{itemize}


\subsection{Questions About the Problem}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:questions-about-the-problem}}
\sphinxAtStartPar
The more you know about your client’s needs the better, so ask anything that comes to mind. If the client can answer your question, it will help you better understand the situation; if the client can’t answer your question you may find that they are suddenly really interested in knowing the answer, and you immediately have some of your first Exploratory Questions to try to resolve.

\sphinxAtStartPar
In the example of the company that wanted to improve recruitment of high\sphinxhyphen{}quality employees in the introduction of this book, I suggested that some of the first exploratory questions you might want to investigate would be things like:
\begin{itemize}
\item {} 
\sphinxAtStartPar
How many job applications are you receiving when you post a job?

\item {} 
\sphinxAtStartPar
What share of your current job applicants are of high quality?

\item {} 
\sphinxAtStartPar
What share of employees you try to hire accept your offer?

\item {} 
\sphinxAtStartPar
What share of employees you do hire turn out to be successful employees?

\end{itemize}

\sphinxAtStartPar
These are all questions that I would ask my stakeholder in one of our first meetings.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Stakeholder Meetings
It is always good to go into meetings with your stakeholder with a clear sense of your objectives — what you hope to communicate, and what information and feedback you need to get before the meeting ends. When your stakeholder is someone you don’t get to meet with regularly, it’s good practice to detail these objectives and provide them — in writing — to your stakeholder in advance of your meeting. This will not only ensure that you and your teammates are on the same page (as you will all have reviewed the document before sending it to your stakeholder), but also ensure that your stakeholder has adequete time to reflect on any questions or issues you wish to raise.

\sphinxAtStartPar
When it comes to your \sphinxstyleemphasis{first} meeting, however, this practice can feel impractical as you may feel so uncertain about the project that you only know the first few questions you want to ask.

\sphinxAtStartPar
But even in a first meeting, preparation is key. Rather than laying out the new issues you wish to raise and questions you want answered, for a first meeting it’s helpful to write out a full \sphinxstyleemphasis{tree} of lines of inquiry you may wish to propose. In other words, for every question you wish to pose to your stakeholder, try to anticipate some likely responses they make provide, then write down a few followup questions to ask if they provide one of those responses.

\sphinxAtStartPar
Time with your stakeholder is \sphinxstyleemphasis{precious}, especially early in a project, make the most of that face time through preparation.
\end{sphinxadmonition}


\section{Step 4: Propose Questions You Might Answer}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:step-4-propose-questions-you-might-answer}}
\sphinxAtStartPar
As a data scientist, answering questions about the world is the instrument you have to solve problems. So once you think you have a sense of your stakeholder’s needs, turn around and propose a handful of questions and ask them if answering those questions would help solve their problem.

\sphinxAtStartPar
This is important because many people have only a vague sense of what they are likely to get as a “deliverable” from the data scientist. They usually have a vague sense that they will get some type of magic machine (a “magic model” or “magic algorithm”) that will just make their problem go away. By concretely framing your deliverable as the answer to a question (or a model that would answer a specific question for each entity like a customer or patient that it encounters), you can get much more valuable feedback before you dive into a problem.


\subsection{Make Your Questions Specific and Actionable}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:make-your-questions-specific-and-actionable}}
\sphinxAtStartPar
In developing your questions, it is important to make them specific and actionable. A specific and actionable question makes it very clear what you need to do next. For example, suppose an international aid organization told you they were worried that urbanization in Africa, Asia, and Latin America was impacting efforts to reduce infant mortality. Some examples of specific, actionable questions are: “Is infant mortality higher among recent migrants to urban centers, controlling for income?” or “Are the causes of infant mortality among recent migrants to urban centers different from those living in rural areas?” Reading those questions, you can probably immediately think of what data you’d need to collect, and what regressions you’d want to run to generate answers to those questions.

\sphinxAtStartPar
Vague questions would be “Is urbanization impacting efforts to reduce infant mortality?”, or “Does urbanization affect infant mortality?” Note that when you read these, they don’t seem to obviously imply a way forward.

\sphinxAtStartPar
Perhaps the best way to figure out if your question is answerable is to write down what an answer to your question would look like. Seriously – try it. Can you write down, on a piece of paper, the graph, regression table, or machine learning diagnostic statistics (complete with labels on your axes, names for variables, etc.) that would constitute an answer to your question? If not, it’s probably too vague.


\section{Step 5: Iterate}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:step-5-iterate}}
\sphinxAtStartPar
And here’s the last but perhaps most important step: \sphinxstylestrong{iterate.} Bring your work back to your stakeholder as often as possible.

\sphinxAtStartPar
Many stakeholders find the idea of data science mysterious and abstract and will struggle to understand what is and is not feasible. By bringing them intermediate results, the whole process will start to become more concrete for the stakeholder, and it will help them provide you with better feedback.

\sphinxAtStartPar
The way this book is organized suggests a natural flow from problem articulation to answering Exploratory Questions to prioritize efforts, to answering Passive\sphinxhyphen{}Prediction Questions to target individuals for extra attention or automate tasks, and finally to Causal Questions to better understand the effects of that extra attention/automation. In reality, however, a good data scientist is always coming back to the stakeholder, updating their plan, and jumping back in the sequence when new questions arise.


\section{Reading Reflection Questions}
\label{\detokenize{20_problems_to_questions/20_stakeholder_management:reading-reflection-questions}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Why should you care if your stakeholder misspecifies their problem?

\end{itemize}


\bigskip\hrule\bigskip


\sphinxstepscope


\part{Types of Questions}

\sphinxstepscope


\chapter{Descriptive v. Prescriptive Questions}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:descriptive-v-prescriptive-questions}}\label{\detokenize{30_questions/05_descriptive_v_prescriptive::doc}}
\sphinxAtStartPar
In the chapters that follow, we will discuss Exploratory, Passive\sphinxhyphen{}Prediction and Causal in detail. First, though, we must discuss another important concept: the distinction between descriptive and prescriptive questions.

\sphinxAtStartPar
\sphinxstyleemphasis{Descriptive Questions} are questions about the state of the world and include all the questions and examples we’ve covered so far in this book. “What kinds of users are clicking our ads?” and “Do high\sphinxhyphen{}income and low\sphinxhyphen{}income countries emit similar amounts of carbon dioxide?” are examples of Descriptive Questions. And because Descriptive Questions are questions about objective reality,%
\begin{footnote}[1]\sphinxAtStartFootnote
If you know enough epistemology to object to me asserting the existence of an “objective reality,” then I assume you can also understand the point I’m trying to get across in this chapter and will forgive me this philosophical slight.
%
\end{footnote} they have right and wrong answers (at least in principle. It may be hard to evaluate whether a given attempt to calculate the answer is actually right or wrong).

\sphinxAtStartPar
But Descriptive Questions are not the only type of question you will come across in your career.

\sphinxAtStartPar
\sphinxstyleemphasis{Prescriptive Questions} are questions about how the world \sphinxstyleemphasis{should be}, not how it actually is. “Should higher income and lower income countries be expected to meet the same carbon emission reduction standards?” or “Do high\sphinxhyphen{}income countries have a moral obligation to provide tuberculosis drugs to developing countries for free (or at cost)?” are both examples of Prescriptive Questions.

\sphinxAtStartPar
Unlike Descriptive Questions, Prescriptive Questions don’t have correct answers. That’s because answers to Prescriptive Questions require evaluating the \sphinxstyleemphasis{desirability} of possible outcomes, which can only be done in the context of a moral/ethical system of values. And as there is no “correct” system of values (in the sense that there is no single universally accepted system of morality), there can be no right or wrong answers to Prescriptive Questions, even in principle.

\sphinxAtStartPar
\sphinxincludegraphics{{question_tree}.png}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The terms “Prescriptive” and “Descriptive” are commonly used for these concepts in the natural sciences, but different academic silos sometimes use different terms. Social scientists, for example, tend to prefer the terms “Positive” in place of Descriptive and “Normative” instead of Prescriptive. These are only difference in nomenclature, however, not substantive meaning.
\end{sphinxadmonition}

\sphinxAtStartPar
The focus of this book is on Descriptive Questions. This is not because Prescriptive Questions are unimportant — indeed, one can easily make the argument that they are \sphinxstyleemphasis{more} important than Descriptive Questions. Moreover, as we will discuss in future chapters, they will arise frequently in your career as a data scientist. No, the reason that Descriptive Questions are the focus of this book is that those are the only questions data science tools can answer, and thus answering Descriptive Questions is the domain in which the data scientist has a clear comparative advantage.

\sphinxAtStartPar
Now, to be clear, none of this is to mean that the answers you generate as a data scientist will not have a \sphinxstyleemphasis{bearing} on how people answer Prescriptive Questions. Data science would be a very dull field indeed if it could not speak to the ethical issues of our day. Data science is powerful precisely because it can inform how we answer Prescriptive Questions by helping us understand the relevant stakes. Data science tools can help decision\sphinxhyphen{}makers understand the likely \sphinxstyleemphasis{consequences of different courses of action}, information that can help people make \sphinxstyleemphasis{informed} decisions about what outcomes they feel are most desirable. To illustrate, let’s consider a few vignettes.


\section{Opioid Reductions}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:opioid-reductions}}
\sphinxAtStartPar
Suppose you have been hired by a medical regulatory board concerned about the rise in opioid overdoses. They are debating whether they \sphinxstyleemphasis{should} (there’s that magic word!) make it harder for patients to get opioids. Fundamentally, however, they worry that while restrictions on opioids may reduce overdoses and addiction, they may also prevent some patients with very real pain conditions from getting the care they need.

\sphinxAtStartPar
Why are they stuck? Well, there may be two causes:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
\sphinxAtStartPar
They may be unsure of the relative moral weight to give preventing overdoses versus ensuring appropriate patient access to opioids, and/or

\item {} 
\sphinxAtStartPar
they may also be unsure about \sphinxstyleemphasis{how much} opioid regulations that reduce overdoses by a certain amount would limit access for patients in need.

\end{enumerate}

\sphinxAtStartPar
The first of these questions is a pure Prescriptive Question — if you could prevent one overdose death at the expense of preventing 10 patients in pain from getting the opioids they need, would you accept that trade\sphinxhyphen{}off?

\sphinxAtStartPar
But the second is actually a Descriptive Question that you — the data scientist — \sphinxstyleemphasis{can} answer! You could study policies that have been implemented in the past and come up with a rigorous estimate of how much opioid regulations that reduce overdoses also reduce access for patients in need. You could also evaluate different kinds of policies to figure out which is most efficient — maybe some policies (like not allowing any opioid prescriptions at all) are good at stopping overdose deaths but also \sphinxstyleemphasis{really} limit appropriate access, while other policies are similarly good at reducing overdoses but have a much smaller effect on limiting access.


\section{The Example of Carbon Emissions}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:the-example-of-carbon-emissions}}
\sphinxAtStartPar
A profoundly difficult Prescriptive Question in debates over carbon reduction is whether developing countries should be held to the same emission reduction targets as more developed countries. On the one hand, developing countries like China and India are the source of most current growth in carbon emissions, and so policies that do not apply to developing countries are unlikely to prevent many of the worst climate change outcomes. On the other hand, these countries produce radically less carbon \sphinxstyleemphasis{per capita} than Europe or the United States, and the industrial growth creating those emissions has been a major factor in lifting billions of people out of extreme poverty.

\sphinxAtStartPar
Hard choices indeed! How does one weigh the improvements in the quality of life of those in extreme poverty against the possible consequences of even greater climate catastrophes?

\sphinxAtStartPar
While that question, in part, is a Prescriptive Question that no regression can answer, data scientists \sphinxstyleemphasis{can} bring data to bear on this question indirectly by helping everyone understand the potential consequences of different carbon targets for developing countries, as well as the feasibility of different strategies for carbon reduction. A data scientist could, for example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Evaluate the effectiveness of different messages politicians in the US and Europe could use to convince their constituents to support greater carbon reduction targets,

\item {} 
\sphinxAtStartPar
Quantify the magnitude of the effect on global warming caused by different emissions targets for developing countries to help politicians in developing countries weigh the poverty\sphinxhyphen{}reducing benefits of carbon\sphinxhyphen{}intensive industrialization against the likely direct effect of flooding, droughts, or more severe storms on their own citizens, or

\item {} 
\sphinxAtStartPar
Estimate the cost\sphinxhyphen{}effectiveness of developed countries sharing lower emissions industrial technologies with developing countries to ameliorate the tradeoff between poverty reduction and emissions.

\end{itemize}

\sphinxAtStartPar
In each of these cases, the data scientist is only answering Descriptive Questions, but in doing so they are helping everyone better understand the consequences of their decisions, and in doing so (hopefully) help the world to make more informed decisions about the trade\sphinxhyphen{}offs they are making.


\section{Recap}
\label{\detokenize{30_questions/05_descriptive_v_prescriptive:recap}}
\sphinxAtStartPar
Answering Descriptive Questions — questions about how the world is or would be in different scenarios — is the core competency of the data scientist. In the chapters that follow, we will explore in detail three different kinds of Descriptive Questions: Exploratory, Passive\sphinxhyphen{}Prediction, and Causal Questions.

\sphinxAtStartPar
While these are the only types of questions that data science tools can answer directly, it is important for you, the data scientist, to also recognize when you encounter Prescriptive Questions — that is, questions about how the world \sphinxstyleemphasis{should} be, or what we \sphinxstyleemphasis{ought} to do. These questions can only be answered with respect to a system of values, and as such, do not have right or wrong answers, and cannot be answered by statistical means. Nevertheless, as a data scientist, you are well\sphinxhyphen{}prepared to help others (and yourself!) make more informed choices when they decide how to answer Prescriptive Questions for themselves.


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{EDA: The Most Pernicious Term in Data Science}
\label{\detokenize{30_questions/07_eda:eda-the-most-pernicious-term-in-data-science}}\label{\detokenize{30_questions/07_eda::doc}}
\sphinxAtStartPar
In our next reading, we will turn our attention to \sphinxstyleemphasis{Exploratory Questions.} First, however, it is important to have a candid discussion about what I feel is one of the most problematic concepts in data science education: \sphinxstyleemphasis{Exploratory Data Analysis} or \sphinxstyleemphasis{EDA}.

\sphinxAtStartPar
The problem with the term Exploratory Data Analysis is that, if you asked most data scientists what it means, they probably couldn’t actually give you a straight answer. If you pressed them further, they would probably say something like “when you look at your data before you start fitting your models.”

\sphinxAtStartPar
While the idea that data scientists should “get to know their data” before fitting a model is well\sphinxhyphen{}meaning (you \sphinxstyleemphasis{absolutely} should!), the ubiquitous but uncritical use of the term has given young data scientists the sense that the undirected poking at data is worthy of a capitalized three world title, complete with a universally recognized acronym.

\sphinxAtStartPar
This is problematic because \sphinxstyleemphasis{any} activity that involves data but lacks a clear motivation is doomed to be unending and unproductive. Data science has emerged precisely because our datasets are far too complex for us to understand directly; indeed, I would argue that the job of a data scientist can be summed up, in part, as a person who identifies \sphinxstylestrong{meaningful} patterns in our data and makes them comprehensible.

\sphinxAtStartPar
But therein lies the problem — without a clear motivation for \sphinxstyleemphasis{why} the data scientist is poking at their data, what makes a pattern meaningful is undefined. And without a clear purpose from which a concept of meaningfulness can be derived, there is no end to the ways one can slice and dice the data with no way of knowing when to stop or what is useful.

\sphinxAtStartPar
I would argue that what most people call Exploratory Data Analysis (EDA) can actually be decomposed into three activities.

\sphinxAtStartPar
The first activity people call EDA is what I call “learning the structure of your \sphinxstyleemphasis{dataset}” (emphasis on learning about your \sphinxstyleemphasis{dataset}, not using your data to learn about the world). This consists of answering questions about your dataset like “what constitutes a single observation in this dataset?,” “what variables are included in this dataset?,” “how many observations are there?,” “how are variables coded?,” and “what population is represented in this data?” These are questions about \sphinxstyleemphasis{the specific dataset} you are working with, \sphinxstyleemphasis{not} the real world, and answers are likely to be found in the dataset documentation and through basic tools for data introspection.%
\begin{footnote}[1]\sphinxAtStartFootnote
In \sphinxcode{\sphinxupquote{pandas}}, this would be things like \sphinxcode{\sphinxupquote{df.columns}} to see what variables are in the data, \sphinxcode{\sphinxupquote{df.info()}} to get a sense of how data is being represented and the number of rows, and simple tools for tabulating unique values like \sphinxcode{\sphinxupquote{df{[}"first column"{]}.value\_counts()}}.
%
\end{footnote}

\sphinxAtStartPar
The second activity that often falls under the label EDA is what I call “validating your dataset.” It’s a poor data scientist who takes the validity of their data on blind faith, so when faced with a new dataset, one should begin with a few “sanity checks” just to make sure things look reasonable. Does the number of observations seem reasonable given what you know about how the data was collected and who is supposed to be represented in the data? If there are date variables in the data, does their range match what should be in this data? And given the specifics of the data, does the range of variables make sense? For example, if you have data on registered voters 18 and over, you should probably check that the age variable has a minimum value of 18 and a maximum value of something sensible (e.g., not 225).

\sphinxAtStartPar
The third and final activity people call EDA is… everything one does with the data before they fit a statistical or machine learning model. This is the second major reason that I feel the very concept of EDA has had a pernicious influence on data science — it implicitly devalues anything done with data that doesn’t entail a complicated model as “lesser” or “just a stop on the way towards the “real” analysis,” when nothing could be further from the truth.

\sphinxAtStartPar
This type of data analysis — looking at summary statistics, calculating distributions of variables, computing tabulations and cross\sphinxhyphen{}tabulations of different things to improve one’s understanding of the world — is categorically different from “learning the structure of your data,” because it is inquiry in the service of better understanding the world, not the structure of your dataset. But it is \sphinxstyleemphasis{not} categorically different analyzing data using statistical models, not just because in many cases generating cross\sphinxhyphen{}tabulations or calculating group averages are essentially equivalent to using a statistical method like linear regression, but also because they are both examples of the same enterprise: attempting to answer questions about the world using data in the service of solving problems.

\sphinxAtStartPar
And just as one cannot properly fit or tune a model without a clear sense of the question one is seeking to answer and how that answer is meant to be used, nor can one know what cross\sphinxhyphen{}tabulations to compute without having a sense of purpose to make clear what constitutes “meaningfulness.”

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
“But I do EDA all the time without a clear question!” I hear you cry. “Sometimes I just want to see what patterns there are in the data.”

\sphinxAtStartPar
To you I say: you may not have realized you had questions in mind, but most of your data explorations have been \sphinxstyleemphasis{implicitly} motivated by a sense of questions you thought might relate to your stakeholder’s problem.

\sphinxAtStartPar
Perhaps you were looking at a store’s retail sales data and decided to see how sales volumes varied by customer age or gender. That may not seem obviously question\sphinxhyphen{}motivated, but I put it to you that you had in mind that those are customer demographics to which the store could target advertising or product stocking decisions. And had someone suggested “why don’t you look at how sales volumes vary by customer birth month or whether their name starts with a letter in the first half of the alphabet,” you would have looked at them funny and said “why on Earth would I do that?”

\sphinxAtStartPar
But the problem with approaching your data with \sphinxstyleemphasis{implicit} motivations is that (a) it’s hard to reflect on them or evaluate whether they rest on solid assumptions about the stakeholder problem, and (b) without an explicit goal, there’s no way to know when you’ve reached your destination, making it \sphinxstyleemphasis{really}  easy to get lost in the data.
\end{sphinxadmonition}


\section{Recap}
\label{\detokenize{30_questions/07_eda:recap}}
\sphinxAtStartPar
Despite its ubiquity, few data scientists could actually tell you what constitutes Exploratory Data Analysis (EDA). Moreover, some of what people might call EDA in practice — answering questions about the world without complex modeling — should not be called EDA, but rather… well, that’s just data science.

\sphinxAtStartPar
So in this book, we will acknowledge the important (but distinct!) goal of two purposeful activities often called EDA:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Learning the structure of your dataset (what constitutes a unit of observation, what variables are in the dataset),

\item {} 
\sphinxAtStartPar
Validating your dataset (does the data pass the sniff test? Does it exhibit the basic properties you would expect given what it claims to be?)

\end{itemize}

\sphinxAtStartPar
But I will \sphinxstyleemphasis{not} use the term EDA itself, and when I differentiate between data science enterprises, I will do so by emphasizing differences in the \sphinxstyleemphasis{end goals} of those activities (answering Exploratory Questions, Passive\sphinxhyphen{}Prediction Questions, or Causal Questions), not the methods used to achieve those ends.


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{Using Exploratory Questions}
\label{\detokenize{30_questions/10_using_exploratory_questions:using-exploratory-questions}}\label{\detokenize{30_questions/10_using_exploratory_questions::doc}}
\sphinxAtStartPar
The hardest part of a data science project is often properly articulating the problem we wish to solve. That’s because properly specifying a problem requires \sphinxstyleemphasis{understanding} the problem well enough to state it, and often we call issues “problems” precisely because we don’t really understand them!

\sphinxAtStartPar
Enter \sphinxstyleemphasis{Exploratory Questions}. Exploratory Questions are questions designed to elicit information about our problem space and aid us in prioritizing our efforts and refining our goals. Exploratory Questions are questions about broader patterns in the world. In their simplest form, they can be answered by simple summary statistics or plots. When more complicated or related to more subtle and contingent patterns, they are likely to be answered through unsupervised machine learning algorithms or the tools of \sphinxstyleemphasis{statistical inference}, such as regressions and generalized linear models. When used for answering Exploratory Questions, the emphasis of regression or generalized linear models is on what the model coefficients can tell us about how different factors may co\sphinxhyphen{}vary in the world. This is distinct from how these same tools may be used to answer Passive\sphinxhyphen{}Prediction Questions, however, where the emphasis is on the predicted values these models can generate.

\sphinxAtStartPar
Of the three classes of questions we detail in this book, answering Exploratory Questions often (though not always) requires the least technical sophistication, and as a result, Exploratory Questions often get the least respect. But because of their critical role in improving our understanding of our objectives, learning to ask and answer Exploratory Questions will have a huge influence on your effectiveness as a data scientist.

\sphinxAtStartPar
In this reading, we will discuss how to \sphinxstyleemphasis{use} Exploratory Questions to guide your work. Then, in the next reading, we will discuss the challenges inherent to \sphinxstyleemphasis{answering} Exploratory Questions.


\section{Using Exploratory Questions to Prioritize Efforts}
\label{\detokenize{30_questions/10_using_exploratory_questions:using-exploratory-questions-to-prioritize-efforts}}
\sphinxAtStartPar
Exploratory Questions are questions about the underlying patterns that characterize our world. Given that, answers to Exploratory Questions should make you feel like you understand the contours of the problem you seek to solve better. More than anything else, then, Exploratory Questions help data scientists prioritize their subsequent efforts and investigations.


\subsection{Code Optimization}
\label{\detokenize{30_questions/10_using_exploratory_questions:code-optimization}}
\sphinxAtStartPar
If this feels too abstract, let’s use a small example of a problem you’ve probably already come across (and if not, probably should have!) in the classroom to make it more concrete: learning to write performant (i.e., fast) code.

\sphinxAtStartPar
Data science is full of computationally intensive tasks that, if approached incorrectly, can leave a data scientist staring at their computer for hours, days, or even weeks (if they allow it). As a result, most data scientists will go through a phase in their development when they start constantly worrying about how to make every line of code they write as fast as possible. They bend over backward to write unnatural, unreadable code to ensure that they aren’t wasting a single CPU clock cycle.

\sphinxAtStartPar
The problem with this is that humans have \sphinxstyleemphasis{incredibly} bad intuition about what tasks take a computer a long time. It turns out that even in programs that take huge amounts of time to run, it is often the case that \sphinxstyleemphasis{most} of the program’s runtime is taken up by a single function or loop. As a result, programmers who fixate on ensuring every line of code they write is optimized for speed end up not only wasting their \sphinxstyleemphasis{own} time, but also writing code that is less natural, harder to maintain, and more likely to contain errors for effectively no benefit.

\sphinxAtStartPar
Indeed, no less a figure than \sphinxhref{https://en.wikipedia.org/wiki/Donald\_Knuth}{Donald Knuth}, one of the greatest programmers in history and author of the famous \sphinxhref{https://en.wikipedia.org/wiki/The\_Art\_of\_Computer\_Programming}{\sphinxstyleemphasis{The Art of Computer Programming}}, famously said of this trying to optimize each line of code at the time it is being written (“premature optimization”):
\begin{quote}

\sphinxAtStartPar
The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; \sphinxstylestrong{premature optimization is the root of all evil (or at least most of it) in programming.} {[}emphasis added{]}
\end{quote}

\sphinxAtStartPar
So what is a programmer interested in performance to do? First, write code in as natural a way as possible. Then, \sphinxstyleemphasis{if} the result is code that is slower than they would like, ask the exploratory question: “What lines of code are contributing most to this program taking so long to run?” And only then, once the programmer has identified the problematic parts of their code, optimize it for performance.

\sphinxAtStartPar
How is this question answered accomplished? Programmers use tools called \sphinxstyleemphasis{profilers} that dip into a running program every few milliseconds to see what functions are currently running. Then after the program has finished running, it reports how often each part of the program code was found to be running, giving the user a sense of the overall distribution of time spent running different parts of the code.


\subsection{Picking the Right Target}
\label{\detokenize{30_questions/10_using_exploratory_questions:picking-the-right-target}}
\sphinxAtStartPar
If the preceding example feels too niche — you want to be a data scientist, after all, not a software engineer! — let’s consider a different example. Suppose you’ve been hired by a new non\sphinxhyphen{}profit interested in helping reduce energy use in buildings in the United States. They know that fixed structures (factors, stores, houses, etc.) are responsible for a huge share of US energy consumption, and are interested in figuring out how to drive down that energy use by helping building owners improve the energy efficiency of their buildings (by providing information on things like government subsidies for efficiency improvements and the potential value of energy efficient windows, better heating and cooling, etc.).

\sphinxAtStartPar
You \sphinxstyleemphasis{could} start out by trying to build a fancy supervised machine learning model that tried to predict the energy use of every building in the US based on infrared satellite data and weather information. Indeed, that may even be what you were asked to do! (See our discussion of how {\hyperref[\detokenize{30_questions/10_using_exploratory_questions:../10_introduction/30_solving_the_right_problem}]{\sphinxcrossref{\DUrole{xref,myst}{stakeholders will often have somewhat wild ideas of what is feasible and what would help most.}}}}).

\sphinxAtStartPar
But given this is a new non\sphinxhyphen{}profit, it sounds like their real need is probably to figure out how to target their efforts to be most effective. So maybe we should step back and start by trying to answer a few Exploratory Questions that would help the organization decide where to focus its attention:
\begin{itemize}
\item {} 
\sphinxAtStartPar
What \sphinxstyleemphasis{type} of buildings (industrial, residential, commercial) consume the most power in the US?
\begin{itemize}
\item {} 
\sphinxAtStartPar
The answer to this question can help you prioritize the \sphinxstyleemphasis{types} of buildings on which to focus your efforts. For example, if industrial or commercial buildings only represent a few percent of all energy consumed by buildings, you don’t need to worry about addressing their needs!

\end{itemize}

\item {} 
\sphinxAtStartPar
In what \sphinxstyleemphasis{region} of the US are buildings consuming the most power?
\begin{itemize}
\item {} 
\sphinxAtStartPar
If most energy is being consumed in a specific area, perhaps the non\sphinxhyphen{}profit should start by focusing its efforts regionally.

\end{itemize}

\item {} 
\sphinxAtStartPar
Is there a \sphinxstyleemphasis{region} of the US where buildings are generating the most CO2?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Not all power is created equal when it comes to climate change! Maybe buildings in California consume a lot of energy, but because they have cleaner power plants, those buildings are indirectly generating less CO2 than those in states in the US South?

\end{itemize}

\item {} 
\sphinxAtStartPar
Does the \sphinxstyleemphasis{average energy use per building} vary by region or building type?
\begin{itemize}
\item {} 
\sphinxAtStartPar
If the non\sphinxhyphen{}profit plans to approach building owners, it may be easier to have an impact working with a few owners of large buildings than lots of residential homeowners. But of course, that also depends on the answer to our previous question about what types of buildings are using the most power/generating the most CO2!

\end{itemize}

\item {} 
\sphinxAtStartPar
In what season is most building energy consumed? Is more energy consumed by heating or AC needs, or do the two use similar amounts of power?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Again, this may impact both the regions the non\sphinxhyphen{}profit may wish to focus on, and also the types of efficiency retrofits they may wish to prioritize.

\end{itemize}

\item {} 
\sphinxAtStartPar
Where is power most expensive?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Building owners are most likely to be interested in efficiency retrofits when power is expensive.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
While answering these questions is likely to require some significant detective work, and may require some thoughtful data wrangling, none require deeply sophisticated statistical machinery. But that doesn’t mean answering these questions wouldn’t provide \sphinxstylestrong{huge} value to the stakeholder.


\subsection{Collecting, Merging, and Creating New Data}
\label{\detokenize{30_questions/10_using_exploratory_questions:collecting-merging-and-creating-new-data}}
\sphinxAtStartPar
Once you start articulating these questions, you can start to see that there is some important data science to do; that’s because the answers to these questions may not all point in the same direction, and so the non\sphinxhyphen{}profit likely needs someone to be able to evaluate how these different factors co\sphinxhyphen{}vary, and the relative magnitude of different trade\sphinxhyphen{}offs (e.g., if fewer buildings use a lot of power in the US South than California, but the US South is using coal power instead of renewable energy, where should the non\sphinxhyphen{}profit focus?).

\sphinxAtStartPar
And that, fundamentally, is what Exploratory Questions are about: understanding the patterns and distribution of features you care about in the world, and using that information to better understand the problem you want to solve.

\sphinxAtStartPar
This also demonstrates one of the key ways that one answers Exploratory Questions: by collecting and merging datasets that had not previously been pulled together. Sometimes this data collection requires no more than finding people who already have the data you need, getting it, and finding a way to merge different data sources (e.g., data on power plant CO2 emissions and data on building energy use), while in other situations this will entail building new datasets yourself by doing things like using Natural Language Processing to make collections of documents (contracts, patient files, public records) analyzable systematically.


\subsubsection{But Where Do I Get Data?}
\label{\detokenize{30_questions/10_using_exploratory_questions:but-where-do-i-get-data}}
\sphinxAtStartPar
It’s hard to overstate how much data is public these days, but to give you a sense, \sphinxhref{https://www.unifyingdatascience.org/html/public\_data.html}{here’s a quick summary of a few terrific sources.}

\sphinxstepscope


\section{Answering Exploratory Questions}
\label{\detokenize{30_questions/15_answering_exploratory_questions:answering-exploratory-questions}}\label{\detokenize{30_questions/15_answering_exploratory_questions::doc}}
\sphinxAtStartPar
In the last reading, we discussed how Exploratory Questions are used by data scientists to help stakeholders better understand their problems and to prioritize subsequent investigations. In this reading, we turn to the questions of what \sphinxstyleemphasis{answering} Exploratory Questions effectively entails.


\subsection{The Three\sphinxhyphen{}Part Goal}
\label{\detokenize{30_questions/15_answering_exploratory_questions:the-three-part-goal}}
\sphinxAtStartPar
Whether one uses simple summary statistics (means and medians), plots, or more sophisticated algorithms from the domains of statistical inference and unsupervised machine learning, answering Exploratory Questions always boils down to the same challenge:

\sphinxAtStartPar
\sphinxstylestrong{Creating (1) understandable summarizations (2) of meaningful patterns in the data, (3) and ensuring they are faithful representations of the data.}

\sphinxAtStartPar
What is meant by these three components exactly? Let’s take each in turn.


\subsubsection{Understandable Summarizations}
\label{\detokenize{30_questions/15_answering_exploratory_questions:understandable-summarizations}}\begin{quote}

\sphinxAtStartPar
Creating \sphinxstylestrong{(1) understandable summarizations} (2) of meaningful patterns in the data, (3) and ensuring they are faithful representations of the data.
\end{quote}

\sphinxAtStartPar
Answering Exploratory Questions effectively is all about taking large datasets that, in their raw form, are effectively incomprehensible to humans and summarizing the patterns in that data in a way that can be understood. These summaries of patterns in the data may take many forms — summary statistics, regression coefficients, plots, etc. — but all, when done well, have a similar goal: to represent the salient aspects of data in a way that is accessible to the human mind.

\sphinxAtStartPar
Professionals from different disciplines often use different terminology to describe this process of summarization. Some like to refer to it as “separating the signal (the thing that’s important) from the noise (all the other variation that doesn’t matter),” others talk about “dimensionality reduction” (basically linear algebra speak for summarization), while still others may talk about “modeling the underlying data generating process that gave rise to the observed data.” Regardless of the terminology one uses, however, these all boil down to the same thing: filtering and discarding the variation the data scientist deems to be irrelevant to make it easier to see and understand the variation deemed important.

\sphinxAtStartPar
The importance of researcher discretion in deciding what variation to discard as noise and what variation to foreground as “important” is one of the defining challenges of answering Exploratory Questions. Other types of questions — like Passive Prediction Questions — often involve using more mathematically sophisticated modeling tools, and consequently are viewed as more challenging. In my experience, however, learning to understand the stakeholder’s problem context \sphinxstyleemphasis{and} the variation in a data set well enough to exercise this discretion effectively is actually one of the things young data scientists struggle with most. It requires both good domain knowledge to understand what is \sphinxstyleemphasis{meaningful} (as we will discuss below), and also for the data scientist to spend a lot of time exploring the data thoughtfully and from different perspectives. This is a hard skill to learn,%
\begin{footnote}[1]\sphinxAtStartFootnote
Although I am far from convinced that the discipline has tried particularly hard to teach it ({\hyperref[\detokenize{30_questions/15_answering_exploratory_questions:07_eda.ipynb}]{\sphinxcrossref{\DUrole{xref,myst}{see my screed against “EDA”}}}}).
%
\end{footnote} but with intentionality, patience, and practice, it is a talent that once learned will helps set you apart from the average pytorch\sphinxhyphen{}jockey.

\sphinxAtStartPar
Summarizations created to answer Exploratory Questions can differ radically in their ambition. At one end of the spectrum are simple summary statistics, like means, median, and standard deviations. These seek to provide a simple characterization of a single feature of a single variable. Slightly more ambitious are various forms of plots — like histograms (which are substantially richer than the aforementioned summary statistics) or scatter plots and heatmaps (which provide substantial granularity and communicate information about the relationship between different variables). The most ambitious efforts make use of multivariate regressions and unsupervised machine learning algorithms to model what they call the \sphinxstyleemphasis{Data Generating Process} (DGP) — the actual physical or social processes that gave rise to the data you observe, and which (hopefully) can be represented in a relatively parsimonious manner, much as the relatively simple laws of physics give rise to the orbits of the planets and the complexity of life.

\sphinxAtStartPar
To illustrate what I mean by trying to deduce something about the data\sphinxhyphen{}generating process, suppose you are a medical researcher interested in a poorly understood disease like Chronic Fatigue Syndrome (CFS). It is generally agreed that CFS is more of a label for a constellation of symptoms than an understood physical ailment, and you have a hypothesis that the symptoms of CFS aren’t actually caused by a single biological dysfunction, but rather that multiple distinct biological dysfunctions give rise to similar symptoms that we have mistakenly grouped under this same umbrella term. In other words, you think that the data\sphinxhyphen{}generating process that gives rise to patients diagnosed with Chronic Fatigue Syndrome consists of two distinct diseases.

\sphinxAtStartPar
You’re fortunate enough to have detailed patient data on people diagnosed with the condition, but it’s impossible for you to just look at these gigabytes of thousands of patient records and “see” any meaningful patterns. You need a way to filter out irrelevant data to identify the “signal” of these two conditions. To aid you in this question, you decide to ask “If you were to group patients into two groups so that the patients in each cluster looked as similar as possible, but patients in different clusters looked as \sphinxstyleemphasis{dissimilar} as possible, how would you group these patients?”

\sphinxAtStartPar
This, you may recognize, is precisely the question clustering algorithms (a kind of unsupervised machine learning algorithm) are designed to answer! So you apply your clustering algorithm to the patient data and get back a partition of the patients into two distinct groups. This, in and of itself, doesn’t constitute a particularly \sphinxstyleemphasis{understandable} summarization of your data, but it provides a starting point for trying to investigate \sphinxstyleemphasis{diagnostically and biologically relevant} differences that exist between these populations. If one cluster included more patients reporting fatigue when doing any exercise, while another cluster reported they felt better when they exercised, but felt a high level of baseline fatigue that didn’t respond to sleep, that might suggest that the \sphinxstyleemphasis{data\sphinxhyphen{}generating process} for these patients was actually driven by two different biological processes. \sphinxstyleemphasis{And} it gives you a great starting point to prioritize your subsequent investigations into what might explain these differences!


\subsubsection{Meaningful Patterns}
\label{\detokenize{30_questions/15_answering_exploratory_questions:meaningful-patterns}}\begin{quote}

\sphinxAtStartPar
Creating (1) understandable summarizations \sphinxstylestrong{(2) of meaningful patterns in the data,} (3) and ensuring they are faithful representations of the data.
\end{quote}

\sphinxAtStartPar
Inherent in creating any summarization is exercising discretion over what variation is relevant (signal) and what variation is not (noise). But just as one person’s trash may be another person’s treasure, so too may one person’s signal be another person’s noise, depending on their goals! Crucially, then, the data scientists’ guiding star when deciding what is important is whether certain variation in the data is \sphinxstyleemphasis{meaningful to the stakeholder’s problem}.

\sphinxAtStartPar
As data scientists, we are blessed with an abundance of tools for characterizing different facets of our data. These range from the simple — means, standard deviations, and scatter plots — to the profoundly sophisticated, like clustering algorithms, principal component analyses, and semi\sphinxhyphen{}parametric generalized additive models.

\sphinxAtStartPar
Regardless of the specific methods being employed, however, none of these tools can really tell us whether the patterns they identify are meaningful, and that’s because what constitutes a meaningful pattern depends on the problem the stakeholder is seeking to address and the context in which they’re operating.

\sphinxAtStartPar
To illustrate the importance of context, suppose you are hired by a hospital to learn what can be done to reduce antibiotic\sphinxhyphen{}resistant infections. So you grab data on the various bacteria that had been infecting patients and write a web scraper and Natural Language Processing pipeline to systematically summarize all available research on the cause of these antibiotic\sphinxhyphen{}resistant bacteria. Your work is \sphinxstyleemphasis{amazing}, seriously top of the line, and after two months you conclude that in most cases, the cause of antibiotic resistance in the bacteria infecting patients is… the use of antibiotics in livestock.

\sphinxAtStartPar
Now, that analysis may not be \sphinxstyleemphasis{wrong} — you have properly characterized a pattern in the data — but it isn’t a pattern that’s meaningful to your stakeholder, who has no ability to regulate the livestock industry. That pattern might be meaningful to someone else — like a government regulator — but in this context, with this stakeholder, it just isn’t helpful. The features of the data that are important, in other words, depend on what we may be able to do in response to what we learn. And there’s no summary statistic, information criterion, or divergence metric that can evaluate whether a pattern of this type is \sphinxstyleemphasis{meaningful}.


\subsubsection{Faithful Representations}
\label{\detokenize{30_questions/15_answering_exploratory_questions:faithful-representations}}\begin{quote}

\sphinxAtStartPar
Creating (1) understandable summarizations (2) of meaningful patterns in the data, \sphinxstylestrong{(3) and ensuring they are faithful representations of the data.}
\end{quote}

\sphinxAtStartPar
What do you means, medians, standard deviations, linear regressions, logistic regressions, generalized additive models (GAMs), singular value decomposition (SVD), principal component analyses (PCAs), clustering algorithms, and anomaly detection algorithms all have in common?

\sphinxAtStartPar
Answer: unless your dataset is extremely degenerate, you can point \sphinxstyleemphasis{any} of these tools at your data and they will return a relatively easy\sphinxhyphen{}to\sphinxhyphen{}understand characterization of the structure of your data.

\sphinxAtStartPar
At first, that may seem extremely exciting. But if you think about it a little longer you will realize the problem: all of these are designed to give you a relatively understandable summary of radically different properties of your data, and even though they will all provide you with a result, these results can’t all possibly be faithful representations of the dominant patterns in your data.

\sphinxAtStartPar
To illustrate the point, suppose I told you that in one university math course, the average grade was a B\sphinxhyphen{}. You might infer that students were doing pretty well! But now suppose I told you that in a different university math course, 20\% of the students had gotten a 0 on the midterm and on the final—you would probably infer something was going seriously wrong in that class. And yet those two statistics could both be true of the same class—the only difference is what patterns in the data \sphinxstyleemphasis{I}, the data scientist, have decided are meaningful to communicate to you, the reader.

\sphinxAtStartPar
The example of the math class in which the average grade was a B\sphinxhyphen{} and 20\% of the students were failing also illustrates one of the great dangers of tools for data summarization: they are so eager to please, they will \sphinxstyleemphasis{always} provide you with an answer, whether that answer is meaningful or not. I think most readers would agree that learning that the average grade in the class was a B\sphinxhyphen{} actually misleads more than it informs (since for the class to have an average grade of 80\% and a 20\% fail rate, the grade distribution would need to be something like 20\% 0’s and 80\% 100’s). Indeed, it’s worth emphasizing that while hearing “the average grade is a B\sphinxhyphen{}” makes the reader think that most kids are doing ok\sphinxhyphen{}ish, the reality is that \sphinxstyleemphasis{no one} in the class is doing ok\sphinxhyphen{}ish! They’re either doing horribly or terrifically!

\sphinxAtStartPar
Less that feel like a contrived example, consider the case of Aimovig, a drug authorized by the FDA in 2018 for treating chronic migraines that was heralded as a “game changer.”

\sphinxAtStartPar
To get Aimovig authorized, the pharmaceutical companies developing (Amgen and Novartis) had to run a clinical trial in which a random sample of people with chronic migraines was given Aimovig (the treatment group) and a random sample was a placebo (the control group). Patients in the clinical trial self\sphinxhyphen{}reported how their migraine frequency changed when in the trial, and the effectiveness of Aimovig was then evaluated by comparing the decrease in self\sphinxhyphen{}reported migraines for those taking Aimovig (on average, a decrease of 6\sphinxhyphen{}7 migraines a month) to the decrease in self\sphinxhyphen{}reported migraines for those taking a placebo (on average, a decrease of 4 migraines a month).%
\begin{footnote}[2]\sphinxAtStartFootnote
A placebo is a “fake” treatment given to patients in clinical trials. Despite not being biologically active — placebos are often simple saline or sugar pills — most patients on placebos see their condition improve when dealing with subjective conditions, like pain.
%
\end{footnote} This difference of 2\sphinxhyphen{}3 migraines a month — called the “Average Treatment Effect” of the trial — was found to be positive and statistically significant, and so the drug was authorized. Indeed, if you see an ad for Aimovig, you’ll probably see the average effect of the drug reported in the same way:

\sphinxAtStartPar
\sphinxincludegraphics{{migraine_average_effect}.png}

\sphinxAtStartPar
That’s great! Chronic migraines can be a crippling disability, so any improvement in treatment is exciting. But you would be excused for asking why people were getting \sphinxstyleemphasis{so} excited about what seems like a relatively small reduction in migraines.

\sphinxAtStartPar
The answer, as it turns out, is that almost nobody experiences this “average effect.” Instead, \sphinxstyleemphasis{most} people who take Aimovig see little to no benefit, but \sphinxstyleemphasis{some} (depending on your criteria, something like 40\%) see their migraine frequency fall by 50\% or more. Amgen and Novartis don’t yet know how to identify who will benefit and who will not before they try the drug, and we don’t allow drug companies to “move the goalposts” after a clinical trial has already started by changing the way they plan to measure the effectiveness of a drug (for fear they will hunt through the data till they find a spurious correlation that makes it look like the drug works when it really doesn’t), so this average effect remains the only statistic that Amgen and Novartis are allowed to report in their advertising.

\sphinxAtStartPar
But if you’re a \sphinxstyleemphasis{doctor} or a \sphinxstyleemphasis{patient}, it seems clear that this simple average effect — a reduction of 2\sphinxhyphen{}3 migraines a month — really does not provide a \sphinxstyleemphasis{faithful} summary of the underlying variation.


\subsubsection{But… I Thought Unsupervised Machine Learning Always Found The “Best”}
\label{\detokenize{30_questions/15_answering_exploratory_questions:but-i-thought-unsupervised-machine-learning-always-found-the-best}}
\sphinxAtStartPar
“Fine,” I hear you say, “that makes sense for simple summary statistics. Those are computed by simple formulas. But what about unsupervised machine learning algorithms or generalized additive models? Those use numerical optimization to find the \sphinxstyleemphasis{best} answer!”

\sphinxAtStartPar
Well… yes and no. As you may recall, in the first chapter of the book I posited that all data science algorithms are just fancy tools for answering questions, and even the most sophisticated unsupervised machine learning algorithms are no exception. While it is true that the machinery that underlies these algorithms is much more sophisticated than the formula we use for calculating a variable’s average, it is important to not attribute too much intelligence to these tools.

\sphinxAtStartPar
Underlying any unsupervised machine learning algorithm is a simple formula that takes as input whatever parameters the algorithm gets to choose (be those factor loads in a PCA model, or the assignment of observations to clusters in a clustering algorithm) and returns as output a single number. Often this number is called “loss,” and the function is called a “loss function,” but occasionally different terminology will be used.

\sphinxAtStartPar
One way to think of the job of an unsupervised machine learning algorithm is to pick the parameter values that minimize this loss function. A clustering algorithm for example, may try and assign observations to clusters to maximize the similarity of observations within a cluster (say, by minimizing the sum of squared differences between the values of certain variables for all observations within a cluster) while also maximizing the differences between observations in different clusters (say, by maximizing the sum of squared differences between the values of certain variables for all observations \sphinxstyleemphasis{not} in the same cluster).

\sphinxAtStartPar
But another way to say that is that the job of an unsupervised machine learning algorithm (or any algorithm, really) is to find the parameter values (coefficients in a regression, observation assignments for a clustering algorithm) that answer the question “If my goal is to minimize {[}whatever the loss function your specific algorithm seeks to minimize{]}, how should I do it?” But while they are likely to find the best way to accomplish that goal given the parameters they control, they will do so \sphinxstyleemphasis{whether or not the “best” solution is actually a “good” solution!} Point a clustering algorithm at any data and ask it to split the data into 3 clusters, and it will pick the best way to split the data into three clusters, even if the three clusters are \sphinxstyleemphasis{almost} indistinguishable. In other words, clustering algorithms assign observations to clusters… even when there’s no real clustering of the data! Dimensionality reduction algorithms will always tell you a way to drop dimensions, and anomaly detection algorithms will always find (relative) outliers.

\sphinxAtStartPar
Moreover, just because your clustering algorithm finds what it thinks is the best solution doesn’t mean there isn’t a \sphinxstyleemphasis{substantively} very different solution that was \sphinxstyleemphasis{just} a little less good it hasn’t told you about.

\sphinxAtStartPar
It’s up to you, the data scientist, to evaluate whether the answers these algorithms provide to relatively myopic questions give a meaningful picture of the data.


\subsubsection{Myopic Tools}
\label{\detokenize{30_questions/15_answering_exploratory_questions:myopic-tools}}
\sphinxAtStartPar
This last point is illustrative of a more general point: data science tools are incredibly powerful at finding answers to questions of the form “If my goal is to minimize X, how should I do it?” type questions — answers you may have never figured out in millions of years! — but their power lies in figuring out the best way to accomplish an articulated goal, \sphinxstyleemphasis{not} in figuring out what goal to pursue.

\sphinxAtStartPar
This is true at both the macro level (doesn’t make sense to look for clusters in my data?) and also at the micro level (when assigning observations to clusters, how do I measure success?). Hidden inside nearly all algorithms you use are a handful of baked\sphinxhyphen{}in choices you may not even realize are being made for you. Take clustering, for example. In general, when clustering observations, one has two objectives: maximize the similarity of observations within each cluster and maximize the \sphinxstyleemphasis{dis}similarity of observations in different clusters. But what you might not have thought about very much is that there’s an inherent tension between these two objectives — after all, the best way to maximize the similarity of observations within each cluster is to only assign observations to the same cluster if they are identical (a choice that creates lots and lots of very small clusters). And the best way to maximize \sphinxstyleemphasis{dis}similarity between clusters is to only put \sphinxstyleemphasis{really really} different observations in different clusters (resulting in a few really big clusters). So how is your clustering algorithm balancing these two considerations? Is the algorithm’s choice of how to balance them in any way a reflection of the balance that makes the most sense in the context of your stakeholder’s problem? (I’ll give you a hint — the algorithm sure can’t answer that question, so you’d better be able to!)

\sphinxAtStartPar
Discretion: it’s everywhere, and you’re exercising it, whether you realize it or not.
















\bigskip\hrule\bigskip


\sphinxstepscope


\section{Internal versus External Validity}
\label{\detokenize{30_questions/17_exploratory_questions_internal_external:internal-versus-external-validity}}\label{\detokenize{30_questions/17_exploratory_questions_internal_external::doc}}
\sphinxAtStartPar
It is at this point I have to come clean about having employed a… small indirection. At the top of this reading, I introduced the idea that answering Exploratory Questions boiled down to creating (1) understandable summarizations (2) of meaningful patterns, and (3) ensuring those summaries faithful represent the data. But that three\sphinxhyphen{}part objective is actually only one\sphinxhyphen{}half of answering an Exploratory Question. More specifically, those are the three components of ensuring high \sphinxstyleemphasis{internal validity} when answering an Exploratory Question. But to generate a truly useful answer to an Exploratory Question, your analysis must also have high \sphinxstyleemphasis{external validity}.

\sphinxAtStartPar
Essentially, \sphinxstyleemphasis{internal validity} is a measure of how well you have analyzed \sphinxstyleemphasis{the data you have}, while \sphinxstyleemphasis{external validity} is how well you expect the answer you generated from that data to generalize to your stakeholder’s context. Internal and external validity arise when answering \sphinxstyleemphasis{any} data science question, and so these two concepts are ones that we will return to time and again in this book.

\sphinxAtStartPar
While the idea of internal validity will feel familiar to anyone who has taken a statistics of machine learning course, external validity is often less discussed in the classroom. This is not because it is less \sphinxstyleemphasis{important} than internal validity. One reason for this is that it is harder to evaluate external validity using statistical methods, causing some instructors to think of it as “outside the scope” of a statistics course. But the second reason I suspect this occurs is more subtle: while a given analysis may be said to have good or bad internal validity, it cannot be said to have good or bad external validity. Rather, the external validity of a study must always be evaluated \sphinxstyleemphasis{with respect to a specific context to which one wishes to generalize the findings.}

\sphinxAtStartPar
This means that external validity is different from internal validity in an important way: when faced with the same facts about a study, everyone should \sphinxstyleemphasis{generally} agree on the internal validity of a study, but the external validity of a study really depends on how you want to use the results. A study of medical care provided to Duke undergraduates may have very \sphinxstyleemphasis{good} external validity with respect to undergraduate students at Emory, UNC, Vanderbilt, and other elite universities with associated research hospitals (that is, the conclusion of the study of Duke students is likely to also be valid for those other students). But that same study may have \sphinxstyleemphasis{poor} external validity with respect to lower income students attending community colleges that have less comprehensive student health insurance and no top\sphinxhyphen{}tier associated hospital system. And it would certainly have terrible external validity with respect to all Americans, never mind people living in other countries.


\subsection{Interval v. External Validity: An Example}
\label{\detokenize{30_questions/17_exploratory_questions_internal_external:interval-v-external-validity-an-example}}
\sphinxAtStartPar
To illustrate what is meant by these terms, suppose you’ve been hired by a car manufacturer. Through extensive market research, they have determined that if they can improve the safety of their cars, they could dramatically improve the image of their brand. They have turned to you, their in\sphinxhyphen{}house data scientist, to help them determine what safety enhancements they should focus on developing.

\sphinxAtStartPar
To help them prioritize their efforts, you decide it would be useful to begin by better understanding the predominant causes of major accidents. After all, if 95\% of accidents were caused by mechanical failures, bad weather, and drunk drivers, then even a perfect system for preventing drowsy driving accidents could only reduce accidents by 5\% at best!%
\begin{footnote}[1]\sphinxAtStartFootnote
It is worth noting that if the goal of the company is to improve brand image and sales — rather than improve safety as much as possible — then what matters is not what \sphinxstyleemphasis{actually} causes the most accidents, but rather what customer \sphinxstyleemphasis{perceive} causes the most accidents. But let us assume — for the sake of this exercise — that your employer’s interest in reducing accidents is sincere. If there is a divergence between customer perceptions and the reality of accident causes, that could be addressed with additional information embedded in any advertisements of the feature being developed.
%
\end{footnote}

\sphinxAtStartPar
The best source of accident investigations of which you are aware is the \sphinxhref{https://www.nhtsa.gov/data}{US National Highway Traffic Safety Administration’s} National Center for Statistics and Analysis (NCSA). Using their \sphinxstyleemphasis{Crash Reporting Sampling System}, the NCSA collects and publishes data from “nationally representative sample of police\sphinxhyphen{}reported traffic crashes, which estimates the number of police\sphinxhyphen{}reported injury crashes and property\sphinxhyphen{}damage\sphinxhyphen{}only crashes in the United States,” as well as data on all fatal accidents collected through their \sphinxstyleemphasis{Fatality Analysis Reporting System}.

\sphinxAtStartPar
The NCSA provides you with data on police\sphinxhyphen{}reported accidents from the Crash Reporting Sampling System and all fatal accidents from the Fatality Analysis Reporting System from the past 5 years (2018\sphinxhyphen{}2023). As you turn to using this data to analyze the causes of severe accidents for your stakeholder, what internal and external validity concerns might you have in the back of your mind?


\subsubsection{Internal Validity Concerns}
\label{\detokenize{30_questions/17_exploratory_questions_internal_external:internal-validity-concerns}}
\sphinxAtStartPar
Internal validity concerns, in this context, relate to your ability to properly characterize the causes of severe accidents in this data. The first of these concerns are those discussed in our previous two readings — namely, whether the summarizations of accident causes you generate from this data are meaningful and faithful representations of the patterns in the data.

\sphinxAtStartPar
But internal validity concerns also extend to things like concerns over the accuracy with which things are measured. For example, it’s reasonable to ask “How well do police accident reports capture the true causes of an accident?” Some factors — like whether a driver was intoxicated — are easy to verify after the fact in major accidents. Other factors, however, may have contributed to accidents — and be easier to address with driver assistance systems — but may have been too hard to verify for the police to put in their reports. For example, maybe weak headlights prevented the driver from seeing a change in the speed limit (causing the documented cause of the accident: speeding). Or perhaps one driver was unable to see an approaching vehicle on a cross street due to width of the A\sphinxhyphen{}piller (that piece of metal running vertically between the front windshield and front side window). Or “weather” may be invoked as a cause in an accident when what was really at play was that the driver had the wipers on too low of a setting (and automatic wipers would have adjusted more quickly, preventing the accident).

\sphinxAtStartPar
All of these are question about whether our summarization of the \sphinxstylestrong{data} provides a proper characterization of \sphinxstylestrong{the world} for the period and group we think are covered.


\subsubsection{External Validity Concerns}
\label{\detokenize{30_questions/17_exploratory_questions_internal_external:external-validity-concerns}}
\sphinxAtStartPar
But worrying about whether we really understand the world covered by our data is only half the battle. After all, our stakeholders aren’t planning to go back in time 5 years and add a new driver safety system to their cars starting in 2018; they’re thinking of developing this safety system to use \sphinxstyleemphasis{in the future}. So what do we think is the \sphinxstyleemphasis{external validity} of findings from 2018\sphinxhyphen{}2023 to the next ten years?

\sphinxAtStartPar
To answer this question, we need to consider how the next ten years might differ from the last five \sphinxstyleemphasis{in ways that are relevant to the phenomenon we care about} (here, the causes of car accidents).

\sphinxAtStartPar
First, since time will have passed between when the crash data was collected and when the new feature rolls out, one might reasonably assume that trends in accident causes during the 2018\sphinxhyphen{}2023 period are likely to continue. In light of that, a thoughtful data scientist may wish to pay extra attention to whether there are clear \sphinxstyleemphasis{trends} in accident causes, and in what direction they are trending.

\sphinxAtStartPar
But one might also pause to ask whether there was anything exceptional about the “data generating process” during the 2018\sphinxhyphen{}2023 period that would not obtain in later years. Something like… a pandemic?

\sphinxAtStartPar
And indeed, one would be right to worry. The figure below shows Vehicle Miles Travelled (VMT) and Fatalities in 2019 (pre\sphinxhyphen{}pandemic) and 2020 (the pandemic began in force in March, if you’ve forgotten). As the figure shows, driving plummeted during this period, and so too did Fatalities.

\sphinxAtStartPar
\sphinxincludegraphics{{2020_vmt_fatalities}.png}

\sphinxAtStartPar
Source: Taken from the US Department of Transportation NHTSA’s \sphinxhref{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813266}{\sphinxstyleemphasis{Overview of Motor Vehicle Crashes in 2020}}

\sphinxAtStartPar
Given that, a thoughtful data scientist may wish to be sure that any patterns identified in this five\sphinxhyphen{}year period are robust to exclusion of 2020\sphinxhyphen{}2022 before making any predictions about the likelihood these patterns would persist in later years.


\paragraph{Other Contexts}
\label{\detokenize{30_questions/17_exploratory_questions_internal_external:other-contexts}}
\sphinxAtStartPar
The preceding discussion assumed an interest in US accidents, but of course most car companies that sell cars in the US also sell cars in Asia and Europe. The issues raised above are examples of what I would contend are relatively \sphinxstyleemphasis{small} threats to the external validity of findings based on data from 2018\sphinxhyphen{}2023 with respect to near\sphinxhyphen{}future US accidents (at least provided the patterns aren’t driven by the pandemic period).

\sphinxAtStartPar
But external validity to Asian or European markets would be a much bigger concern. Because traffic laws, speed limits, alcohol laws, and how roads are constructed and laid out are all very different in different regions of the world, it seems quite unlikely that patterns identified in US accident data would have much external validity to Asian or European auto markets.


\subsection{Conclusion}
\label{\detokenize{30_questions/17_exploratory_questions_internal_external:conclusion}}
\sphinxAtStartPar
Internal and External Validity are both key concepts for being an effective data scientist, and they are concepts to which we will return regularly in this book. Moreover, they are goals that are sometimes in tension — the more control one has over a study context, the more likely one is to have good Internal Validity; but that control can often create an artificiality to limits External Validity. Thus Internal and External Validity should not necessarily be thought of as things to try and simultaneously maximize at all costs; rather, they are best thought of as distinct features of any analysis that should always be considered. I would also add that, of the two, I would argue that External Validity — while not more important in and of itself — is the more often overlooked.


\subsection{Reading Reflection Questions}
\label{\detokenize{30_questions/17_exploratory_questions_internal_external:reading-reflection-questions}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Why is the External Validity of a study only defined with respect to a given context?

\item {} 
\sphinxAtStartPar
A group of Duke doctors wishes to understand exercise behavior in patients who have recently experienced cardiac surgery. To measure exercise behavior, they surveyed their patients post\sphinxhyphen{}surgery about their exercise behavior. But one doctor on the team is concerned that people may mis\sphinxhyphen{}report their exercise (the patients know they’re \sphinxstyleemphasis{supposed} to be exercising, so the doctor is concerned they will report they are exercising even if they are not). Is this an Internal or External Validity concern?

\item {} 
\sphinxAtStartPar
After these Duke doctors published their results, word of the study reached a doctor in rural Louisiana. Looking at the study, however, she found that the patients treated by the Duke doctors tended to be younger and healthier than the patients she sees. Is this an Internal or External Validity concern?

\end{itemize}




\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{Using Passive Prediction Questions}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:using-passive-prediction-questions}}\label{\detokenize{30_questions/20_using_passive_prediction_questions::doc}}
\sphinxAtStartPar
In the past few chapters, we explored the role of Exploratory Questions in helping data scientists better understand the problems they seek to solve and to prioritize subsequent efforts. While useful, however, answering Exploratory Questions is rarely sufficient to solve a stakeholder’s problem in and of itself. To really solve problems, often data scientists must turn to answering Passive Prediction Questions — the focus of this chapter — and/or Causal Questions (a topic we will return to in future chapters).

\sphinxAtStartPar
As discussed in the introduction of this book, Passive Prediction Questions are questions about the future or potential outcomes of \sphinxstyleemphasis{individual entities} (customers, patients, stores, etc.). “How likely is Patient X to experience a heart attack in the next two years?,” for example, or “How likely is it Mortgage Holder Y will fail to make their mortgage payment next month?”

\sphinxAtStartPar
Unlike Exploratory Questions, data scientists don’t generally come up with “\sphinxstyleemphasis{an} answer” to Passive Prediction Questions; rather, data scientists answer Passive Prediction Questions by developing statistical models that take the attributes of an entity as inputs and spit out a unique answer \sphinxstyleemphasis{for each entity}. A company interested in spam detection, for example, might hire a data scientist to develop a model that takes the content of an email as input and, for every email a person receives, answers the question “If the recipient of this email looked at it, would they consider it spam?” The exact statistical machinery one uses will vary across applications, but answering these questions is the realm where terminology like “supervised machine learning” is most likely to be used.

\sphinxAtStartPar
Since Passive Prediction Questions don’t usually have “\sphinxstyleemphasis{an} answer,” a data scientist faced with a Passive Prediction challenge will often start by considering the \sphinxstyleemphasis{feasibility} of developing a model to give individual\sphinxhyphen{}level answers to a Passive Prediction Question. “Given data on new customer behavior on my website,” for example, “\sphinxstylestrong{can I} predict how much a customer is likely to spend over the next year?” At the end of the day, however, what a stakeholder cares about is not whether one \sphinxstyleemphasis{can} predict future spending; they care about the actual answer to the question, “Given this new customer’s behavior on my website, are they likely to spend a lot over the next year?” for a given customer. For that reason — and because understanding exactly what question a model is answering is key to its effective use — this individual\sphinxhyphen{}level question will be our focus here.


\section{Flavors of Passive Prediction Questions}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:flavors-of-passive-prediction-questions}}
\sphinxAtStartPar
Passive Prediction Questions come in two flavors, corresponding to the two primary business purposes detailed above.

\sphinxAtStartPar
The first flavor of Passive Prediction Questions is questions about what is likely to occur in the future for a specific individual. Answering these questions is useful for identifying individuals for additional care or attention. For example, a hospital might want to know “How likely is Patient A to experience complications after surgery?” so they can decide whether the patient should receive extra nursing attention during recovery, or a factory owner might ask “How likely is this machine to break down in the next month?” to help them determine when to take the machine offline for maintenance. This is the more intuitive flavor of Passive Prediction Question, as it accords nicely with the normal meaning of the term “predict.”

\sphinxAtStartPar
The second favor of Passive Prediction Questions is questions about what \sphinxstyleemphasis{would} happen in different circumstances. Answering this type of question is the key to automation, as the “different circumstance” in question is often one in which a job is being done by an actual person. For example, a statistical model that can answer the question “if a radiologist were to look at this mammogram, would they conclude it showed evidence of cancer?” effectively is a model that automates the review of mammograms. A model that can answer the question “if you showed this email to its intended recipient, would they say it is spam?” is an algorithm that automates spam detection.


\subsection{OK, but… Doesn’t This Feel a Little Contrived?}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:ok-but-doesn-t-this-feel-a-little-contrived}}
\sphinxAtStartPar
You would be forgiven for asking whether I’ve gone a little too far in trying to force the simple task of automation into the “all data science tools are tools for answering questions” framework of the book. Yes, I see how you \sphinxstyleemphasis{could} call an algorithm that automates the review of mammograms a tool for answering the question “if a radiologist were to look at this mammogram, would they conclude it showed evidence of cancer?” But that seems awfully convoluted. Why can’t we just call it an algorithm that looks for cancer in mammograms?

\sphinxAtStartPar
The answer is that while it is certainly less succinct, but as we discussed in the introduction, because of how these models are developed, it is \sphinxstyleemphasis{more accurate} to say that our mammogram reading algorithm is trying to answer the question “if a radiologist were to look at this mammogram, would they conclude it showed evidence of cancer?” than to say that it’s “looking for cancer.” The way that most statistical models are developed (“trained”) to answer Passive Prediction Questions is using a large dataset of “training examples” — that is, data in which the outcome we care about is included in the dataset. For predicting individual\sphinxhyphen{}level future outcomes, this will be historical data in which outcomes — like surgery complications or factory machine failures — have already occurred. For automation, this will be data in which a person has already completed the task, and their conclusions or actions have been recorded. To train a mammogram reading algorithm, in other words, you first need a database of mammograms that radiologists have already labeled as indicating cancer or not. And what the model is actually trained to do is not “find cancer” but rather to “give the same answers given by human radiologists.”

\sphinxAtStartPar
And this is where the distinction between “predicting what a radiologist would say” and “detecting cancer” becomes important: because this kind of statistical model was trained to emulate the behavior of radiologists in labelled mammograms, the model will recapitulate any systematic biases held by the human radiologists who reviewed the mammograms in the training data. Did the radiologists struggle to detect cancer in dense breast tissue? So too will the algorithm trained on their labels.

\sphinxAtStartPar
The tendency for algorithms to replicate human biases present in training data, unfortunately, extends to gender and racial biases. In 2018, for example, Reuters reported that Amazon was forced to scrap an effort to use machine learning to automatically review resumes because it turned out the \sphinxhref{https://www.reuters.com/article/idUSKCN1MK0AG/}{algorithm — trained on historic hiring data — was biased against women.} The \sphinxstyleemphasis{exact} details of the source of the bias is unclear — for obvious reasons Amazon is not eager to report on the failure — but my suspicion is that things went wrong in one of two ways.

\sphinxAtStartPar
The first is that the algorithm was given data on all past Amazon applicant resumes, along with data on which applicants had actually been hired. The algorithm was then effectively trained to answer the question “If an Amazon hiring manager looked at this resume, is it likely they would be hired?” In that case, the algorithm was recapitulating the gender bias of previous hiring managers.

\sphinxAtStartPar
My second guess is that the algorithm was given the resumes of \sphinxstyleemphasis{current} employees along with employee reviews. In that case, the algorithm was effectively being asked to answer the question “Given this resume, is it likely this is a person a current manager would rate highly once employed?” In that case, the algorithm was recapitulating gender biases in employee reviews.

\sphinxAtStartPar
In either case, these are examples of an \sphinxstyleemphasis{alignment problem}: the people developing these models \sphinxstyleemphasis{wanted} the algorithm to pick the applicants who would be the most productive employees, but the model they \sphinxstyleemphasis{actually} developed was trying to identify employees who looked like previously successful applicants. Had the previous hiring system been effective, this wouldn’t be a problem, but because the previous system included a gender bias, so too did the resulting algorithm. But because the engineers developing these tools did not think carefully enough about the question the model was actually being taught to answer, the problem was not identified until it was too late.


\section{Differentiating Between Exploratory and Passive Prediction Questions}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:differentiating-between-exploratory-and-passive-prediction-questions}}
\sphinxAtStartPar
If you have not felt a little confused about the distinction between Exploratory and Passive\sphinxhyphen{}Prediction Questions previously, there’s a good chance you find yourself struggling with that issue here, and for understandable reasons.

\sphinxAtStartPar
The first thing to emphasize is that the distinction between Exploratory and Passive Prediction Questions is a distinction in one’s \sphinxstyleemphasis{goal}, not the statistical machinery one might use to achieve that goal.

\sphinxAtStartPar
With Passive\sphinxhyphen{}Prediction Questions, our interest is in the values that get spit out of a model for each entity in the data. When answering a Passive\sphinxhyphen{}Prediction Question, the \sphinxstyleemphasis{only} thing we care about is the quality of those predictions, and so we evaluate the success of a model that aims to answer a Passive\sphinxhyphen{}Prediction Question by the quality of those predictions (using metrics like AIC, AUC, R\sphinxhyphen{}Squared, Accuracy, Precision, Recall, etc.). Thus, when using a logistic regression to answer a Passive\sphinxhyphen{}Prediction Question, we don’t actually care about what factors are being used to make our predictions, just that they improve the predictions. Our interest is only the quality of our predicted values, and a good model is one that explains a substantial portion of the variation in our outcome.

\sphinxAtStartPar
With Exploratory Questions, our interest is in improving our understanding of the problem space, not in making precise predictions for each entity in our data. Thus, in the example of logistic regression, our interest is in the factors on the “right\sphinxhyphen{}hand side” of our logistic regression and how they help us understand what shapes outcomes, not the exact accuracy of our predictions. A good model, in other words, doesn’t actually have to explain a large share of variation at the level of individual entities, but it does have to help us understand our problem space.

\sphinxAtStartPar
A model that looked at the relationship between individuals’ salaries and their age, education, and where they live might tell us a \sphinxstyleemphasis{lot} about the importance of a college degree to earnings (which we could see by the large and statistically significant coefficient on having a college degree), even if it only explains a small amount of overall variation in salaries (e.g., the R\sphinxhyphen{}Squared might only be 0.2).

\sphinxAtStartPar
This distinction also has important implications when working with more opaque supervised machine learning techniques, like deep learning, random forests, or SVMs. These techniques are often referred to as “black boxes” because exactly how different impute factors relate to the predictions that the model makes is impossible to understand (in other words, it’s like the input data is going into a dark box we can’t see into, and then predictions are magically popping out the other side). These models can be very useful for answering Passive\sphinxhyphen{}Prediction Questions, as they can accommodate very unusual, non\sphinxhyphen{}linear relationships between input factors and predicted values, but because these relationships are opaque to us, the data scientist, they don’t really help us understand the problem space.


\section{The “Passive” in Passive Prediction}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:the-passive-in-passive-prediction}}
\sphinxAtStartPar
The term “passive” in “Passive Prediction Questions” is meant to emphasize the distinction between Passive Prediction Questions and Causal Questions. Both Passive Prediction Questions and Causal Questions can be thought of as trying to “predict” some future outcome, but they differ in the contexts in which their predictions are valid. A full accounting of the distinction between Passive Prediction Questions and Causal Questions will have to wait until we have covered Causal Questions in detail, for the moment, we can get a sense of things by introducing a very casual definition of what it means for some cause X to effect some outcome Y.

\sphinxAtStartPar
In causal parlance, when we say that some factor X causes outcome Y (and that X is not merely correlated with Y), what we usually mean is that if we were to go out and actively change X, Y would change as a result. This isn’t a fully rigorous definition, but it drives home that causation is about what happens when we \sphinxstyleemphasis{actively manipulate} X.

\sphinxAtStartPar
To see how this distinction illustrated, let’s return to the example of a hospital interested in predicting which patients are likely to experience complications after surgery. Using past patient data, you are able to develop a model that very accurately answers the question “Given their pre\sphinxhyphen{}surgery vitals, how likely is a patient to experience complications after surgery?” Hooray! The hospital uses this model to determine which patients should get extra nursing visits and extra attention during recovery. You’ve done a great job answering a Passive Prediction Question by discovering a pattern in the world — a set of correlations between measurable variables — you can take advantage of.

\sphinxAtStartPar
Now in the course of developing this model, suppose you discover that one of the strongest predictors of complications after surgery is patient blood pressure — patients with high blood pressure are substantially more likely to experience complications than those with normal blood pressure. This leads you to wonder whether treating patients with high blood pressure with pressure\sphinxhyphen{}reducing medications prior to surgery might reduce complications. In other words, you now want to know the effect of going into the world and manipulating patient blood pressure — a Causal Question.

\sphinxAtStartPar
In the first case, you really don’t care if blood pressure is \sphinxstyleemphasis{causing} the surgical complications, by which we mean you don’t care if reducing blood pressure would reduce complications, or whether high blood pressure is just an easily observable symptom of an underlying condition that is the root cause of surgical complications (like leading a stressful life, or having relationship problems at home). In either case, the \sphinxstyleemphasis{correlation} is sufficient for your purposes of identifying patients you need to keep tabs on.

\sphinxAtStartPar
But if you want to know what would happen if you directly manipulated blood pressure, knowing that blood pressure and complications are correlated is not sufficient. After all, if living alone results in high blood pressure \sphinxstyleemphasis{and} difficulty recovering from surgery, then treating patient blood pressure may have no effect at all!

\sphinxAtStartPar
When answering Passive Prediction Questions, we are searching for correlations we can leverage to make accurate predictions, not causal relationships we can directly manipulate to shape outcomes. Indeed, those who specialize in answering Passive Prediction Questions (like computer scientists who specialized in supervised machine learning) don’t really care that “correlation does not (necessarily) imply causation.”

\sphinxAtStartPar
\sphinxincludegraphics{{correlation}.png}


\section{How do Large Language Models (LLMs) Fit Into This?}
\label{\detokenize{30_questions/20_using_passive_prediction_questions:how-do-large-language-models-llms-fit-into-this}}
\sphinxAtStartPar
Given their emergence as one of the most high profile examples of something people call “AI” these days, it’s worth directly addressing how LLMs like chatGPT, Llama, Bard, etc. fit into this framework.

\sphinxAtStartPar
One of the most powerful ways to understand LLMs is to think of them as tools for answering “If I came across this text {[}whatever text is in the LLMs prompt, pre\sphinxhyphen{}prompts, or other inputs in the model’s context window{]} while wandering around the internet,%
\begin{footnote}[1]\sphinxAtStartFootnote
A more accurate way to phrase this would be “If I came across this text \sphinxstyleemphasis{in my training data}…” For most LLMs, “my training data” is a corpus that consists of both text that is \sphinxstyleemphasis{not} on the internet — like every book every published — and fails to include many things that \sphinxstyleemphasis{are} on the internet but which are difficult to crawl in an automated way. You can learn more about the corpus of internet text used by many LLMs — the 9.5 petabyte \sphinxstyleemphasis{Common Crawl} dataset — \sphinxhref{https://foundation.mozilla.org/en/research/library/generative-ai-training-data/common-crawl/}{here.} But it does seem very clear the \sphinxstyleemphasis{vast} majority of training data comes from the internet. LLMs makers are becoming increasingly caggy about what data they use for training — in part because they don’t wish to make copyright lawsuits against them \sphinxstyleemphasis{too} easy — but books made up only \sphinxhref{https://arxiv.org/abs/2302.13971}{4.5\% of the training data for Meta’s first version of LLaMa}.
%
\end{footnote} what word am I most likely to encounter next?” LLMs then ask this question over and over, adding the newly selected word to the context window one at a time and then feeding the updated “conversation” back into itself as input to help it predict the next word. Indeed, this repetitiveness is why the technology behind most LLMs is called a \sphinxstyleemphasis{recurrent} neural network — because it keeps adding a word to the “conversation” you are having (it’s “context window”), then feeding the updated conversation back into the model as an updated input.

\sphinxAtStartPar
To be clear, this is a little reductionist. First, LLMs are able to \sphinxstyleemphasis{abstract} away from literal text to something like the meaning of words — they recognize that “pet” and “dog” have similar meanings in the sentences “I took my dog for a walk” and “I took my pet for a walk” — but even these abstractions are the result of looking for common patterns across existing text. And second, LLMs are also “finetuned” by having humans rate responses. But these nuances don’t change the fact that \sphinxstyleemphasis{fundamentally} LLMs are tools for recapitulating text and ideas that already exist in the world, with a strong bias towards what humans have \sphinxstyleemphasis{tended} to write most on the internet — a truth that both explains some of their power, but also helps to explain their fundamental limitations (e.g., their complete lack of fidelity to the truth, the fact they reflect societies’ gender and racial biases, etc.).


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Passive Prediction Questions and Internal Validity}
\label{\detokenize{30_questions/23_passive_prediction_internal:passive-prediction-questions-and-internal-validity}}\label{\detokenize{30_questions/23_passive_prediction_internal::doc}}
\sphinxAtStartPar
As with Exploratory Questions, when answering Passive Prediction Questions there are two major types of concerns: those related to internal validity, and those related to external validity.


\subsection{Internal Validity}
\label{\detokenize{30_questions/23_passive_prediction_internal:internal-validity}}
\sphinxAtStartPar
Of all the places where data science is fragmented, none is more evident than in how data scientists evaluate how effectively we think a model is representing our data, \sphinxstyleemphasis{especially} when focused on prediction.

\sphinxAtStartPar
The first data science perspective on evaluating the internal validity of a model comes from the field of statistics. Statisticians have approached evaluating model fit with, unsurprisingly, methods based on the idea of random sampling and the properties of statistical distributions. They make assumptions about the distributions underlying data and use those to derive theoretically\sphinxhyphen{}motivated metrics. That’s the origin of statistics like Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), as well as the emphasis on the validity of the standard errors assigned to factors on the right\sphinxhyphen{}hand side of the regression.

\sphinxAtStartPar
When computer scientists were first developing their own machine learning techniques… I’m editorializing a little here, but I think it’s safe to say that initially they either didn’t \sphinxstyleemphasis{know about} a lot about these metrics, or they thought that they could do a better job investing their own. So they developed the “split\sphinxhyphen{}train\sphinxhyphen{}test” approach to model evaluation: they split their data into two parts, train their model on part of the data, then test how well the model is able to predict the (known) outcomes in the test dataset.

\sphinxAtStartPar
Of course, over time these two fields have largely converged in adopting one another’s methods, and some—like cross\sphinxhyphen{}validation—live comfortably in the middle. But if you’re ever wondering why, when you get to a machine learning class, it seems like everything you learned in stats has been abandoned (or end up in a stats class and have the opposite experience), it’s largely an artifact of parallel development of methods of model evaluation in computer science and statistics departments.

\sphinxAtStartPar
But the ins and outs of fitting machine learning or statistical models is not the comparative advantage of this text, and so our time is better spent turning to External Validity.

\sphinxstepscope


\section{Passive Prediction Questions and External Validity}
\label{\detokenize{30_questions/24_passive_prediction_external:passive-prediction-questions-and-external-validity}}\label{\detokenize{30_questions/24_passive_prediction_external::doc}}
\sphinxAtStartPar
Where \sphinxstyleemphasis{internal validity} is a measure of how well a model captures the meaningful variation in the data we already have, \sphinxstyleemphasis{external validity} is a measure of how well we think that our model is likely to perform when faced with new data.

\sphinxAtStartPar
As we learned before, the external validity of a model is specific to the context in which a model is being used. A model will generally have very \sphinxstyleemphasis{high} external validity when used to answer Passive\sphinxhyphen{}Prediction Questions in a setting that is very similar to the setting from which the data used to train the model was collected, but \sphinxstyleemphasis{low} external validity when applied in geographically, temporally, or socially different settings.

\sphinxAtStartPar
Some of the factors that influence the External Validity to Passive Prediction Questions are the same as those that shape the External Validity of Exploratory Questions, such as the population represented in the data (patterns in data from one country will often differ from patterns in data from another country), the time period in question (consumer behavior may vary across seasons, and many patterns in data change over longer timespans). But there are other concerns that are a little more specific to Passive Prediction Questions:


\subsection{Extrapolation and Training Parameter Ranges}
\label{\detokenize{30_questions/24_passive_prediction_external:extrapolation-and-training-parameter-ranges}}
\sphinxAtStartPar
Tools for evaluating internal validity help ensure that statistical and machine learning models will tend to fit the data on which they are trained relatively well. However, while most statistical models are capable of generating predicted values over a very broad range of input values, their reliable outside the range of values on which they were trained is often very limited. Asking a model to make predictions for inputs on which the model wasn’t trained is called \sphinxstyleemphasis{extrapolating}, and is a great way to get oneself into trouble.

\sphinxAtStartPar
To illustrate, consider the two models in the figure below (\sphinxhref{https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/06LeastSquares/extrapolation/complete.html}{source})—one a linear fit, and one a higher\sphinxhyphen{}order polynomial. Both model the data similarly \sphinxstyleemphasis{in the range for which data is available} — and so will perform similarly when one uses the metrics described above to evaluate the model’s internal validity — but make very different predictions when asked to extrapolate values of \sphinxcode{\sphinxupquote{x}} below 0 or above 2.

\sphinxAtStartPar
\sphinxincludegraphics{{extrapolation}.jpg}

\sphinxAtStartPar
In addition to reducing overfitting, strategies like regularization are designed to constrain the “wonkiness” of models outside the domain on which they were trained, but almost by definition, absent data in those extended ranges, there’s no way to know for certain whether the model will generalize.


\subsubsection{What Constitutes Extrapolation?}
\label{\detokenize{30_questions/24_passive_prediction_external:what-constitutes-extrapolation}}
\sphinxAtStartPar
In the example above, the term extrapolation refers to predicting values below 0 and above 2. But what constitutes an extrapolation depends in part on the complexity of the model. With a nice, interpretable linear model, it’s not hard to have confidence that the model will make a reasonable prediction for \(x=0.5\), even though that specific value wasn’t in the training data. But when one begins to work with highly non\sphinxhyphen{}linear models that allow for interactions and aren’t easily interpretable — such as deep learning neural networks — the only place one can feel sure of the behavior of the model is at the exact data points in the training data. The same flexibility that allows these models to accommodate unusual non\sphinxhyphen{}linear relationships can also lead to bizarre behavior between actual points in the training data. A credit risk model may make perfectly reasonable predictions for a married 45\sphinxhyphen{}year\sphinxhyphen{}old women of Hispanic descent who lives in Colorado and a married 47\sphinxhyphen{}year\sphinxhyphen{}old women of Hispanic descent who lives in Colorado because both of those profiles were present in the training data, but make a crazy prediction for a a married 46\sphinxhyphen{}year\sphinxhyphen{}old women of Hispanic descent who lives in Colorado or a married 45\sphinxhyphen{}year\sphinxhyphen{}old women of Hispanic descent who lives in Montana.

\sphinxAtStartPar
Indeed, it’s precisely for this reason that for many high stakes decisions, regulators are increasingly requiring the use of interpretable models that include guarantees (like monotonicity).

\sphinxAtStartPar
Basically, the more flexible the model, the more data points are required to constrain the model’s behavior (the so\sphinxhyphen{}called “curse of dimensionality”), and the more cautious you should become. There’s a reason that LLMs hallucinate despite being fed unfathomably large amounts of data.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
A common misconception among young data scientists is that the split\sphinxhyphen{}train\sphinxhyphen{}test workflow commonly used in machine learning innoculates against external validity concerns. After all, the idea of split\sphinxhyphen{}train\sphinxhyphen{}test is that models are trained on one set of observations and evaluated against an entirely different set of observations.

\sphinxAtStartPar
While split\sphinxhyphen{}train\sphinxhyphen{}test can \sphinxstyleemphasis{help} reduce external validity concerns by guarding against over\sphinxhyphen{}fitting, a fundamental limitation of the workflow is that training observations and test observations both come from the \sphinxstyleemphasis{same context}. Indeed, because test and training datasets are created by taking a single dataset and randomly splitting the observations, they \sphinxstyleemphasis{should} always have the same properties (at least in expectation) — a guarantee one certainly won’t get when moving from the data used to build a model to a real world deployment.
\end{sphinxadmonition}


\subsection{Adversarial Users}
\label{\detokenize{30_questions/24_passive_prediction_external:adversarial-users}}
\sphinxAtStartPar
Another external validity concern for Passive Prediction Models is \sphinxstyleemphasis{adversarial users}. Adversarial users are users who deliberately attempt to subvert a statistical or machine learning model. The idea of adversarial users might seem like the stuff of spy novels, but they’re actually \sphinxstyleemphasis{much} more common than you might think.

\sphinxAtStartPar
To illustrate, consider the Essay RoboGrader. Training an algorithm to answer the question “if a human English professor read this essay, what score would they give it?” is relatively straightforward — get a bunch of essays, give them to some English professors, then fit a supervised machine learning algorithm to that training data. What could go wrong?

\sphinxAtStartPar
The problem with this strategy is that the training data was generated by humans \sphinxstyleemphasis{who knew they were writing essays for humans to read}. As a result, they wrote good essays. The machine learning algorithm then looked for correlations between human essay rankings and features of the essays, and as a result it could easily predict essay scores, at least on a coarse scale.

\sphinxAtStartPar
But what happens when humans realize they aren’t being graded by humans? Well, now instead of writing for a human, they will write for the algorithm. They figure out what the algorithm rewards — big, polysyllabic words (don’t worry, doesn’t matter if they’re used correctly), long sentences, and connecting phrases like “however” — and stuff them into their essays.%
\begin{footnote}[1]\sphinxAtStartFootnote
I cannot for the life of me find the podcast where I first discovered this phenomenon being discussed, but it had lots of great colorful examples of school kids doing this. For now all I can find are these articles: \sphinxhref{https://www.wbur.org/hereandnow/2020/09/03/online-learning-algorithm}{here}, \sphinxhref{https://www.npr.org/2018/06/30/624373367/more-states-opting-to-robo-grade-student-essays-by-computer}{here}, \sphinxhref{https://www.theverge.com/2012/4/23/2969331/erater-robotic-essay-grader-effectiveness}{here}, \sphinxhref{https://www.nytimes.com/2012/04/23/education/robo-readers-used-to-grade-test-essays.html}{here} and \sphinxhref{https://www.vice.com/en/article/pa7dj9/flawed-algorithms-are-grading-millions-of-students-essays}{here}.
%
\end{footnote}

\sphinxAtStartPar
This works because the essay writers who used polysyllabic words and long sentences in the training data happened to also be the students who were writing good essays. These were reliable predictors of scores in essays people wrote for humans. But they \sphinxstyleemphasis{aren’t} a reliable predictor of essay quality in a world where students know the essays \sphinxstyleemphasis{aren’t} being written for humans, just machines.

\sphinxAtStartPar
Another way of thinking about this is that we’re back to the classic problem of alignment problems: they \sphinxstyleemphasis{want} the algorithm to reward good writing, but that’s not actually what they trained it to do. In this case, however, the alignment problem is rearing its head because people are actively trying to exploit this difference.

\sphinxAtStartPar
While these examples are fun, not all are, and adversarial users is a HUGE and never ending problem for spam filters, network intrusion detection, credit and fraud monitoring, and more.


\subsubsection{Goodhart’s Law, Cambell’s Law, and the Lucas Critique}
\label{\detokenize{30_questions/24_passive_prediction_external:goodhart-s-law-cambell-s-law-and-the-lucas-critique}}
\sphinxAtStartPar
It’s worth noting that the threat of adversarial users is not a new phenomenon unique to the age of machine learning. Indeed, the idea of adversarial users is closely linked to at least three much older ideas:
\begin{quote}

\sphinxAtStartPar
“When a measure becomes a target, it ceases to be a good measure.”
\end{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Goodhart\%27s\_law}{Goodhart’s Law, named for Charles Goodhart.}

\end{itemize}
\begin{quote}

\sphinxAtStartPar
“The more any quantitative social indicator is used for social decision\sphinxhyphen{}making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor”
\end{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Campbell\%27s\_law}{Campbell’s Law, named for Donald Campbell}

\end{itemize}
\begin{quote}

\sphinxAtStartPar
“Given that the structure of any {[}statistical{]} model consists of optimal decision rules of economic agents, and that optimal decision rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in policy will systematically alter the structure of {[}statistical{]} models.”
\end{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Lucas\_critique}{Lucas Critique, named for Robert Lucas}

\end{itemize}

\sphinxAtStartPar
And since no idea is serious until it’s been importalized in an XKCD comic:

\sphinxAtStartPar
\sphinxincludegraphics{{goodharts_law}.png}


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{The Right Way To Be Wrong}
\label{\detokenize{30_questions/26_passive_prediction_errors:the-right-way-to-be-wrong}}\label{\detokenize{30_questions/26_passive_prediction_errors::doc}}
\sphinxAtStartPar
With the rise of online machine learning competitions like Kaggle and an academic literature fixated on publishing papers showing marginal improvements in performance metrics on standard benchmarks, you could be forgiven for thinking that the hardest part of data science is finding the right model and features to max out standard metrics like Area Under the Curve (AUC), Accuracy (share of cases correctly classified) or F1 scores.

\sphinxAtStartPar
However, this is far from the case when it comes to solving real\sphinxhyphen{}world problems. Yes, advancement in academic computer science is often tied to one’s ability to write a new algorithm that performs marginally better on standard benchmarks, and most problem sets or online competitions you encounter will pre\sphinxhyphen{}specify how model performance will be evaluated.

\sphinxAtStartPar
But when it comes to solving real\sphinxhyphen{}world problems, determining what success looks like is actually a core part of your job as a data scientist. And what makes this task difficult is not how you measure your model’s successes — the number of true positives and true negatives the model generates — but in determining the types of mistakes it makes when it gets things wrong.

\sphinxAtStartPar
Understanding how a model fails is just as important as minimizing mistakes in the first place (i.e., maximizing accuracy). Depending on the context, there can be \sphinxstyleemphasis{huge} asymmetries regarding the consequences of false positives and false negatives. Tell someone with cancer they’re fine (false negative), and the result may be the death of the patient from an easily treatable condition; tell someone healthy there’s a chance they have cancer, and you may cause stress and additional tests, but the patient death is very unlikely.

\sphinxAtStartPar
Similarly (but in a much less scary context), classify a credit card applicant as a “good credit risk” who is not actually credit\sphinxhyphen{}worthy (a false positive), and your company may lose the credit limit on the card they issue; classify someone as high risk who is actually not, and your company may lose the transaction fees that customer would have generated, but they won’t lose tens of thousands of dollars in unpaid bills.

\sphinxAtStartPar
When answering Passive Prediction Questions, the choice of how to balance true positives, true negatives, false positives, and false negatives is \sphinxstyleemphasis{the} bridge between the math of statistics and machine learning and the specifics of real\sphinxhyphen{}world problems. And as a result, an ability to speak thoughtfully about how to balance these interests is one of the most important differentiators between data scientists who have only ever fit models for problem sets and data scientists business leaders trust to solve their problems.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In case you need more motivation to care about how best to distribute your true and false positives and negatives to best solve your stakeholder’s problem, allow me to offer the following: if you are comfortable just endorsing a single success metric (accuracy, F1 score, AUC), \sphinxstyleemphasis{most} Passive Prediction Questions can be answered relatively well by automated tools. And that means that if the only thing that differentiates you from the next data scientist (or generation of chatGPT) is that you can get a slightly higher AUC than the person sitting next to you, how much value do you think you are likely bringing to your stakeholder (and thus how well paid)?

\sphinxAtStartPar
Don’t believe me? Consider scikit\sphinxhyphen{}learn — by design, nearly all of the algorithms in that package have the exact same APIs — you run \sphinxcode{\sphinxupquote{test\_train\_split()}}, \sphinxcode{\sphinxupquote{.fit()}}, and \sphinxcode{\sphinxupquote{.predict()}}. That means it’s almost trivially easy to write a program that just loops over all the models in the library, fits them, and checks their performance against the simple metric you chose. Indeed, products exist that do just this, often under names like \sphinxcode{\sphinxupquote{AutoML}}.

\sphinxAtStartPar
Will these do as well as a well trained data scientist? Not yet — there’s skill in feature engineering and choosing which path to go down when computationally constrained. And if you’re someone working at the forefront of developing new machine learning algorithms or infrastructure, then this does not apply to you. But if you are someone who is primarily interested in applying the best tools of data science to solve real world problems, then you should also bear in mind that anything easily computable lends itself to automation.%
\begin{footnote}[1]\sphinxAtStartFootnote
An idea closely related to a point made by Steven Pinker in his 1994 book \sphinxstyleemphasis{The Language Instinct}: “The main lesson of thirty\sphinxhyphen{}five years of AI research is that the hard problems are easy and the easy problems are hard. The mental abilities of a four\sphinxhyphen{}year\sphinxhyphen{}old that we take for granted – recognizing a face, lifting a pencil, walking across a room, answering a question – in fact solve some of the hardest engineering problems ever conceived… As the new generation of intelligent devices appears, it will be the stock analysts and petrochemical engineers and parole board members who are in danger of being replaced by machines. The gardeners, receptionists, and cooks are secure in their jobs for decades to come.”
%
\end{footnote}

\sphinxAtStartPar
Thinking carefully about your stakeholder’s problem and being able to get them to articulate the relative value they place on true and false positives and negatives, then translating that domain expertise into an optimization problem — \sphinxstyleemphasis{that} is a task that requires substantially more critical thinking and interpersonal skills, and consequently is less likely to be made obsolete any time soon.
\end{sphinxadmonition}


\section{The Problem with Accuracy}
\label{\detokenize{30_questions/26_passive_prediction_errors:the-problem-with-accuracy}}
\sphinxAtStartPar
Let’s begin our discussion about balancing true and false positives and negatives with my least favorite metric for classification problems: Accuracy.

\sphinxAtStartPar
There is perhaps no better way for a data scientist to demonstrate they don’t know what they’re doing than for them to proudly proclaim that their model has an accuracy score of ninety\sphinxhyphen{}something percent without additional context. And yet it is a mistake that I see constantly. It is as though, being students, young data scientists implicitly assume that accuracy scores, like grades, exist on an absolute scale, where values in the 90s are “A”s and something to celebrate and values in the 70s are “C”s and something to feel bad about, when in reality neither is necessarily the case.

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Reporting Accuracy Without Context}

\sphinxAtStartPar
There is perhaps no better way for a data scientist to demonstrate they don’t know what they’re doing than for them to proudly proclaim that their model has an accuracy score of ninety\sphinxhyphen{}something percent without additional context.
\end{sphinxShadowBox}

\sphinxAtStartPar
How do I know this fallacy is common, you ask? As Director of Admissions for the \sphinxhref{https://datascience.duke.edu/}{Duke Masters of Interdisciplinary Data Science (MIDS)} program, I read hundreds of Statements of Purpose essays and resumes every year from aspiring data scientists from around the world. And despite having done this for years, I continue to be shocked by the number of applicants who proudly proclaim something like “I fit a model using XYZ method and was able to achieve a 95\% accuracy score,” or report an accuracy score in the 90s in their resume as though those numbers, absent context, were meaningful.

\sphinxAtStartPar
So why is reporting accuracy scores without context such a problem? There are at least three reasons.


\subsection{Accuracy and Imbalanced Data}
\label{\detokenize{30_questions/26_passive_prediction_errors:accuracy-and-imbalanced-data}}
\sphinxAtStartPar
Most data you will encounter in your career will be \sphinxstyleemphasis{imbalanced}, meaning that one of the outcomes you are trying to predict with your model (assuming a classification task) will be much, much less prevalent than the other. In these situations, because accuracy is just “the share of cases correctly classified,” getting high acccuracy can be achieved trivially by always predicting the more prevalent outcome.

\sphinxAtStartPar
To illustrate, consider routine mammograms. Mammograms are x\sphinxhyphen{}rays of women’s breast tissue used to screen for early signs of breast cancer. In the United States, it is recommended that all women over 40 get a mammogram every two years. Unsurprisingly, therefore, \sphinxstyleemphasis{vast} majority of routine mammograms are medically unremarkable. According to the Susan G. Komen society, roughly 90\% of routine mammograms are perfectly normal and require no followup.%
\begin{footnote}[2]\sphinxAtStartFootnote
The vast majority of the roughly 10\% of scans that are abnormal are eventually determined to be false positives.
%
\end{footnote} Thus a “model” that reports that \sphinxstyleemphasis{any} routine mammogram it is given is normal will immediately achieve an accuracy score of 90\%.

\sphinxAtStartPar
Moreover, most data scientists wouldn’t even consider 90/10 data to be particularly imbalanced. In any given year in the United States, only about 3\% of single\sphinxhyphen{}family residential mortgages are in a state of delinquency%
\begin{footnote}[3]\sphinxAtStartFootnote
In other words, they have failed to make a mortgage payment for at least a certain period of days.
%
\end{footnote}, and fraudulent credit card purchases \sphinxhref{https://www.federalreserve.gov/newsevents/pressreleases/other20181016a.htm}{make up less than one\sphinxhyphen{}tenth of 1\% of all credit card transactions}. That means a “model” that reports all mortgages are in good standing or that all credit card transactions are valid will immediately have accuracy scores of 97\% and \(>\) 99.9\%, respectively.


\subsection{Relative to What?}
\label{\detokenize{30_questions/26_passive_prediction_errors:relative-to-what}}
\sphinxAtStartPar
The second problem with reporting accuracy scores absent context is that the value of a model can only ever be evaluated relative to what came before. A model with a 93\% accuracy score is unlikely to be of particular value to a business if the model they were using before you arrived had an accuracy score of 98\%, and your model does not have any other benefits to offset its lower accuracy. Similarly, a model with an accuracy score of 70\% may constitute a considerable innovation to a business that could not make predictions more accurately than with 50\%\sphinxhyphen{}50\% odds.

\sphinxAtStartPar
Indeed, a related issue with treating accuracy as an absolute scale akin to academic grades is that model performance depends on the amount of \sphinxstyleemphasis{signal} in the data on which it is trained. Data only has so much predictive information in it, and in theory, the way a model \sphinxstyleemphasis{should} be evaluated is in terms of how close it comes to harnessing that full predictive potential of the data on which it is being trained. Of course, we are not gods, and so we will never know the exact predictive potential of a given dataset, but the principle is one to bear in mind — the potential of a model is bounded by the data on which is being trained, and the only way to get a model that exceeds that true performance frontier is by overfitting your data (creating an \sphinxstyleemphasis{illusion} of better performance that will not hold up when the model is actually deployed).


\subsection{But What About The Mistakes?}
\label{\detokenize{30_questions/26_passive_prediction_errors:but-what-about-the-mistakes}}
\sphinxAtStartPar
The third reason reporting accuracy scores without context — and indeed the reason that accuracy is a problematic metric in general — is that it tells you nothing about the types of misclassifications your model is making. Are all its errors false positives? Are they all false negatives? In what ratio do they occur?

\sphinxAtStartPar
Accuracy says \sphinxstyleemphasis{nothing} about how different types of mistakes are being balanced, which is why accuracy is sometimes a fun statistic to use on problem sets but a \sphinxstyleemphasis{terrible} metric in the real world.


\section{ROC and Area Under the Curve (AUC)}
\label{\detokenize{30_questions/26_passive_prediction_errors:roc-and-area-under-the-curve-auc}}
\sphinxAtStartPar
“OK, fine,” I hear you say, “but no one uses accuracy anymore — we all use ROC AUC scores!”

\sphinxAtStartPar
First, again, I can tell you from reading hundreds of essays and looking at hundreds of resumes, accuracy is still an \sphinxstyleemphasis{extremely} commonly used metric. But setting that aside…

\sphinxAtStartPar
Yes, AUC is certainly a more holistic metric than accuracy. Where accuracy evaluates the share of cases correctly classified, AUC averages the share of correct positive predictions across the full range of classification thresholds (one of the reasons it is commonly used in competitions). But it is not a substitute for thinking carefully about the correct metric \sphinxstyleemphasis{for the specific problem you are seeking to solve}.

\sphinxAtStartPar
First, most models are deployed at a specific classification threshold, so averaging across all classification thresholds may create a good \sphinxstyleemphasis{general purpose} indicator of a model’s performance, but it makes AUC poorly suited to evaluating how well a model will perform in any \sphinxstyleemphasis{specific} context (i.e., at a specific classification threshold).

\sphinxAtStartPar
Second, the ROC AUC metric is myopically focused on the proportion of correct positive predictions. But depending on our problem, we may also care about the ratio of false negatives to false positives or other properties of our negative predictions.


\section{Choosing the Best Way to be Wrong}
\label{\detokenize{30_questions/26_passive_prediction_errors:choosing-the-best-way-to-be-wrong}}
\sphinxAtStartPar
How, then, should one approach being more thoughtful about model evaluation?

\sphinxAtStartPar
The first step is always to evaluate the \sphinxstyleemphasis{relative value} of the four different types of classifications: true positives, true negatives, false positives, and false negatives. Writing a model that reviews the results of blood tests for signs of a terminal but treatable disease? You probably want to associate a strong negative value with false negatives (where you tell a sick patient they’re healthy) and a smaller negative value with false positives (being told you might have a lethal condition is stressful, even if later tests (which may have their own risks) may show it to be a false positive!). And you may then normalize your true positives and true negatives to zero.

\sphinxAtStartPar
You can then fit your classification model and, for each classification threshold, calculate the “cost” of the resulting distribution of true and false positives and negatives. Find the model and classification threshold that minimizes this problem\sphinxhyphen{}specific cost function, and you’ve identified the model and threshold that’s best \sphinxstyleemphasis{for your specific problem}.

\sphinxAtStartPar
Where do these values come from? Sometimes your stakeholder will be able to tell you the actual financial cost of different types of errors (e.g., when deciding whether to issue someone a credit card), but other times these values are more subjective. What’s the relative cost of falsely telling someone they may have a terminal disease condition? How might that vary depending on the risks associated with any followup procedures required to confirm a diagnosis or the amount of time it takes for the diagnosis to be confirmed? Those are hard, subjective questions you may not have the domain expertise to answer yourself. But because you \sphinxstyleemphasis{understand} the role of these values in how your eventual model will operate, you can raise these questions with your stakeholder (who should have better domain knowledge) and solicit values from them.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Up until now, we’ve emphasized how we manage errors in the context of discrete, binary classification tasks, but it is worth emphasizing that this is only because binary classification is the easiest context in which to think about these problems. However, the issues raised her apply equally to classification tasks with more than two categories, and to efforts to answer Passive Prediction Questions about continuous outcomes. Latent in any model you use is a cost function, and implicit in that cost function is how mistakes are evaluated.

\sphinxAtStartPar
Linear regression, for example, minimizes the sum of squared errors across all observations, and (by default) it gives equal weight to the squared error associated with each observation. But if you don’t feel that’s an appropriate weighting scheme, you are not bound to it — weighted linear regression is a version of linear regression where the user provides a set of weights to associate with each observation. Have some customers you know are more valuable to your company? Perhaps you want to have the model give more weight to errors associated with those customers so the final model performs better for those customers. Or working with data from stores with different sales volumes? Maybe you want to give more weight to stores with larger sales volumes.

\sphinxAtStartPar
Don’t want to work with squared errors at all? Great! There’s a whole discipline called \sphinxhref{https://www.statsmodels.org/stable/examples/notebooks/generated/robust\_models\_1.html}{robust linear modeling} that uses different norms for evaluating errors (often with the goal of reducing the influence of outliers, as the name implies, but all they are doing is modifying how the errors the model seeks to minimize are handled).
\end{sphinxadmonition}


\section{The Ethics of Misclassifications}
\label{\detokenize{30_questions/26_passive_prediction_errors:the-ethics-of-misclassifications}}
\sphinxAtStartPar
How our models make mistakes is often not just a financial decision; when statistical models are used to make high stakes decisions — like who is sent to prison and who is let out on bail pending trial — how errors are distributed can often be deeply, \sphinxstyleemphasis{deeply} ethically frought issue.

\sphinxAtStartPar
To illustrate, let’s consider the example of Risk Assessment models. Risk Assessment models are models used in the US criminal justice system that are designed to help judges and parole boards determine the risk that a criminal defendant or incarcerated person may re\sphinxhyphen{}offend if released from jail. These are increasingly commonly used for purposes like determining whether arrested individuals should be released while they await trial and whether incarcerated individuals should be paroled (released before the end of their prison sentence to a half\sphinxhyphen{}way house or monitored release).

\sphinxAtStartPar
The way Risk Assessment models are being used in the US has many, many problems. But a recent debate about an aspect of one of the most commonly used models — the COMPAS Risk Model — is illuminating about the moral issues that arise when handling misclassifications. If you would like to learn more about this debate and difference concepts of algorithmic fairness, I strongly recommend \sphinxhref{https://www.yalelawjournal.org/article/bias-in-bias-out}{Bias In, Bias Out} (2019), a Yale Law Journal article by \sphinxhref{https://www.law.upenn.edu/faculty/sgmayson}{Sandra G. Mayson}.

\sphinxAtStartPar
In 2016, the investigative news outlet ProPublica published a piece about COMPAS called \sphinxhref{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}{Machine Bias}. In it, journalists Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner used data from Florida to evaluate how well the COMPAS risk assessment model predicted criminal recitivism (the term for when a criminal defendant commits another crime in the future).

\sphinxAtStartPar
The \sphinxhref{https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}{ProPublica analysis determined}, in part, that:
\begin{quote}

\sphinxAtStartPar
Black defendants were often predicted to be at a higher risk of recidivism than they actually were. Our analysis found that black defendants who did not recidivate over a two\sphinxhyphen{}year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts (45 percent vs. 23 percent).
\end{quote}

\sphinxAtStartPar
In other words, the false positive rate for Black defendants was higher than it was for White defendants.

\sphinxAtStartPar
While apparently rather damning of the model, COMPAS’ response was that this was actually a consequence of the fact that their model generated equal True Positive Rates across Black and White defendants. And while I am generally loath to defend COMPAS, as I think it has \sphinxstyleemphasis{many} problems and should not be in use, in this particular case they have a point.

\sphinxAtStartPar
As Sandra Mayson points out in her paper, the issue is that the likelihood someone will be arrested for a future crime (be arrested for recitivism) is imbalanced by race. As a consequence, a model that has the same True Positive Rate for Black and White defendants (i.e., the share of people predicted to re\sphinxhyphen{}offend who do re\sphinxhyphen{}offend is the same for both groups) will \sphinxstyleemphasis{necessarily} have different False Positive Rates for the two groups.

\sphinxAtStartPar
To illustrate, consider the following figure from Mayson’s paper with two groups, drawn as grey and black. Solid outlines are individuals who do not recitivate, while hollow figures are those who are re\sphinxhyphen{}arrested. In the grey group, 2/10 individuals are re\sphinxhyphen{}arrested, while only 1/10 are re\sphinxhyphen{}arrested in the black group. The vertical line represents the classification threshold used by the model.

\sphinxAtStartPar
Note that this implies the grey outlines are in the situation akin to that of Black Americans (more likely to be arrested for recitivism) and the black outlines are in the position of White Americans (less likely to be rearrested), which is a little confusing.

\sphinxAtStartPar
\sphinxincludegraphics{{mayson_fig1}.png}

\sphinxAtStartPar
With the line in the location shown in the figure, we can see that the model has the same True Positive Rate for both populations — grey and black (50\%). But as a result, the \sphinxstyleemphasis{False} Positive Rate is higher for the grey individuals (2/8). Thus while the ProPublica finding is true — the False Positive Rate of COMPAS is higher for Black defendants — the only way to even this out would be to shift the classification threshold for the black outlines over, as illustrated in Figure 2 from her paper:

\sphinxAtStartPar
\sphinxincludegraphics{{mayson_fig2}.png}

\sphinxAtStartPar
This balances the False Positive Rates for the two groups, but in doing so results in the True Positive Rate being lower for the black outlines.

\sphinxAtStartPar
Can we do better? Well, we could get equal True Positive Rates and False Positive Rates, but only by accepting differential False \sphinxstyleemphasis{Negative} rates, as illustrated in Mayson’s Figure 3:

\sphinxAtStartPar
\sphinxincludegraphics{{mayson_fig3}.png}

\sphinxAtStartPar
As Mayson writes:
\begin{quote}

\sphinxAtStartPar
As this example illustrates, if the base rate of the predicted outcome differs across racial groups, it is impossible to achieve (1) predictive parity; (2) parity in false\sphinxhyphen{}positive rates; and (3) parity in false\sphinxhyphen{}negative rates at the same time (unless prediction is perfect, which it never is). Computer scientists have provided mathematical proofs of this fact.%
\begin{footnote}[4]\sphinxAtStartFootnote
See Alexandra Chouldechova, \sphinxstyleemphasis{Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments}, 5 BIG DATA 153 (2017) and Jon Kleinberg et al., \sphinxstyleemphasis{Inherent Trade\sphinxhyphen{}offs in the Fair Determination of Risk Scores}, LEIBNIZ INT’L PROC. INFORMATICS, Jan. 2017, at 43:1, 43:4.
%
\end{footnote} When base rates differ, we must prioritize one of these metrics at the expense of another. Race neutrality is not attainable.
\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Please note that in the discussion of imbalanced recitivism arrest rates, I said \sphinxstyleemphasis{arrested for} recitivism — a huge problem with all of these risk assessment models is that we know from many studies that even in situations where Black and White Americans \sphinxstyleemphasis{commit} crimes at similar rates (like drug use), Black Americans are substantially more likely to be \sphinxstyleemphasis{arrested} for those crimes. Thus, an imbalance in arrests for recitivism do not necessarily imply differences in \sphinxstyleemphasis{actual} recitivism.

\sphinxAtStartPar
Indeed, this form of inequity is also a problem on the input side — Risk Assessment models take into account whether a defendant has prior convictions, but given Black Americans are more likely to be arrested even in situations where were know criminal behavior occurs at similar rates for Black and White Americans, and Black Americans are likely to have lower\sphinxhyphen{}incomes and thus less likely to have lawyers who can get charges dismissed, these models tend to treat them as higher risk.
\end{sphinxadmonition}

\sphinxAtStartPar
So what is the right answer? Well, there isn’t one — each of these different schemes is defensible under different ethical frameworks. And that’s the problem — there’s no algorithm or machine learning metric that can tell you the “right” answer in a situation like this — it requires critical thought, engagement with {\hyperref[\detokenize{30_questions/05_descriptive_v_prescriptive::doc}]{\sphinxcrossref{\DUrole{std,std-doc}{prescriptive Questions}}}}, and careful discussion with your stakeholder.


\bigskip\hrule\bigskip


\sphinxstepscope


\chapter{Performance Differential Is (Often) A Myth}
\label{\detokenize{30_questions/27_interpretable_models:performance-differential-is-often-a-myth}}\label{\detokenize{30_questions/27_interpretable_models::doc}}
\sphinxAtStartPar
There’s just only so much signal in the data.


\chapter{Regulatory Compliance}
\label{\detokenize{30_questions/27_interpretable_models:regulatory-compliance}}

\chapter{Trusting Model == Trusting Data}
\label{\detokenize{30_questions/27_interpretable_models:trusting-model-trusting-data}}

\chapter{Ethical Transparency}
\label{\detokenize{30_questions/27_interpretable_models:ethical-transparency}}

\chapter{Ease of Adoption}
\label{\detokenize{30_questions/27_interpretable_models:ease-of-adoption}}
\sphinxAtStartPar
(Cite study)


\chapter{No Magic, Not Undue Deference}
\label{\detokenize{30_questions/27_interpretable_models:no-magic-not-undue-deference}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Most models internal, where IP\sphinxhyphen{}black\sphinxhyphen{}box doesn’t matter

\item {} 
\sphinxAtStartPar
Handoff and implementation easier

\item {} 
\sphinxAtStartPar
Ethical review.

\end{itemize}

\sphinxstepscope


\chapter{Using Causal Questions}
\label{\detokenize{30_questions/30_using_causal_questions:using-causal-questions}}\label{\detokenize{30_questions/30_using_causal_questions::doc}}
\sphinxAtStartPar
In our last reading, we learned a little about what it means to measure a causal effect, and why it is inherently difficult. That is a topic we will return to shortly, as understanding why measuring causal effects is hard is key to being able to measure them effectively. But first, take a moment to discuss how Causal Questions come up and are addressed in practice to help contextualize the more technical readings that will follow.

\sphinxAtStartPar
In our previous readings, we learned how answering different types of questions can help us better understand the world around us. By answering Exploratory Questions, we can better understand the contours of our problem — where our problem is most acute, whether there are groups who have figured out how to get around the problem on their own, etc. This, in turn, can help us prioritize our subsequent efforts. By answering Passive Predictive Questions, we can help identify individual entities — patients, customers, products, etc. — to whom we may wish to pay extra attention or recommend certain products. By answering Passive Predictive Questions, we can also automate tasks by predicting how a person (or more complicated process) \sphinxstyleemphasis{would} have classified an entity.

\sphinxAtStartPar
In both cases, however, answering these questions only helps us better understand the world \sphinxstyleemphasis{around} us. But to the extent to which, as data scientists, we want to intervene to directly address problems, we are rarely interested in just knowing about the world around us — we want to \sphinxstyleemphasis{act} on the world, and wouldn’t be great if data science could provide us with a set of tools designed to help us predict the \sphinxstyleemphasis{consequences} of our actions?

\sphinxAtStartPar
Enter \sphinxstyleemphasis{Causal Questions}. Causal Questions ask what \sphinxstyleemphasis{effect} we can expect from \sphinxstyleemphasis{acting} — that is, actively \sphinxstyleemphasis{manipulating} or \sphinxstyleemphasis{intervening} — in the world around us in some way. For example, if we pay to show an ad to a specific customer, what will the \sphinxstyleemphasis{effect} of that choice be the likelihood they buy something on our website? Or if we choose to give a new drug to a patient, what will the \sphinxstyleemphasis{effect} of that choice be on their disease?

\sphinxAtStartPar
Because of their potential to help us understand the future consequences of our actions, it should come as no surprise that the ability to answer Causal Questions is of \sphinxstyleemphasis{profound} interest to everyone from companies to doctors and policymakers. At the same time, however, it may also come as no surprise that answering Causal Questions is an inherently challenging undertaking.

\sphinxAtStartPar
In this reading, we will discuss where Causal Questions arise in practice, and the workflow one goes about answering them, glossing over the nuances of \sphinxstyleemphasis{how} exactly we answer Causal Questions. Then in our next reading, we will turn from workflows to theory and discuss in detail what it actually means to measure the effect of an action \(X\) (e.g., administering a new drug to a patient, or showing an ad to a user) on an outcome \(Y\) (patient survival, customer spending, etc.). This section will, at times, feel a little abstract and woo\sphinxhyphen{}woo, but please hang in there. Answering Causal Questions is as much about critical thinking as it is about statistics, and the concepts introduced here will prove crucial to your ability to be effective in this space.




\section{When Do Causal Questions Come Up?}
\label{\detokenize{30_questions/30_using_causal_questions:when-do-causal-questions-come-up}}
\sphinxAtStartPar
Causal Questions arise when stakeholders want to \sphinxstyleemphasis{do} something — buy a Superbowl ad, change how the recommendation engine in their app works, authorize a new prescription drug — but they fear the action they are considering may be costly and not actually work. In these situations, stakeholders will often turn to a data scientist in the hope that the scientist can “de\sphinxhyphen{}risk” the stakeholder’s decision by providing guidance on the likely effect of the action \sphinxstyleemphasis{before} the action is undertaken at full scale.

\sphinxAtStartPar
\sphinxstyleemphasis{Usually}, the action the stakeholder is considering will not have been pulled out of a hat. Rather, a stakeholder will generally pose a Causal Question because they have some reason to suspect a given course of action may be beneficial. Indeed, Causal Questions often emanate from patterns discovered when answering Exploratory or Passive Predictive Questions.


\subsection{Where Causal Questions Come From — An Example}
\label{\detokenize{30_questions/30_using_causal_questions:where-causal-questions-come-from-an-example}}
\sphinxAtStartPar
For example, suppose the Chief of Surgery at a major hospital is interested in reducing surgical complications. They begin by asking “What factors predict surgical complications?” (a Passive Predictive Question) and develop a predictive model that allows the hospital to identify patients who are likely to have issues so that caretakers can provide additional support to these patients during recovery.

\sphinxAtStartPar
In the course of developing this model, the Chief discovers that one of the strongest predictors of surgical complications is patient blood pressure — patients with high blood pressure are substantially more likely to experience complications than those with normal blood pressure.

\sphinxAtStartPar
This leads the Chief to wonder about whether they could reduce surgical complications if they treated patients with high blood pressure with pressure\sphinxhyphen{}reducing medications prior to surgery. In other words, the Chief Surgeon wants to know “What effect treating patients with high blood pressure would have on surgical complication rates?”

\sphinxAtStartPar
But rather than just starting to give all patients with high blood pressure new drugs (and delay their surgeries while the drugs take effect), the Chief wants \sphinxstyleemphasis{you} to provide a more rigorous answer to their question. After all, high blood pressure may be \sphinxstyleemphasis{causing} the complications (and thus the medicine may help), but it could also be that high blood pressure isn’t the \sphinxstyleemphasis{cause} of the complications, but rather the \sphinxstyleemphasis{symptom} of a third factor that causes both high blood pressure \sphinxstyleemphasis{and} complications that makes people with high blood pressure different from those with low blood pressure — like leading an overly stressful life. The Chief doesn’t need to know whether high blood pressure is the \sphinxstyleemphasis{cause} of complications or just a “warning light” that identifies people at risk to use that information for directing additional support to those patients during recovery; but it \sphinxstyleemphasis{does} matter for determining whether it makes sense to delay surgeries to teach patient high blood pressure!

\sphinxAtStartPar
This is, of course, just one example, and it’s not hard to imagine others. Perhaps your online retailer stakeholder has noticed that one of your competitors has stopped showing customer reviews in the search results, for example, so they suspect it must be improving sales for your competitor and want to know if it would work for your site too! But the point is that Causal Questions generally don’t appear out of the blue, but rather because someone has noticed a pattern in the world and wants to act on it. Thus, many Causal Questions may actually take the form of hypotheses or hunches that your stakeholder wants to be investigated.


\section{The Two\sphinxhyphen{}Fold Challenge of Causal Questions}
\label{\detokenize{30_questions/30_using_causal_questions:the-two-fold-challenge-of-causal-questions}}
\sphinxAtStartPar
In our last reading, we discussed how answering Causal Questions is difficult in part because measuring the effect of any action on any outcome is a definitionally difficult endeavor. But answering Causal Questions is also difficult for a more practical — less epistemological — reason: risk aversion!

\sphinxAtStartPar
As we noted above, stakeholders generally turn to data scientists because they want to know the likely consequences of an action \sphinxstyleemphasis{before they actually undertake the action at full scale.} This may seem obvious, but it bears repeating — not only is answering Causal Questions hard because we never get to measure outcomes in both a universe where our treatment occurs and also a universe where it does not (the Fundamental Problem of Causal Inference), but \sphinxstyleemphasis{also} because stakeholders want to know about the likely consequences of an action they aren’t ready to actually undertake!

\sphinxAtStartPar
As a result, the job of a data scientist who wants to answer a Causal Question is to design a study that not only measures the effect of a treatment, but also does so in a setting that is enough like the context in which the stakeholder wants to act that any measured effect will generalize to the stakeholder’s context.

\sphinxAtStartPar
We call these two objectives of a study \sphinxstyleemphasis{internal validity} (how well the study answers the Causal Question \sphinxstyleemphasis{in the setting the study is conducted}) and \sphinxstyleemphasis{external validity} (how well the results of the study generalize to the context the stakeholder cares about). And to provide value to a stakeholder, a data scientist’s analysis must have both.


\subsection{Internal and External Validity: An Example}
\label{\detokenize{30_questions/30_using_causal_questions:internal-and-external-validity-an-example}}
\sphinxAtStartPar
To illustrate, suppose you work for a medical device company in Boston that wants the US Food and Drug Administration (FDA) to authorize a new cochlear implant your company has developed (a partially surgically implanted device for helping those with certain types of hearing loss regain hearing). Before authorizing the device, the FDA wants to be sure that it’s safe and effective — in other words, it wants to know what the \sphinxstyleemphasis{effect} of authorizing the device for patients throughout the United States would be on patient health.

\sphinxAtStartPar
Your job, therefore, is to conduct a study that (a) convincingly measures the effect of the device on patients (has high internal validity), \sphinxstyleemphasis{and} (b) does so in a way that convinces the FDA that the findings from \sphinxstyleemphasis{your study} are likely to be the same as what would be seen if the device were being used across the United States (has external validity to the context the FDAs cares about).

\sphinxAtStartPar
In medical trials, internal validity is usually ensured by conducting a randomized experiment — referred to as a Randomized Control Trial (RCT) in medical circles — according to a set of FDA requirements. We’ll discuss what features must be present for us to have confidence in the results of a randomized experiment soon, but they are things like making sure that the people in the control group look like the people in the treatment group in terms of things we can measure (age, gender, etc.) to help us feel confident that when people were randomly assigned to control and treatment groups, we didn’t end up in a really unlikely situation where, purely by chance, only men ended up in control group and only women ended up in treatment group.

\sphinxAtStartPar
External validity, by contrast, comes from things like \sphinxstyleemphasis{who} is enrolled in the trial. The average age of children getting cochlear implants is between 2 and 3, so if your study only included children between 12 and 18 months of age, the FDA may worry that the results of the study would not \sphinxstyleemphasis{generalize} to the US population as a whole.

\sphinxAtStartPar
In the context of a clinical trial, this issue of external validity may seem easy to address — just get a sample of people who “look like” the US population (when applying for US FDA approval)! Historically, however, \sphinxhref{https://www.theguardian.com/lifeandstyle/2015/apr/30/fda-clinical-trials-gender-gap-epa-nih-institute-of-medicine-cardiovascular-disease}{women}%
\begin{footnote}[1]\sphinxAtStartFootnote
In 1977, the FDA actually banned enrollment of women of “childbearing potential” from Phase 1 and Phase 2 clinical trials in the \sphinxhref{https://www.womenshealth.gov/30-achievements/04\#:~:text=In\%201977\%2C\%20the\%20FDA\%20issued,thalidomide)\%20causing\%20serious\%20birth\%20defects.}{interest of avoiding birth defects}.
%
\end{footnote} and \sphinxhref{https://www.thelancet.com/journals/lanam/article/PIIS2667-193X(22)00069-2/fulltext}{minorities} have been underrepresented in clinical trial participants.%
\begin{footnote}[2]\sphinxAtStartFootnote
This seems likely to be due, in part, to hesitancy to enroll in clinical trials by individuals aware of past abuses of minority patients, as in the \sphinxhref{https://www.cdc.gov/tuskegee/index.html}{Tuskegee Syphilis Study}.
%
\end{footnote} Moreover, the people designing clinical trials often limit enrollment to participants who, aside from the specific condition being treated, are healthy to avoid complications. This reduction in complications may increase the \sphinxstyleemphasis{internal} validity, but as many patients face more than one health challenge, it may reduce external validity.

\sphinxAtStartPar
Outside of drug or medical device trials, however, external validity can much harder to establish. For example, the functionality of many internet services and apps depends on network effects — testing out a new social feature on Instagram by making it available to only a handful of users in a randomized trial (an A/B test, in the language of tech companies) may not give you a meaningful sense of how the feature would be used if it was visible to all users. And the way that bank customers use a new budgeting app in the context of a two\sphinxhyphen{}week study may not be indicative of how they would use it over the long run when the feature is no longer new.


\subsection{External Validity To \sphinxstyleemphasis{What}}
\label{\detokenize{30_questions/30_using_causal_questions:external-validity-to-what}}
\sphinxAtStartPar
Throughout this text, we will refer to whatever course of action the stakeholder is actually considering pursuing as the “stakeholder’s context.” As a result, when discussing the external validity of a study, we will implicitly be referring to its external validity \sphinxstyleemphasis{to the stakeholder’s context.} But it is worth emphasizing that while the internal validity of a study is a single things — you have some level of confidence that the study measured the effect they set out to measure — external validity is \sphinxstyleemphasis{relative}. A study conducted in a hospital in Denver, Colorado may have good external validity from the perspective of a doctor in a Pheonix, Arizona hospital, but that same study may not have very good external validity from the perspective of a doctor at a hospital in Pune, India. So always remember that external validity is not an absolute property of an analysis, but a property that is \sphinxstyleemphasis{relative} to the context to which one wishes to generalize the results.


\section{The Causal Question Work Flow}
\label{\detokenize{30_questions/30_using_causal_questions:the-causal-question-work-flow}}
\sphinxAtStartPar
Before we dive into the technical details of answering Causal Questions, it’s worth starting with a high\sphinxhyphen{}level overview of how data scientists approach answering Causal Questions.


\subsection{Identify Relevant Previous Studies}
\label{\detokenize{30_questions/30_using_causal_questions:identify-relevant-previous-studies}}
\sphinxAtStartPar
Once a Causal Question has been posed, the next step is \sphinxstylestrong{to identify any research that has already been done} that may help answer your causal question. It’s hard to overstate how often this step is overlooked by data scientists, but it’s \sphinxstyleemphasis{such} a no\sphinxhyphen{}brainer once you think of it! There’s no reason to spend days or weeks trying to design a study to answer a question if someone else has already put the time and money into doing it for you!

\sphinxAtStartPar
If your stakeholder is somebody who works in public policy or medicine, then the first place to look for previous studies is in academic medical or policy journals. But don’t assume that if you aren’t working on a medical or public policy question you won’t be able to find an answer to your question in academic or pseudo\sphinxhyphen{}academic publications — lots of data scientists present research done at private companies at ``industry’’ conferences like the \sphinxhref{https://ide.mit.edu/events/2022-conference-on-digital-experimentation-mit-codemit/}{MIT Conference on Digital Experimentation (CODE@MIT)} or the \sphinxhref{https://netmob.org/}{NetMob Cellphone MetaData Analysis Conference}!

\sphinxAtStartPar
And if you are at a company, ask around! Someone at your own company may have looked into a similar question before, and talking to them could save you a lot of effort.


\subsection{Evaluate Previous Studies}
\label{\detokenize{30_questions/30_using_causal_questions:evaluate-previous-studies}}
\sphinxAtStartPar
If you do find studies, then for each study you will have to ask yourself two questions:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Did the study authors do a good job of answering the Causal Question? \sphinxstyleemphasis{in the context they were studying}}?

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Do I believe that the \sphinxstyleemphasis{context} in which the study was conducted is similar enough to my own context that their conclusions are relevant to me?}

\end{itemize}

\sphinxAtStartPar
This first question is about the \sphinxstyleemphasis{internal validity} of the study, and we’ll talk at length about how to evaluate that in the context of causal inference in the coming weeks. The second question is about the \sphinxstyleemphasis{external validity} (i.e., the generalizability) of the study to your context. There are lots of extremely well\sphinxhyphen{}conducted studies in the world that may be seeking to answer the same question as you, but if, for example, they investigated the effect of a new drug in \sphinxstyleemphasis{young} patients, and your hospital only treats very old patients, you may not be comfortable assuming their results are good predictors for what might happen in your hospital.


\subsection{Plan A New Study}
\label{\detokenize{30_questions/30_using_causal_questions:plan-a-new-study}}
\sphinxAtStartPar
If you were unable to find any studies that answer your Causal Question satisfactorily (either on their own or in combination), then it may be time to do a study of your own!

\sphinxAtStartPar
When most people think about answering Causal Questions, their minds immediately jump to randomized experiments. Randomized experiments are \sphinxstyleemphasis{often} the best strategy for trying to answer Causal Questions, but they are not always the best choice.

\sphinxAtStartPar
Studies designed to answer Causal Questions can be divided into roughly two types: experimental studies and observational studies.

\sphinxAtStartPar
In an experimental study, a researcher has control over everything that happens in the study, including who enrolls in the study and also who in the study gets assigned to the treatment group and who gets assigned to the control group. Nearly all clinical trials, A/B tests where the version of a website or app users see is randomly determined, and field experiments where, say, voters are randomly assigned to receive different types of mailers from political campaigns to measure their effect on voter turnout are all examples of “experimental studies.”

\sphinxAtStartPar
In an observational study, by contrast, researchers use data from a context where the researchers did not control who was treated and who was not. This includes data from public opinion surveys, data on user behavior and demographics, or census data.

\sphinxAtStartPar
(We say studies can be divided into roughly two types because some studies fall into a category sometimes called “quasi\sphinxhyphen{}experimental.” In these studies, researchers were not in control over who was treated and who was not, but they have some reason for thinking that \sphinxstyleemphasis{something in the world} — like a chance storm, or a draft lottery — caused who was treated and who was not to be determined randomly. But these types of studies tend to be more relevant for academics than applied data scientists, and evaluating them is incredibly difficult, so we will largely ignore them in this text.)

\sphinxAtStartPar
While it is sometimes believed that only experimental studies can generate valid answers to Causal Questions, this is \sphinxstyleemphasis{unequivocally untrue}, as is the slightly more generous version of this claim, that experimental studies always constitute the best form of evidence for answering Causal Questions. As we will explore in \sphinxstyleemphasis{great} detail in the coming days, the validity of conclusions drawn from \sphinxstyleemphasis{both} experimental and observational studies rests on whether a number of fundamentally untestable assumptions hold. As a result, both types of studies are capable of providing meaningful answers to causal questions \sphinxstyleemphasis{and} of being deeply misleading.

\sphinxAtStartPar
Moreover, while experimental studies often (but not always) have greater \sphinxstyleemphasis{internal validity} (they are often better able to ensure that they have measured the true causal effect in the lab), this often comes at the expense of lower \sphinxstyleemphasis{external validity}, because ensuring the researchers can control who is treated and who is not requires operating the study take place in a highly monitored, often artificial and unrealistic setting. Observational studies, by contrast, are often based on data collected in the real world, and as a result may yield answers that tell us more about what is likely to happen in our own real\sphinxhyphen{}world application, even if they have somewhat lower internal validity.


\section{Wrapping Up and Next Steps}
\label{\detokenize{30_questions/30_using_causal_questions:wrapping-up-and-next-steps}}
\sphinxAtStartPar
Hopefully, this reading has given you a better sense of \sphinxstyleemphasis{how} Causal Questions are used to solve stakeholder problems, and when and where they come up in the life of a practicing data scientist. In the readings that follow, we will turn first to the details of the \sphinxstyleemphasis{Potential Outcomes Model}, a rigorous, formal statistical framework for understanding the Counterfactual Model of Causality. This framework will not only provide a presentation of the Counterfactual Model of Causality that may be appealing to those who draw intuition from mathematical formalism, but also machinery that we can use to evaluate how much confidence we can have in answers generated using different methods of answering Causal Questions — including both experimental and observational studies.


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Answering Causal Questions}
\label{\detokenize{30_questions/40_answering_causal_questions:answering-causal-questions}}\label{\detokenize{30_questions/40_answering_causal_questions::doc}}
\sphinxAtStartPar
In this reading, we turn the surprisingly slippery question “What do we mean when we say X causes Y, and how do we measure the effect of an action \(X\) (e.g., administering a new drug to a patient, or showing an ad to a user) on an outcome \(Y\) (patient survival, customer spending, etc.)?”

\sphinxAtStartPar
This section will, at times, feel a little abstract and woo\sphinxhyphen{}woo, but please hang in there. Answering Causal Questions is as much about critical thinking as it is about statistics, and the concepts introduced here will prove crucial to your ability to be effective in this space.


\subsection{What Does It Mean for X to Cause Y?}
\label{\detokenize{30_questions/40_answering_causal_questions:what-does-it-mean-for-x-to-cause-y}}
\sphinxAtStartPar
To understand what it means to answer a Causal Question, and why answering Causal Questions is intrinsically hard, we must start by taking a step back to answer the question: “what do we mean when we say some action X \sphinxstyleemphasis{causes} a change in some outcome Y?”

\sphinxAtStartPar
Seriously, what do we mean when we say “X causes Y?” Try and come up with a definition!

\sphinxAtStartPar
While this question may \sphinxstyleemphasis{seem} simple, it turns out that this question has been the subject of serious academic debate for hundreds of years by philosophers no less famous than David Hume. Indeed, even today there is still debate over how best to answer this question.

\sphinxAtStartPar
In this course, we will make use of the \sphinxstyleemphasis{Counterfactual Model of Causality} (sometimes called the Neyman\sphinxhyphen{}Rubin causal model). In plain English, it posits that for “doing X to cause Y”, it must be the case that if we do X, then Y will occur, and if we did not do X, then Y would not occur. This is by far the most used definition of causality today, and yet remarkably, it only emerged in the 20th Century and was only really fleshed out in the 1970s. Yeah… that recently.

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{Counterfactual Model of Causality}

\sphinxAtStartPar
For it to be the case that doing X causes Y”, it must be the case that if we do X, then Y will occur, and if we did not do X, then Y would not occur.
\end{sphinxShadowBox}


\subsection{Measuring the Effect of X on Y}
\label{\detokenize{30_questions/40_answering_causal_questions:measuring-the-effect-of-x-on-y}}
\sphinxAtStartPar
At first blush, this definition may seem simple. But its simplicity belies a profoundly difficult practical problem. See, this definition relies on \sphinxstyleemphasis{comparing} the value of our outcome Y in two states of the world: the world where we do X, and the world where we don’t do X. But as we only get to live in one universe, we can never perfectly know what the value of our outcome Y would be in \sphinxstyleemphasis{both} a world where we do X and one where we don’t do X for a given entity at a given moment in time. As such, we can \sphinxstylestrong{never} directly measure the causal effect of X on Y for a given entity (say, a given patient or customer) at a given moment in time — a problem known as the \sphinxstylestrong{Fundamental Problem of Causal Inference} (\sphinxcode{\sphinxupquote{causal inference}} being what people call the practice of answering Causal Questions).

\sphinxAtStartPar
To illustrate, suppose we were interested in the effect of taking a new drug (our X) on cancer survival (our Y) for a given patient (a woman named Shikha who arrived at the hospital on June 18th 2022). We can give her the drug and evaluate whether she is still alive a year later, but that alone can’t tell us whether the new drug \sphinxstyleemphasis{caused} her survival according to our counterfactual model of causality — after all, if she survives maybe she would have survived even without the drug! To actually know the effect of the drug on Shikha \sphinxstyleemphasis{by direct measurement,} we would have to be able to measure her survival both in the world where we gave her the drug \sphinxstyleemphasis{and} the world where we did not and compare outcomes.

\sphinxAtStartPar
Since we can never see both states of the world — the world where we undertake the action whose effect we want to understand and the world where we don’t — almost everything we do when trying to answer Causal Questions amounts to trying to find something we \sphinxstyleemphasis{can} measure that we think is a \sphinxstyleemphasis{good approximation} of the state of the world we can’t actually see.

\sphinxAtStartPar
A quick note on vocabulary: by convention, we refer to the action whose effect we want to understand as a “treatment,” and the state of the world where an entity receives the treatment as the “treated condition.” Similarly, we refer to the state of the world where an entity does \sphinxstyleemphasis{not} receive the treatment as the “control condition.” We use this language even when we aren’t talking about medical experiments or even experiments at all. We also refer to the state of the world we cannot observe as the “counterfactual” of the world we can observe — so the world where Shikha does not get the cancer drug is the \sphinxstyleemphasis{counterfactual condition} to the world where Shikha does get the drug.

\sphinxAtStartPar
It’s at this point most people start throwing out “but what about…“‘s, and that’s good! You should be — that’s exactly the kind of thinking you have to do when trying to answer Causal Questions. For example, “what about if we measured the size of Shikha’s tumor before she took the drug and compared it to the size of her tumor after? If the tumor got smaller as soon as she started the drug, then surely the drug caused the tumor to shrink!”

\sphinxAtStartPar
Maybe! Implicitly, what you have done is asserted that you think that the size of Shikha’s tumor before we administered the drug is a good approximation for what you think the size of Shikha’s tumor \sphinxstyleemphasis{would have been} had we not given her the drug.

\sphinxAtStartPar
But this type of comparison will always fall short of the Platonic ideal given by our definition of causality. Yes, Shikha’s tumor \sphinxstyleemphasis{may} have stayed the same size if we had not given her the drug (in which case the size of the tumor before she took the drug would be a good approximation), but it is also possible that regardless of whether we’d given her the drug, her cancer would have shrunk on its own.%
\begin{footnote}[1]\sphinxAtStartFootnote
The fact that diseases naturally change over time on their own is known as a disease’s “natural history.”
%
\end{footnote}

\sphinxAtStartPar
According to the Counterfactual Model of Causality, we could only ever \sphinxstyleemphasis{know} if taking the drug caused a decrease in tumor size if we could both administer the drug and observe the tumor \sphinxstyleemphasis{and also observe a parallel world in which the same person at the same moment in time was not given the drug for comparison}. And since we can never see this parallel world — the \sphinxstyleemphasis{counterfactual} to the world we observe — the best we can do is come up with different, imperfect tricks for \sphinxstyleemphasis{approximating} what might have happened in this parallel world, like comparing the tumor size before and after we administer the drug, imperfect though that may be.

\sphinxAtStartPar
So does that mean we’re doomed? Yes and no. Yes, it \sphinxstyleemphasis{does} mean that we’re doomed to never be able to take the exact measurements that make it possible to directly answer a Causal Question. But no, that doesn’t mean we can’t do anything — in the coming weeks, we will learn about different strategies for approximating counterfactual conditions, and in each case we will learn about what \sphinxstyleemphasis{assumptions} must be true for our strategy to provide a valid answer to our Causal Question. By making the assumptions that underlie each empirical strategy explicit, we will then be able to evaluate the plausibility of these assumptions.

\sphinxAtStartPar
In the example of Shikha, for example, we know that our comparison of tumor size before taking the drug to tumor size after taking the drug is only valid if her tumor \sphinxstyleemphasis{would not have gotten smaller without the drug}. This is something we can’t measure directly, but we can look to other patients with similar tumors, or the history of her tumor size to evaluate how often we see tumors get smaller at the rate observed after she took the drug. If it’s very rare for these types of tumors to ever get smaller, than we can have more confidence that a decrease in tumor size was the result of the drug.

\sphinxAtStartPar
We are also sometimes in a position to be more proactive than our effort to answer Causal Questions. Rather than trying to make inferences from the world around us using what is termed “observational data” (data that was generated through a process we did not directly control, a process we only “observe”), we can sometimes generate our own data through randomized experiments.

\sphinxAtStartPar
Randomized experiments — perhaps the most familiar tool for answering Causal Questions — are also just another way of approximating the unobservable counterfactual condition. In a randomized experiment — also known as “Randomized Control Trials (RCTs)”, or “A/B Tests” whether you’re hanging out with statisticians, doctors, or web developers — participants are assigned to either receive the treatment (the treatment group) or not (the control group) based on the flip of a coin, a roll of a die, or more commonly a random number generator on a computer. Provided we have enough participants, the Law of Large Numbers then promises that, \sphinxstyleemphasis{on average}, the people assigned to the control group will (probably) be “just like” the people assigned to the treatment group in every possible way (save being treated). Subject to a few other assumptions we’ll discuss in great detail later, that means that the outcomes of the control group — being just like the treatment group \sphinxstyleemphasis{on average} — will be a good approximation of what \sphinxstyleemphasis{would} have happened to the treatment group in a world where they did not receive the treatment.

\sphinxAtStartPar
Randomized experiments are not a silver bullet, however. The validity of experimental comparisons still rests on a number of assumptions, many of which cannot be directly tested. For example, we can never be entirely sure that when we randomly assigned people to control and treatment groups, the process was truly random, or that we ended up with people who were similar in both groups (the law of large numbers only promises that getting similar groups becomes \sphinxstyleemphasis{more likely} as the size of the groups increases, not that it will happen with certainty!). Moreover, conducting a randomized experiment requires working in a context where the researcher can control everything, and that can sometimes generate results that may not generalize to the big messy world where you actually want to act.


\subsection{So where does that leave us?}
\label{\detokenize{30_questions/40_answering_causal_questions:so-where-does-that-leave-us}}
\sphinxAtStartPar
For many data scientists, this will feel \sphinxstyleemphasis{profoundly} dissatisfying. Many people come to data science because of the promise that it will provide direct answers to questions about the world using statistics. But because of the Fundamental Problem of Causal Inference, this will never be possible when answering Causal Questions. Rather, the job of a data scientist answering Causal Questions is a lot like the job of a detective trying to solve a crime — your task is to determine what \sphinxstyleemphasis{probably} happened at a crime scene. You can gather clues, collect forensic evidence, and interview suspects, all in an effort to come up with the \sphinxstyleemphasis{most likely} explanation for a crime. But no matter how hard you try, you can’t go back in time to witness the crime itself, so you will never be able to be entirely sure if you are right or not.

\sphinxAtStartPar
But just as we investigate and prosecute crimes despite our inability to ever be 100\% certain an arrested suspect is guilty, so too must businesses and governments make decisions using the best available evidence, even when that evidence is imperfect. But it is our job, as data scientists, to help provide our stakeholders with the best available evidence, and also to help them understand the strength of the evidence we are able to provide.


\subsection{Why Passive\sphinxhyphen{}Prediction Is Not Enough}
\label{\detokenize{30_questions/40_answering_causal_questions:why-passive-prediction-is-not-enough}}
\sphinxAtStartPar
At this point, it is worth pausing to reflect on a question it may not have occurred to you to ask above — if answering Causal Questions is usually about \sphinxstyleemphasis{predicting} what would happen if we were to act on the world in a certain way, then how/why is it different from answering the kind of Passive\sphinxhyphen{}Prediction Questions we discussed previously?

\sphinxAtStartPar
There are a number of different ways one can frame the answer to this question, but the one I like most for Data Scientists is that  when answering a Passive\sphinxhyphen{}Predictive Question, we can usually achieve our goals simply by identifying \sphinxstyleemphasis{correlations} that we think are likely to persist into the future. For example, suppose we run the maintenance department for a rental car company. The fact that a car whose \sphinxstyleemphasis{Check Engine} light is on is a car that is likely to break down if it isn’t taken to a mechanic is enough for us to identify cars in trouble! Obviously, the \sphinxstyleemphasis{Check Engine} light isn’t \sphinxstyleemphasis{causing} the cars to break down, but it doesn’t have to to be useful.

\sphinxAtStartPar
But when seeking to answer Causal Questions, we wish to go beyond just identifying cars in trouble, and instead predict what might happen to cars if we \sphinxstyleemphasis{chose to act} in different ways. This requires going beyond simple Passive\sphinxhyphen{}Prediction because, in choosing to act, we are asking about how things might turn out in a world where we are behaving differently than we are currently — in other words, we are no longer being passive.

\sphinxAtStartPar
Thus, in a sense, answering Causal Questions is therefore \sphinxstyleemphasis{always} an example of “out\sphinxhyphen{}of\sphinxhyphen{}sample extrapolation” or “out\sphinxhyphen{}of\sphinxhyphen{}sample prediction”, because by definition we are saying we want to know what happens in a world where at least one major agent — us! — changes their behavior. And indeed, there’s a very real sense in which that’s what we \sphinxstyleemphasis{mean} by a causal relationship: a relationship between our actions and an outcome that would persist even if we change our behavior!

\sphinxAtStartPar
What’s a situation where a correlation is sufficient for Passive Prediction but not answering a Causal Question? Well, let’s go back to our example of the rental car maintenance manager — suppose rather than using \sphinxstyleemphasis{Check Engine} lights to identify cars that needed more attention, the manager decided to just cut the cables that run to all the \sphinxstyleemphasis{Check Engine} lights! After all, the cars that are breaking down all have their \sphinxstyleemphasis{Check Engine} light on, and the cars that don’t have their \sphinxstyleemphasis{Check Engine} lights almost never break down! So why not just disable the \sphinxstyleemphasis{Check Engine} lights on all these cars so they stop breaking down?

\sphinxAtStartPar
Now that we’ve been clear about what we mean when we ask “does X cause Y?”, we can now understand why this is a perfect example of why correlation does not always imply causation.

\sphinxAtStartPar
Fundamentally, the manager is asking “would cutting the cables to the \sphinxstyleemphasis{Check Engine} lights prevent our cars from breaking down?” For that to be true, we know that in an ideal universe, we would want to compare a car on the verge of breaking down that has its \sphinxstyleemphasis{Check Engine} light intact to that same car in a world where we cut the \sphinxstyleemphasis{Check Engine} light — then we can see if there is a difference in whether these cars break down!

\sphinxAtStartPar
But this is \sphinxstyleemphasis{not} the data our manager has turned to draw their conclusion — rather, they are comparing cars with their \sphinxstyleemphasis{Check Engine} lights on and cars without their \sphinxstyleemphasis{Check Engine} lights on. And it turns out that cars \sphinxstyleemphasis{without} their \sphinxstyleemphasis{Check Engine} lights on are not a good approximation for the cars \sphinxstyleemphasis{with} their \sphinxstyleemphasis{Check Engine} lights on because the cars without the light on are different from the cars with the light on in ways that matter for the likelihood of breaking down (they have engine problems!) \sphinxstyleemphasis{other} than the \sphinxstyleemphasis{Check Engine} light!

\sphinxAtStartPar
Depending on what classes you may have taken in the past, you may have heard these differences referred to as “confounders” or “omitted variables” — those are just different words or ways of talking about the same idea! Confounders or omitted variables are just different words for features that are different between the “treated” and “untreated” observations being examined that the untreated observations are bad approximations of the counter\sphinxhyphen{}factual condition for the treated observations!


\subsection{Next Steps}
\label{\detokenize{30_questions/40_answering_causal_questions:next-steps}}
\sphinxAtStartPar
In this reading, we learned — in an intuitive sense — why answering Causal Questions is inherently hard. But this explanation, while accurate, is a little informal to be rigorous. In the readings that follow, we will be introduced to the \sphinxstyleemphasis{Potential Outcomes Framework} — the formal statistical framework that underlies the Neyman\sphinxhyphen{}Rubin Counterfactual Model of Causality. This framework will help us reason more systematically about how and when methods like randomized experiments, linear regression, matching, and differences\sphinxhyphen{}in\sphinxhyphen{}differences can help us answer Causal Questions.

\sphinxAtStartPar
But first, in the interest of not losing perspective on the forest for the trees, a discussion of \sphinxstyleemphasis{how} Causal Questions are used in practice.


\bigskip\hrule\bigskip


\sphinxstepscope


\part{Causal Inference}

\sphinxstepscope


\chapter{The Potential Outcomes Framework}
\label{\detokenize{35_causal/10_potential_outcomes:the-potential-outcomes-framework}}\label{\detokenize{35_causal/10_potential_outcomes::doc}}
\sphinxAtStartPar
We’ve all heard the saying “correlation does not imply causation.” And that’s true, as far as it goes. But the saying would be more accurate (albeit less catchy) if it were “correlation does not \sphinxstyleemphasis{necessarily} imply causation.” That’s because while it is true that correlation does not always imply causation, it does under certain circumstances.

\sphinxAtStartPar
In this chapter, we will learn about the \sphinxstyleemphasis{Potential Outcomes Framework.} The Potential Outcomes Framework introduces mathematical notation to our discussion of parallel universes, and in doing so allows us to reason far more rigorously about causal effects.

\sphinxAtStartPar
Crucially, the Potential Outcomes Framework will also allow us to be very precise about what it is we are measuring when we measure a correlation, what we need to measure in order to answer a Causal Question, and what must be true for those two quantities to be the same.


\section{The Potential Outcomes Set Up}
\label{\detokenize{35_causal/10_potential_outcomes:the-potential-outcomes-set-up}}
\sphinxAtStartPar
The potential outcomes framework is built around the idea that we all have well\sphinxhyphen{}defined \sphinxstyleemphasis{potential} outcomes that would be observed if we were to be exposed to a treatment, and also potential outcomes that would be observed if we were not (i.e., we were exposed to a “control” condition). These potential outcomes are fixed and defined values within each of us. But, of course, the challenge is that we can only ever \sphinxstyleemphasis{observe} ourselves in one state of exposure.

\sphinxAtStartPar
Let’s suppose that we are interested in the effect of some treatment \(T\) on an outcome \(Y\) for a population of individuals \(i \in {1, 2, 3... N}\).

\sphinxAtStartPar
For ease of exposition, we will begin by assuming that our treatment \(T\) is a \sphinxstyleemphasis{binary treatment}, meaning it can only take on two values: \(T \in \{0, 1\}\). There’s nothing special about binary treatments, but they simplify notation, and so we often use them when discussing the potential outcomes framework, though everything we discuss here generalizes to continuous treatments.

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
Although we use the terminology \sphinxstyleemphasis{treatment} and \sphinxstyleemphasis{control} — terms you have most often encountered in the context of clinical trials — there is no, I repeat, \sphinxstylestrong{no} notion of randomization here. We can read \(T=1\) as “with the causal factor of interest present” and \(T=0\) as “without the causal factor of interest present,” where the reason for the presence or absence of the causal factor of interest is unknown.

\sphinxAtStartPar
This is often a source of confusion, so it bears repeating. We call this a \sphinxstyleemphasis{treatment}, but in doing so we do not imply that the receipt of treatment came from a randomized application. Instead, we are simply saying that \(T=1\), for instance, indicates a state in which the causal factor of interest is present.
\end{sphinxadmonition}

\sphinxAtStartPar
Corresponding to the two values of our treatment variable, we also have two values of our outcome variable — \(Y_i\) — for each individual. In particular, an individual \(i\) has both a potential outcome under the treatment condition (\(i\)’s value of \(Y\) in a world where \(i\) receives the treatment):
\begin{equation*}
\begin{split}Y^{T=1}_i \equiv Y^{1}_i\end{split}
\end{equation*}
\sphinxAtStartPar
and a potential outcome under the control condition (\(i\)’s value of \(Y\) in a world where \(i\) does not receive the treatment):
\begin{equation*}
\begin{split}Y^{T=0}_i \equiv Y^{0}_i\end{split}
\end{equation*}
\sphinxAtStartPar
We can think about these individual\sphinxhyphen{}level realizations of the random variable as the potential outcomes in different theoretical states of the world that we all carry within ourselves. We have a potential outcome for if the world is \(T=1\), and we have a potential outcome for if the world is \(T=0\). They both exist simultaneously in us, though we are only able to observe one.


\subsection{The Observed Outcomes Set Up}
\label{\detokenize{35_causal/10_potential_outcomes:the-observed-outcomes-set-up}}
\sphinxAtStartPar
Now that we have defined the potential outcomes, let’s discuss the set up for \sphinxstyleemphasis{observed} outcomes. Where \(T\) was in effect the theoretical treatment condition that allowed us to define \sphinxstyleemphasis{potential} outcomes for a theoretically applied causal factor, we use \(D\) to refer to the \sphinxstyleemphasis{observed} receipt of either treatment or control.
\begin{equation*}
\begin{split}D \in \{0,1\}\end{split}
\end{equation*}
\sphinxAtStartPar
As with T, whether \(D\) is equal to zero or one is not (necessarily) a function of randomization. We should read \(D_i = 1\) as “observed with the causal factor of interest” and read \(D_i = 0\) as “observed without the causal factor of interest.”

\sphinxAtStartPar
Because \(D\) is observed, not just potential, it is not possible for an individual to \sphinxstyleemphasis{simultaneously} be observed in treatment and control. Therefore, for individual \(i\)
\begin{equation*}
\begin{split}D_{i} = 0 \text{ or } D_{i} = 1\end{split}
\end{equation*}
\sphinxAtStartPar
Either we have an individual observed with the causal factor of interest present, or without the causal of interest present, but not both.

\sphinxAtStartPar
It follows that, for a given value of \(D\) — that is, for the given observed presence of the causal factor of interest or not — we will only ever be able to \sphinxstyleemphasis{observe}
\begin{equation*}
\begin{split}Y^0_i  \text{ if } D_{i} = 0\end{split}
\end{equation*}
\sphinxAtStartPar
or
\begin{equation*}
\begin{split}Y^1_i  \text{ if } D_{i} = 1\end{split}
\end{equation*}
\sphinxAtStartPar
But for any individual \(i\), we cannot observe both.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Why \(D\)? Honestly, I’m not sure, but I \sphinxstyleemphasis{think} it’s related to the term “dummy variable,” which is another term for an indicator variable. When analyzing actual, observed data, one often finds a dummy variable in the data that indicates whether a given observation was exposed to treatment or not. Since that dummy is indicating the actual, observed treatment status of an observation, it (kind of) makes sense to call this variable \(D\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
There is a natural tempatation to think of \(T\) and \(D\) symmetrically, given that both take on two values and are related to treatment exposure. Avoid this temptation.

\sphinxAtStartPar
Where the potential outcome frameworks assumes all entities have outcomes defined under both conditions of \(T\) (\(Y^0_i\) and \(Y^1_i\)), any given entity has only \sphinxstylestrong{one} value of \(D_i\). In the world we live, each entity either did or did not experience the treatment.

\sphinxAtStartPar
So while \(T\) relates to a pretty abstract notion of parallel worlds, \(D\) \sphinxstylestrong{just defines two different populations of entities in the data}.)
\end{sphinxadmonition}

\begin{sphinxShadowBox}

\sphinxAtStartPar
\(D_i\) just defines two different populations of entities in the data.
\end{sphinxShadowBox}


\section{Defining The Causal Effect}
\label{\detokenize{35_causal/10_potential_outcomes:defining-the-causal-effect}}
\sphinxAtStartPar
Now that we have that notation out of the way, let’s apply it to the definition of causation we introduced in our previous readings. We previously said that the \sphinxstyleemphasis{effect} of some treatment \(T\) on \(Y\) for an individual \(i\) is the difference in \(i\)’s outcomes in a world where the individual receives the treatment and \(i\)’s outcomes in a world where they do not. In our new notation, we can call this quantity \(\delta_i\), and write it as:
\begin{equation*}
\begin{split}\delta_i = Y^1_i - Y^0_i\end{split}
\end{equation*}
\sphinxAtStartPar
That is, for individual \(i\), the causal effect, defined as \(\delta_i\), is the difference between the individual \sphinxstyleemphasis{potential outcome} for the state of the world where the causal factor is present, minus the \sphinxstyleemphasis{potential outcome} for the state of the world where the causal factor is not present.

\sphinxAtStartPar
Of course, this is \sphinxstyleemphasis{clearly} a quantity we’ll never be able to observe — no person is ever able to be \sphinxstyleemphasis{both} subject to the treatment and not be subject to the treatment. And so for reasons that will become obvious later, we’ll need to move from estimating an \sphinxstyleemphasis{individual} quantity to estimating a quantity for a \sphinxstyleemphasis{population}.

\sphinxAtStartPar
When we move from the individual to the population, the causal effect needs to be defined as some sort of summary statistic that incorporates the information from the realization of the random variables \(Y^{T}\) for \sphinxstyleemphasis{all} individuals.

\sphinxAtStartPar
The first quantity we will investigate is the Average Treatment Effect, abbreviated as the ATE. We can define this quantity as:
\begin{equation*}
\begin{split}
ATE =& \frac{1}{N}\sum_{i \in {1, 2, 3 ... N}}\delta_i \\
=& \frac{1}{N}\sum_{i \in {1, 2, 3 ... N}}Y^1_i - Y^0_i
\end{split}
\end{equation*}
\sphinxAtStartPar
To avoid the constant need for summation notation, however, let’s assume our data is a random sample from the population we care about and move to working with expectations (\(E()\)):
\begin{equation*}
\begin{split}
ATE & = E(\delta_i) \\
& = E(Y^1_i - Y^0_i) \\
& = E(Y^1_i) - E(Y^0_i)
\end{split}
\end{equation*}
\sphinxAtStartPar
(We can bring the expectation inside the parentheses in this last step because the expectation is a linear operator.)

\sphinxAtStartPar
The equation above is similar in look and interpretation to \(\delta_i\) at the individual level further above. And the logic is exactly the same. We have simply moved from assessing the causal effect for an individual to assessing an \sphinxstyleemphasis{average} causal effect for a population, and thus taken the expected value of the individual\sphinxhyphen{}level causal effects for our entire population.

\sphinxAtStartPar
Why do we care about the Average Treatment Effect? While not perfect, it is our best guess for the average change in our outcome of interest we would observe if \sphinxstyleemphasis{everyone} in the population we are studying were subject to treatment. Thinking about authorizing a new prescription drug? You want to know the average effect it will have across all patients who take it. Thinking about launching a big advertising campaign? You want to know the average effect it will have on all the consumers who see it. \(ATE\), in other words, is the quantity your stakeholder — who wants you to answer a Causal Question because they want you to predict the effect of some action they’re thinking of taking — wants to know.


\section{What Is a Correlation?}
\label{\detokenize{35_causal/10_potential_outcomes:what-is-a-correlation}}
\sphinxAtStartPar
Having established the quantity we \sphinxstyleemphasis{want} to measure — the Average Treatment Effect for a population, \(ATE = E(Y^1_i) - E(Y^0_i)\) — let’s pause for a moment to come at things from the other side.

\sphinxAtStartPar
We have established that \(ATE\) is what we \sphinxstyleemphasis{want} to measure. But the fact we will never be able to observe both \(Y^0\) and \(Y^1\) for our full populations — and thus we can never directly observe \(E(Y^1_i) - E(Y^0_i)\) — remains true. So let’s pause for a moment to ask: “What \sphinxstyleemphasis{can} we observe?”

\sphinxAtStartPar
The first thing we can observe is outcome \(Y^0_i\) for all \(i\) that weren’t actually subject to treatment (i.e., for whom \(D_i = 0\)). Given that, we can estimate:
\begin{equation*}
\begin{split}E(Y^0_i | D_i = 0)\end{split}
\end{equation*}
\sphinxAtStartPar
And we can observe \(Y^1_i\) for all \(i\) that \sphinxstyleemphasis{were} actually subject to treatment (i.e., for whom \(D_i = 1\)). So we can also estimate:
\begin{equation*}
\begin{split}E(Y^1_i | D_i = 1)\end{split}
\end{equation*}
\sphinxAtStartPar
And if we estimate the difference between the outcomes for these two populations — those who were treated (\(D_i =1\)) and those who were not (\(D_i = 0\)) — we get the following quantity:
\begin{equation*}
\begin{split}E(Y^1_i | D_i = 1) - E(Y^0_i | D_i = 0)\end{split}
\end{equation*}
\sphinxAtStartPar
What is this? Well, it turns out that, for a binary treatment, this is the correlation between being treated and our outcome Y. It is, in fact, the exact quantity you get from a linear regression of \(Y\) on \(D\)!

\sphinxAtStartPar
And this is where all this notational work gets interesting. Most statistics students throw up a slide at the end of any presentation that says “but of course correlation does not imply causation” and move on — invoking this saying like an incantation they hope will protect them from critical questions. But we don’t have to be like most statistics students anymore.

\sphinxAtStartPar
Now that we’ve rigorously defined what we measure when we estimate an empirical correlation, and we know what need to measure in order to measure a causal effect, we can ask: when are these two equal? Because if we can detail the conditions under which these two quantities are the same, then we’ve identified the situations in which correlation \sphinxstyleemphasis{does} imply causation.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
If this type of conditioning notation (the \(| D_i = 1\) inside expectation) feels foreign, just think of it as subsetting the population for which you are estimating average outcomes. Again, \(D\) is just a variable for differenting between the two populations in our data — those that were exposed to the treatment and those that were not.
\end{sphinxadmonition}


\section{Correlation and Causation}
\label{\detokenize{35_causal/10_potential_outcomes:correlation-and-causation}}
\sphinxAtStartPar
Let’s begin by giving the empirical correlation between treatment and outcomes a name. As we’re \sphinxstyleemphasis{trying} to estimate the Average Treatment Effect (\(ATE\)), we will use the convention of adding a “hat” to a variable to indicate it is an empirically estimated quantity (rather than the “true”) quantity. Thus our correlation betweens:
\begin{equation*}
\begin{split}\widehat{ATE} = E(Y^1_i | D_i = 1) - E(Y^0_i | D_i = 0)\end{split}
\end{equation*}
\sphinxAtStartPar
So when does this equal \(E(Y^1_i) - E(Y^0_i)\)? Let’s apply a few manipulations to see if we can get \(\widehat{ATE}\) to look like \(ATE\).

\sphinxAtStartPar
First,  we use that classic math trick of adding and subtracting a carefully chosen term to an equation. This addition and subtraction offsets, so this is the same as adding 0 (and is thus allowed), but ends up being very helpful. In particular, we will add and subtract \(E(Y_i^0|D_i = 1)\):
\begin{equation*}
\begin{split}
\widehat{ATE }=  E(Y_i^1|D_i = 1) - E(Y_i^0|D_i = 0) + \underbrace{E(Y_i^0|D_i = 1) - E(Y_i^0|D_i = 1)}_\text{(Add up to zero)}
\end{split}
\end{equation*}
\sphinxAtStartPar
We can then shuffle these terms around to isolate two distinct quantities of interest: The Average Treatment Effect on the Treated (ATT), and Baseline Differences:
\begin{equation*}
\begin{split}
\widehat{ATE }&=  E(Y_i^1|D_i = 1) - E(Y_i^0|D_i = 0) + E(Y_i^0|D_i = 1) - E(Y_i^0|D_i = 1) \\
&= \underbrace{E(Y^1_i|D_i = 1) - E(Y_i^0|D_i = 1)}_\text{$ATT$} + \underbrace{E(Y^0_i|D_i = 1) - E(Y_i^0|D_i = 0)}_\text{Baseline Difference}
\end{split}
\end{equation*}

\subsection{Baseline Differences}
\label{\detokenize{35_causal/10_potential_outcomes:baseline-differences}}
\sphinxAtStartPar
Let’s actually start with the second of these quantities, \sphinxstyleemphasis{Baseline Differences}.  In substantive terms, Baseline Differences is just the difference in our outcome \(Y\) that would exist between our two groups (who \sphinxstyleemphasis{actually did} receive the treatment (\(D_i = 1\)) and the people who didn’t (\(D_i = 0\))) \sphinxstyleemphasis{in a world where no one was treated}. That’s why we call these “baseline” differences — they are differences in outcomes that we would observe even in a world that the treatment we are interested didn’t exist.

\sphinxAtStartPar
While I will use the term \sphinxstyleemphasis{Baseline Differences} for this quantity in this book, it also corresponds to the concept of “confounders” — factors that create a difference in outcomes between the treated group and the untreated group that are not caused by the treatment.

\sphinxAtStartPar
Unsurprisingly, therefore, the first condition that must be met for our correlation to imply causation (for \(\widehat{ATE}\) to equal \(ATE\)) is for there to be no baseline differences:

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{No Baseline Differences:}
\begin{equation}\label{equation:35_causal/10_potential_outcomes:eqn:condition_1}
\begin{split}
0 = E(Y^0_i|D_i = 1) - E(Y_i^0|D_i = 0)
\end{split}
\end{equation}\end{sphinxadmonition}

\sphinxAtStartPar
When true, our equation for \(\widehat{ATE}\) simplifies substantially from:
\begin{equation*}
\begin{split}
\widehat{ATE} = \underbrace{E(Y^1_i|D_i = 1) - E(Y_i^0|D_i = 1)}_\text{$ATT$} + \underbrace{E(Y^0_i|D_i = 1) - E(Y_i^0|D_i = 0)}_\text{Baseline Difference}
\end{split}
\end{equation*}
\sphinxAtStartPar
to
\begin{equation*}
\begin{split}
\widehat{ATE} = \underbrace{E(Y^1_i|D_i = 1) - E(Y_i^0|D_i = 1)}_\text{$ATT$}
\end{split}
\end{equation*}
\sphinxAtStartPar
So what, then, is ATT?


\subsection{Average Treatment Effect on the Treated (ATT)}
\label{\detokenize{35_causal/10_potential_outcomes:average-treatment-effect-on-the-treated-att}}
\sphinxAtStartPar
Perhaps the easiest way to understand \(ATT\) is to directly compare the formulas for \(ATT\) with \(ATE\):
\begin{equation*}
\begin{split}
ATT =& E(Y^1_i|D_i = 1) - E(Y_i^0|D_i = 1) \\
ATE =& E(Y^1_i) - E(Y_i^0)
\end{split}
\end{equation*}
\sphinxAtStartPar
As you can see, the only difference between the two terms is that \(ATT\) is just the \(ATE\) *for the population of entities that were actually subject to the treatment (\(D_i = 1\)).

\sphinxAtStartPar
So for our estimated correlation \(\widehat{ATE}\) to be the same as the Average Treatment Effect \(ATE\) we are most interested in, it must be the case that \(ATT = ATE\). When does that happen?

\sphinxAtStartPar
Well, as you may recall, the Average Treatment Effect is just an average effect of the treatment for everyone being studied. That means that if \(\lambda\) is the share of the population for which \(D_i = 1\), then \(ATE\) is:
\begin{equation*}
\begin{split}
ATE =& E(Y^1_i) - E(Y_i^0) \\
 =& \lambda \left(\underbrace{E(Y^1_i | D_i = 1) - E(Y_i^0 | D_i = 1)}_\text{ATT}\right) + (1-\lambda) \left(\underbrace{E(Y^1_i | D_i = 0) - E(Y_i^0 | D_i = 0)}_\text{Avg Treatment Effect on Untreated}\right)
\end{split}
\end{equation*}
\sphinxAtStartPar
So when does \(ATT = ATE\)? When \(E(Y^1_i | D_i = 1) - E(Y_i^0 | D_i = 1) = E(Y^1_i | D_i = 0) - E(Y_i^0 | D_i = 0)\). Or, in substantive terms, when the average effect of our treatment is the same for the subpopulation we observe experiencing the treatment as it is for the subpopulation that we observe \sphinxstyleemphasis{not} experiencing the treatment:

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{No Differential Treatment Effects:}
\begin{equation}\label{equation:35_causal/10_potential_outcomes:eqn:condition_2}
\begin{split}
ATT &= E(Y^1_i|D_i = 0) - E(Y_i^0|D_i = 0) \\
E(Y^1_i|D_i = 1) - E(Y_i^0|D_i = 1) &= E(Y^1_i|D_i = 0) - E(Y_i^0|D_i = 0)
\end{split}
\end{equation}\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
What would it look like for different populations to respond differently to treatment, and why might that happen in the real world? Well, it turns out it happens a lot when you aren’t doing randomized experiments.

\sphinxAtStartPar
Suppose you polled your classmates after an exam, and they \sphinxstyleemphasis{all} had headaches. Half of your classmates then choose for themselves to use acetaminophen (Tylenol) (\(D_i=1\)), and immediately feel better. Should you infer, then, that the ATE of acetomenophen is 100\%?

\sphinxAtStartPar
Well… probably not. While it sounds like the \(ATT\) may be 100\%, it’s worth asking \sphinxstyleemphasis{why} the other half of students \sphinxstyleemphasis{didn’t} take acetaminophen. What if they didn’t take acetaminophen because they knew from past experience that acetaminophen doesn’t alleviate their headache pain? In other words, the students who didn’t take acetaminophen (\(D_i = 0\)) chose to not take acetaminophen precisely because they already \sphinxstyleemphasis{knew} that its effect on them is 0. Then we have a clear situation where \(ATT \neq ATE\).
\end{sphinxadmonition}


\section{Wrapping It Up}
\label{\detokenize{35_causal/10_potential_outcomes:wrapping-it-up}}
\sphinxAtStartPar
We started off this chapter by noting that correlation does not \sphinxstyleemphasis{necessarily} imply causation. But there are certain condition under which correlation \sphinxstyleemphasis{does} imply causation, and we now know what those conditions are.

\sphinxAtStartPar
First, for our observed correlation between treatment and our outcome of interest to imply a causal effect, it must be the fact that there are no Baseline Differences. If there are Baseline Differences between the treated and untreated, then we just don’t have a way to know what portion of the difference between these groups is the effect of treatment and what differences were pre\sphinxhyphen{}existing.

\sphinxAtStartPar
(Some of you may be saying “But wait! Can’t I use a regression to estimate what share of differences are down to observable differences?” Yes, and we’ll discuss that soon, but the key word is “observable” — regression can help address Baseline Differences \sphinxstyleemphasis{if} we can measure the factors that give rise to those differences.)

\sphinxAtStartPar
If we don’t have Baseline Differences, then our correlation is at least a good estimate of the Average Treatment Effect on the Treated (\(ATT\)) — that is, the causal effect of our treatment \sphinxstyleemphasis{on the subpopulation that we observed experiencing to treatment}. And there will be times that the best we can do is get a good estimate of \(ATT\).

\sphinxAtStartPar
But if we really want to get a good estimate of the Average Treatment Effect for the \sphinxstyleemphasis{full} population we’re studying — something we want if we want to be able to estimate the effect of our treatment if we roll it out to everyone — then we also need it to be the case that the people we observe receiving the treatment (\(D_i = 1\)) respond to the treatment in the same way as those that don’t (\(D_i = 0\)).


\subsection{Testability of Assumptions}
\label{\detokenize{35_causal/10_potential_outcomes:testability-of-assumptions}}
\sphinxAtStartPar
Any now the big catch: we need two conditions to be true to be sure we are getting a good estimate of a causal effect:
\begin{equation*}
\begin{split}
0 & = E(Y^0_i|D_i = 1) - E(Y_i^0|D_i = 0)\\
E(Y^1_i|D_i = 1) - E(Y_i^0|D_i = 1) &= E(Y^1_i|D_i = 0) - E(Y_i^0|D_i = 0)
\end{split}
\end{equation*}
\sphinxAtStartPar
But neither of these are directly observable. In the case of No Baseline Differences, we can never directly observe \(E(Y^0_i|D_i = 1)\). And in the case of No Differential Treatment Effects, neither \(E(Y_i^0|D_i = 1)\) nor \(E(Y_i^0|D_i = 1)\) are observable.

\sphinxAtStartPar
As a result, it is only through critical thinking and domain knowledge that we can do our best to \sphinxstyleemphasis{reason} about whether we believe these conditions are met in any given situation. And \sphinxstyleemphasis{that} is why causal inference is hard.







\renewcommand{\indexname}{Index}
\printindex
\end{document}