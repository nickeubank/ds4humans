

# Adversarial Users

Of all external validity concerns data scientists tend to underappreciate, none is as likely to cause serious problems as the existence of *adversarial users*.

Adversarial users are users who attempt to subvert to intended functioning on a statistical or machine learning model. At first blush, adversarial users seem the stuff of spy novels and international espionage. In reality, however, they are likely to exist any time an algorithm or model is used to make decisions that are important to people.

The pervasive threat of adversarial users emerges because humans are strategic actors, and once people realize that outcomes they care about — insurance approvals, promotions, hiring, etc. — are being influenced by a formula, they will change their behavior to accommodate/game that formula.

## Adversarial Users as External Validity Concern

Adversarial users emerge because as soon as a model is deployed to make decisions, that deployment itself constitutes a change in how the world operates. We train our models using historical data in which people are not attempting to adapt their behavior to accommodate the model we are training. But as soon as we deploy our model, people's behavior will begin to change in response to the deployment, immediately threatening the validity of our model.

To the best of my knowledge, the term "adversarial users" is only used by computer scientists, but the concept that using any type of formula for evaluation or decision-making will immediately change how people behave (and thus the validity of the formula) has a long and storied history as immortalized by some famous "laws:"

> "When a measure becomes a target, it ceases to be a good measure."

- [Goodhart's Law, named for Charles Goodhart.](https://en.wikipedia.org/wiki/Goodhart%27s_law)

> "The more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor"

- [Campbell's Law, named for Donald Campbell](https://en.wikipedia.org/wiki/Campbell%27s_law)

> "Given that the structure of any [statistical] model consists of optimal decision rules of economic agents, and that optimal decision rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in policy will systematically alter the structure of [statistical] models."

- [Lucas Critique, named for Robert Lucas](https://en.wikipedia.org/wiki/Lucas_critique)

And since no idea is serious until it's been immortalized in an XKCD comic:

![Goodhart's Law XKCD](https://imgs.xkcd.com/comics/goodharts_law.png)

## Robograders: A Close To Home Example

To illustrate what "adversarial users" look like in what may feel like a familiar context, consider the Essay RoboGrader. Training an algorithm to answer the question "If a human English professor read this essay, what score would they give it?" is relatively straightforward — get a bunch of essays, give them to some English professors, then fit a supervised machine learning algorithm to that training data. What could go wrong?

The problem with this strategy is that the training data was generated by humans *who knew they were writing essays for humans to read*. As a result, they wrote good essays. The machine learning algorithm then looked for correlations between human essay rankings and features of the essays, and as a result, it could easily predict essay scores, at least on a coarse scale.

But what happens when humans realize they aren't being graded by humans? Well, now, instead of writing for a human, they will write for the algorithm. They figure out what the algorithm rewards — big, polysyllabic words (don't worry, doesn't matter if they're used correctly), long sentences, and connecting phrases like "however" — and stuff them into their essays.[^robograders]

[^robograders]: I cannot for the life of me find the podcast where I first discovered this phenomenon being discussed, but it had lots of great colorful examples of school kids doing this. For now all I can find are these articles: [here](https://www.wbur.org/hereandnow/2020/09/03/online-learning-algorithm), [here](https://www.npr.org/2018/06/30/624373367/more-states-opting-to-robo-grade-student-essays-by-computer), [here](https://www.theverge.com/2012/4/23/2969331/erater-robotic-essay-grader-effectiveness), [here](https://www.nytimes.com/2012/04/23/education/robo-readers-used-to-grade-test-essays.html) and [here](https://www.vice.com/en/article/pa7dj9/flawed-algorithms-are-grading-millions-of-students-essays).

This works because the essay writers who used polysyllabic words and long sentences in the training data happened to also be the students who were writing good essays. These were reliable predictors of scores in essays people wrote for humans. But they *aren't* a reliable predictor of essay quality in a world where students know the essays *aren't* being written for humans, just machines.

Another way of thinking about this is that we're back to the issue of alignment problems: they *want* the algorithm to reward good writing, but that's not actually what they trained it to do. In this case, however, the alignment problem is rearing its head because people are actively trying to exploit this difference.





