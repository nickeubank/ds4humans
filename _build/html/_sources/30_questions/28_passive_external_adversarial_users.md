
# Adversarial Users

Of all external validity concerns I find data scientists tend to under-estimate, none is as likely to cause serious problems as the existence of *adversarial users*.

Adversarial users are users who attempt to subvert to intended functioning on a statistical or machine learning model. At first blush, adversarial users seem the stuff of spy novels and international espionage. In reality, however, they are likely to exist any time an algorithm or model is used to make decisions that are important to people.

The pervasive threat of adversarial users emerges because humans are strategic actors, and once people realize that outcomes they care about — insurance approvals, promotions, hiring, etc. — are being influenced by a formula, they will change their behavior to accommodate/game that formula.

## Adversarial Users as External Validity Concern

Adversarial users emerge because as soon as a model is deployed to make decisions, that deployment itself constitutes a change in how the world operates. We train our models using historical data in which people are not attempting to adapt their behavior to accommodate the model we are training. But as soon as we deploy our model, people's behavior will begin to change in response to the deployment, immediately threatening the validity of our model.

To the best of my knowledge, the term "adversarial users" is only used by computer scientists, but the concept that using any type of formula for evaluation or decision-making will immediately change how people behave (and thus the validity of the formula) has a long and storied history as immortalized by some famous "laws:"

> "When a measure becomes a target, it ceases to be a good measure."

- [Goodhart's Law, named for Charles Goodhart.](https://en.wikipedia.org/wiki/Goodhart%27s_law)

> "The more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor"

- [Campbell's Law, named for Donald Campbell](https://en.wikipedia.org/wiki/Campbell%27s_law)

> "Given that the structure of any [statistical] model consists of optimal decision rules of economic agents, and that optimal decision rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in policy will systematically alter the structure of [statistical] models."

- [Lucas Critique, named for Robert Lucas](https://en.wikipedia.org/wiki/Lucas_critique)

And since no idea is serious until it's been immortalized in an XKCD comic:

![Goodhart's Law XKCD](https://imgs.xkcd.com/comics/goodharts_law.png)

## Robograders: A Close To Home Example

To illustrate what "adversarial users" look like in what may feel like a familiar context, consider the Essay RoboGrader. Training an algorithm to answer the question "if a human English professor read this essay, what score would they give it?" is relatively straightforward — get a bunch of essays, give them to some English professors, then fit a supervised machine learning algorithm to that training data. What could go wrong?

The problem with this strategy is that the training data was generated by humans *who knew they were writing essays for humans to read*. As a result, they wrote good essays. The machine learning algorithm then looked for correlations between human essay rankings and features of the essays, and as a result it could easily predict essay scores, at least on a coarse scale.

But what happens when humans realize they aren't being graded by humans? Well, now instead of writing for a human, they will write for the algorithm. They figure out what the algorithm rewards — big, polysyllabic words (don't worry, doesn't matter if they're used correctly), long sentences, and connecting phrases like "however" — and stuff them into their essays.[^robograders]

[^robograders]: I cannot for the life of me find the podcast where I first discovered this phenomenon being discussed, but it had lots of great colorful examples of school kids doing this. For now all I can find are these articles: [here](https://www.wbur.org/hereandnow/2020/09/03/online-learning-algorithm), [here](https://www.npr.org/2018/06/30/624373367/more-states-opting-to-robo-grade-student-essays-by-computer), [here](https://www.theverge.com/2012/4/23/2969331/erater-robotic-essay-grader-effectiveness), [here](https://www.nytimes.com/2012/04/23/education/robo-readers-used-to-grade-test-essays.html) and [here](https://www.vice.com/en/article/pa7dj9/flawed-algorithms-are-grading-millions-of-students-essays).

This works because the essay writers who used polysyllabic words and long sentences in the training data happened to also be the students who were writing good essays. These were reliable predictors of scores in essays people wrote for humans. But they *aren't* a reliable predictor of essay quality in a world where students know the essays *aren't* being written for humans, just machines.

Another way of thinking about this is that we're back to the issue alignment problems: they *want* the algorithm to reward good writing, but that's not actually what they trained it to do. In this case, however, the alignment problem is rearing its head because people are actively trying to exploit this difference.

## Adversarial User Examples

The fact that an empirical regularity may be a good basis for answering Passive Predictive Questions in historical data but not once the model is actually deployed raises its head far, *far* more often than you might expect. To illustrate, here are just a few examples:

### Search Engine Optimization

The history of Google and other search engines is, essentially, a history of adversarial users ruining things for everyone. Have you ever wondered why, when you search for a recipe online, you have to scroll through paragraphs of pointless narrative before you get to the actual recipe? Or why youtube thumbnails are full of shocked faces and clickbait titles? In short: adversarial users! 

Google's perpetual challenge is to (a) find features that identify the websites that users want to see at the top of their search results, (b) update their search algorithm to up-rank sites with those features, then (c) find new features to use as everyone figures out what features Google is rewarding and adds them to their spammy sites.

In the beginning, for example, Google's first ranking algorithm — [PageRank](https://en.wikipedia.org/wiki/PageRank) — essentially up-ranked sites that were linked to by other sites.[^pagerank] The more the web seemed to "like" a site, the higher it would rank in Google! Essentially, it outsourced the evaluation of website quality to web itself, generating results of a quality that quickly turned "Google" from a noun to a verb.

[^pagerank]: Google didn't just count the number of links, of course, it weighted links by the ranking of referring site. A link from a high ranked source was obviously more valuable than a link from a random blog. There's a recursive dynamic to this too, of course, that essentially boils down to eigenvector centrality. But that's unimportant.

It wasn't long, though, before people realized there was a way to game this system. If a site could increase the number of links to their site, they could increase their ranking and thus their site traffic. So people started creating websites just to create links to the page on which they made money. Entire ecosystems emerged of people and sites linking to one another to "artificially" boost rankings, a practice known as Search Engine Optimization (SEO).

Google, of course, noticed this and shifted metrics. Over the years, Google has been forced to turn to a near endless number of different heuristics for evaluating page quality, including things like "time users spend on a page," or "number of user clicks on a result." And each time this occurred, adversarial users looked for ways to game the system, and even well-intentioned websites were forced to join the rat race as well, often making their sites worse to ensure they could compete with the "high scores" being generated by bad actors.

(If you're interested in multitudinous ways in which SEO is responsible for how the internet looks and feels today, you really can do no better than this recent feature from [The Verge](https://www.theverge.com/features/23931789/seo-search-engine-optimization-experts-google-results).)

### Zillow

In 2021, the real estate information website Zillow [announced](https://www.wsj.com/articles/zillow-quits-home-flipping-business-cites-inability-to-forecast-prices-11635883500?mod=article_inline) that it was shutting down an initiative to use its price models to buy and flip US residential houses, an initiative it later admitted had lost a [staggering \$881 million dollars](https://www.wsj.com/articles/zillows-shuttered-home-flipping-business-lost-881-million-in-2021-11644529656?st=Bo8Ysa&reflink=desktopwebshare_permalink) in 2021 alone.

So what went wrong? Portions of Zillow's loss appear to have been the result of over-confidence in their ability to predict overall housing market movements. But [as pointed out](https://www.gsb.stanford.edu/insights/flip-flop-why-zillows-algorithmic-home-buying-venture-imploded) by a group of finance professors at the Stanford Graduate School of Business, another major contributor was likely to have been a phenomenon known as "adverse selection," a special flavor of the adversarial user problem.

To understand what happened, put yourself in Zillow's shoes — you have a model that's quite good at predicting the price at which houses will sell (the estimates from this model are referred to as "Zestimates"). So good, in fact, that you think you could make some money by offering to pay homeowners cash to buy their homes at a discount to your "Zestimate" of the home's value, then flip the house for what it's really worth.

(If you've never done it before, selling a home in the US is an *incredibly* complicated and time-consuming process, so it's not unreasonable to expect many people would accept a slightly low price in exchange for a quick sale.)

So you pour *billions* into the house flipping business, using your Zestimate model to decide what homes are worth. And... well, you know how this ends: by the end of 2021, you'd lost 881 million dollars. But why?

Well, imagine there are two houses in similar neighborhoods with similar square footage, the same number of beds and bathrooms, in the same school systems, etc. Suppose your Zestimate for both homes — based on past sales and all publicly available data on the house and a few questions you ask the sellers — is \$350,000. So you offer them both \$300,000 for their homes, figuring you'll make a profit of \$50,000 on each.

But as a data scientist, you know that all models are imperfect. Sure, on average your Zestimates are dead on, but neither of these homes is probably worth exactly \$350,000. Suppose the first home — Home A — has a truly beautiful view over one of the best parks in the city, and while it's not too far from the freeway, there are a set of tall apartment buildings between the home and the freeway that block all traffic noise. Let's suppose that because of all of these factors — factors that aren't available to the Zillow algorithm — *true* value of Home A is actually \$450,000.

Now let's consider Home B. It's the same distance from the freeway, but where tall apartment buildings blocked traffic noise from Home A, the local geography channels the noise *right at* Home B. Moreover, while every other home on the street has a great view of the nearby park, *right* across the street from Home B is a city electrical utility station. The house also has fewer windows, and all are blocked by neighbor's homes. Again, because of all these factors that aren't available to the Zillow algorithm, the *true* value of Home B is actually only \$250,000. 

So when you, Zillow, make offers of \$300,000 to the owners of both Home A and Home B, what do you think each owner will do? Well, obviously, the owner of Home A is gonna think "I'm getting low-balled. No way I'm selling for \$300,000." And the owner of Home B is gonna say "holy cow, what are these idiots thinking? Yes! Please! I will absolutely sell you my house for \$300,000!" And Zillow will lose *at least* \$50,000 on Home B even before it has to pay fees and taxes. And *that's* how companies lose hundreds of millions trying to buy and sell homes at scale based on models, a phenomenon that's not only impacted Zillow, but [also other companies that tried to do something similar during this period](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3616555).

#### Adverse Selection and Asymmetric Information

Economists use the term adverse selection for this phenomenon because homeowners were *selecting* whether to accept Zillow's offer, and doing so in a way that was *adverse* (bad) for Zillow. And the reason that deals that were bad for Zillow were especially likely to happen was because the homeowners in our example had *private information* about the value of their homes that was unavailable to Zillow's algorithm. In economics, we refer to this as asymmetric information, and it happens *everywhere*.

Take car insurance. In the United States, car insurance is provided by private companies, and these private companies have to be sure that the total amount of money they take in monthly premiums from all their clients must be enough to cover what they pay out to clients who experience accidents. That means that the more clients a car insurance company has that are safe drivers, the less they will have to pay out for car repairs and the lower they can set their monthly premiums. BUT: most people have a sense of whether they are good drivers or not, and the people who get the most out of car insurance are the people who get into a lot of accidents. So if you're an insurance company, how do you ensure that you aren't swamped by bad drivers, especially if you're offering a lower price than your competitors?

One of the answers is that you do everything you can to reduce the degree of asymmetric information by collecting information on client accident histories and traffic tickets and including that in your pricing model. You offer potential discounts to drivers willing to install a device on their car that allows the insurance company to monitor how people drive (how often do they speed, slam the brakes, etc.). And you secretly buy data [that car manufactures are quietly (and in some cases, illegally) collecting](https://www.nytimes.com/2024/04/23/technology/general-motors-spying-driver-data-consent.html?unlocked_article_code=1.sE4.eFtk.dNaHXEZcJ5rc&smid=url-share) on how individual's driving behavior.

But the other strategy is to try to find ways to reduce the likelihood a driver is buying a policy because they expect to get in a lot of car accidents. It's one reason that everyone offers discounts if you bundle your home and auto insurance — maybe a bad driver will shop around for the car insurance they expect to need most when buying car insurance on its own, but you probably weren't thinking about your driving when you decided who would insure your home, so the pool of people eligible for discounted car insurance because they already have home insurance with a given company is probably less subject to adverse sorting than the people just buying auto insurance.

That's also why it's also much easier to get some types of insurance — like life insurance — through the company you work for than on your own — companies know that the average person working at Duke is probably less likely to be dealing with a terminal disease than the average person who just shows up at their door asking to take out a life insurance policy.






