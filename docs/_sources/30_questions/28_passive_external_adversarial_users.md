
# Adversarial Users

Another external validity concern for Passive Prediction Models is *adversarial users*. Adversarial users are users who deliberately attempt to subvert a statistical or machine learning model. The idea of adversarial users might seem like the stuff of spy novels, but they're actually *much* more common than you might think.

To illustrate, consider the Essay RoboGrader. Training an algorithm to answer the question "if a human English professor read this essay, what score would they give it?" is relatively straightforward — get a bunch of essays, give them to some English professors, then fit a supervised machine learning algorithm to that training data. What could go wrong?

The problem with this strategy is that the training data was generated by humans *who knew they were writing essays for humans to read*. As a result, they wrote good essays. The machine learning algorithm then looked for correlations between human essay rankings and features of the essays, and as a result it could easily predict essay scores, at least on a coarse scale.

But what happens when humans realize they aren't being graded by humans? Well, now instead of writing for a human, they will write for the algorithm. They figure out what the algorithm rewards — big, polysyllabic words (don't worry, doesn't matter if they're used correctly), long sentences, and connecting phrases like "however" — and stuff them into their essays.[^robograders]

[^robograders]: I cannot for the life of me find the podcast where I first discovered this phenomenon being discussed, but it had lots of great colorful examples of school kids doing this. For now all I can find are these articles: [here](https://www.wbur.org/hereandnow/2020/09/03/online-learning-algorithm), [here](https://www.npr.org/2018/06/30/624373367/more-states-opting-to-robo-grade-student-essays-by-computer), [here](https://www.theverge.com/2012/4/23/2969331/erater-robotic-essay-grader-effectiveness), [here](https://www.nytimes.com/2012/04/23/education/robo-readers-used-to-grade-test-essays.html) and [here](https://www.vice.com/en/article/pa7dj9/flawed-algorithms-are-grading-millions-of-students-essays).

This works because the essay writers who used polysyllabic words and long sentences in the training data happened to also be the students who were writing good essays. These were reliable predictors of scores in essays people wrote for humans. But they *aren't* a reliable predictor of essay quality in a world where students know the essays *aren't* being written for humans, just machines.

Another way of thinking about this is that we're back to the classic problem of alignment problems: they *want* the algorithm to reward good writing, but that's not actually what they trained it to do. In this case, however, the alignment problem is rearing its head because people are actively trying to exploit this difference.

While these examples are fun, not all are, and adversarial users is a HUGE and never ending problem for spam filters, network intrusion detection, credit and fraud monitoring, and more.

## Goodhart's Law, Cambell's Law, and the Lucas Critique

It's worth noting that the threat of adversarial users is not a new phenomenon unique to the age of machine learning. Indeed, the idea of adversarial users is closely linked to at least three much older ideas:

> "When a measure becomes a target, it ceases to be a good measure."

- [Goodhart's Law, named for Charles Goodhart.](https://en.wikipedia.org/wiki/Goodhart%27s_law)

> "The more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor"

- [Campbell's Law, named for Donald Campbell](https://en.wikipedia.org/wiki/Campbell%27s_law)

> "Given that the structure of any [statistical] model consists of optimal decision rules of economic agents, and that optimal decision rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in policy will systematically alter the structure of [statistical] models."

- [Lucas Critique, named for Robert Lucas](https://en.wikipedia.org/wiki/Lucas_critique)

And since no idea is serious until it's been immortalized in an XKCD comic:

![Goodhart's Law XKCD](https://imgs.xkcd.com/comics/goodharts_law.png)


## SEO as adversarial